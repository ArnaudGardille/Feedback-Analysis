{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjPRVXra8lVZ"
      },
      "source": [
        "# Dependancies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKSREXZg85yy"
      },
      "source": [
        "## Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMSukVZh29HK"
      },
      "outputs": [],
      "source": [
        "#!pip install sentence_transformers langchain openai tqdm datasets asyncio scikit-learn cohere tiktoken umap altair"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Md3DNHlV22Ai"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "from datasets import load_dataset\n",
        "import umap\n",
        "import altair as alt\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from typing import List\n",
        "import enum\n",
        "\n",
        "from langchain_community.llms import Ollama\n",
        "from langchain.output_parsers.regex_dict import RegexDictParser\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from langchain_core.messages import HumanMessage, SystemMessage, ChatMessage\n",
        "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
        "from pydantic import BaseModel, Field, validator, create_model\n",
        "from openai import AsyncOpenAI, OpenAI\n",
        "import asyncio\n",
        "import os\n",
        "\n",
        "import requests\n",
        "import json\n",
        "\n",
        "import itertools\n",
        "from copy import deepcopy\n",
        "from tqdm.notebook import tqdm, trange\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "import umap.umap_ as umap\n",
        "#import umap\n",
        "import hdbscan\n",
        "\n",
        "from typing import Literal, Union, Optional\n",
        "from pydantic.config import ConfigDict\n",
        "\n",
        "import openai\n",
        "import instructor\n",
        "\n",
        "from src.bubble import *\n",
        "from src.models import *\n",
        "from src.utilities import *\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiENBWdi22Al"
      },
      "outputs": [],
      "source": [
        "#PROJECT =  \"Metro\" #\"Cheerz\"\n",
        "#project_path = 'Results/'+PROJECT\n",
        "#os.makedirs(project_path, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "aspects_df = get(\"Aspect\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "aspects_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Insights extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TYPES_LIST = ['Point positif', 'Nouvelle fonctionnalité', 'Point de douleur', 'Bug']\n",
        "\n",
        "tags_df = get(\"Tag\", constraints=[])\n",
        "#types_df = get(\"Type\", constraints=[])\n",
        "categories_df = get(\"Category\")\n",
        "subcategories_df = get(\"SubCategory\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "company_infos = bubble_client.get(\n",
        "    \"Company\",\n",
        "    bubble_id=COMPANY_ID,\n",
        ")\n",
        "project_infos = bubble_client.get(\n",
        "    \"Project\",\n",
        "    bubble_id=PROJECT_ID,\n",
        ")\n",
        "\n",
        "feedback_context = {\n",
        "    \"entreprise\": company_infos[\"Name\"],\n",
        "    \"context\": company_infos['Context'],\n",
        "    \"role\": company_infos['Role'],\n",
        "    \"cible\": project_infos['Target'],\n",
        "    \"types\": '- '+' \\n- '.join(TYPES_LIST),\n",
        "    \"tags\": '- '+' \\n- '.join([row[\"Name\"]+' : '+row[\"Description\"] for _,row in tags_df.iterrows()]),\n",
        "    #\"types\": '- '+' \\n- '.join([row[\"Name\"]+' : '+row[\"Description\"] for _,row in types_df.iterrows()]),\n",
        "    #\"insight_types\": types_descr,\n",
        "    #\"insight_categories\": tags_descr,\n",
        "    #\"question\": project_infos['Study_question'],\n",
        "    #\"exemple_commentaire\": exemple_commentaire,\n",
        "    #\"example_insights\": '\\n- '.join(list(examples_insights_df['Insights qui devraient en découler'])),\n",
        "}\n",
        "\n",
        "feedback_context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ID_CATEG_NONE = categories_df[categories_df[\"Name\"].isna()].iloc[0][\"_id\"]\n",
        "SUBCATEG_NONE = subcategories_df[subcategories_df[\"Name\"].isna()]\n",
        "ID_CATEG_NONE, SUBCATEG_NONE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TypeInsight = enum.Enum(\"Type de l'insight\", [(convert_text_to_constants(t), t) for t in TYPES_LIST])\n",
        "#types_to_id = {convert_text_to_constants(row.Name): row._id for _, row in types_df.iterrows()}\n",
        "\n",
        "#TypeInsight = enum.Enum(\"Type de l'insight\", [(convert_text_to_constants(row.Name), row.Name) for _, row in types_df.iterrows()])\n",
        "TagInsight = enum.Enum(\"Tag de l'insight\", [(convert_text_to_constants(row.Name), row.Name) for _, row in tags_df.iterrows()])\n",
        "tags_to_id = {convert_text_to_constants(row.Name): row._id for _, row in tags_df.iterrows()}\n",
        "#type_to_id = {convert_text_to_constants(row.Name): row._id for _, row in types_df.iterrows()}\n",
        "#type_to_id[convert_text_to_constants('Point de douleur')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "list(TypeInsight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#FeedbackIndex = enum.Enum(\"Indice du retour associé\", [(str(i), i) for i in range(BATCH_SIZE)])\n",
        "\n",
        "class Insight(BaseModel):\n",
        "    insight: str = Field(description=\"Insight, c'est a dire infirmation importante que révèle cette étude à l'entreprise, et lui permettera d'améliorer son experience utilisateur, sa stratégie ou son produit.\") \n",
        "    insight_type: TypeInsight = Field(description=\"Type de l'insight, parmis \"+', '.join(TYPES_LIST)) \n",
        "    insight_tags: List[TagInsight] = Field(description=\"Tags de l'insight. Peut eventuellement être une liste vide.\")\n",
        "    associated_indexes: List[int] = Field(description=\"Indices des retours associés.\")\n",
        "    details: List[str] = Field(description=\"Détails de l'insights. Peut eventuellement être une liste vide.\") \n",
        "    consequences: List[str] = Field(description=\"Conséquences pour l'entrerpise. Peut eventuellement être une liste vide.\")\n",
        "    recommandations: List[str] = Field(description=\"Recommandations pour l'entrerpise. Peut eventuellement être une liste vide.\")\n",
        "    def __str__(self):\n",
        "        return \"\"\"{0}\n",
        "\n",
        "Détails:\n",
        "    - {1}\n",
        "Conséquences:\n",
        "    - {2}\n",
        "Recommandations:\n",
        "    - {3}\n",
        "Retours: \n",
        "    - {4}\n",
        "Type: \n",
        "    {5}\n",
        "Tags: \n",
        "    {6}\n",
        "        \"\"\".format(self.insight, '\\n    - '.join(self.details), '\\n    - '.join(self.consequences), '\\n    - '.join(self.recommandations), '\\n    - '.join([str(x) for x in self.associated_indexes]), self.insight_type._value_, \", \".join([x._value_ for x in self.insight_tags]))\n",
        "\n",
        "class ListInsights(BaseModel):\n",
        "    insights: List[Insight] = Field(description=\"Liste des insights qui ont été déduits.\")\n",
        "    def __str__(self):\n",
        "        return '\\n\\n'.join([str(x) for x in self.insights])\n",
        "\n",
        "#ListInsights.model_json_schema() \n",
        "    \n",
        "test = ListInsights(insights=[Insight(insight='Accueil chaleureux et personnel serviable', insight_type=TypeInsight.POINT_POSITIF, insight_tags=[TagInsight.MAGASIN, TagInsight.SERVICE_CLIENT], associated_indexes=[96, 123, 475, 1249, 1267, 1372, 1695, 1965], details=[], consequences=[], recommandations=[]), Insight(insight='Ecoute attentive et conseils pertinents', insight_type=TypeInsight.POINT_POSITIF, insight_tags=[TagInsight.MAGASIN, TagInsight.SERVICE_CLIENT], associated_indexes=[20, 891, 1372], details=[], consequences=[], recommandations=[])])\n",
        "print(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('Prompts/fr/prompt_regroupement.txt') as f:\n",
        "    prompt_regroupement = PromptTemplate.from_template(f.read())\n",
        "\n",
        "\n",
        "print(prompt_regroupement.template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "with open('Prompts/fr/prompt_regroupement_create_example.txt') as f:\n",
        "    prompt_regroupement_create_example = PromptTemplate.from_template(f.read())\n",
        "\n",
        "example = apply_async_analysis([prompt_regroupement_create_example], ListInsights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "example_clustering_json = example[0].json()\n",
        "from pprint import pprint\n",
        "pprint(example_clustering_json)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompts = []\n",
        "subcat_ids = []\n",
        "for subcat_id, df in aspects_df[aspects_df['Explanation'].notna()].groupby('SubCategory'):\n",
        "    subcat = subcategories_df[subcategories_df['_id'] == subcat_id].iloc[0]\n",
        "    cat = categories_df[categories_df['_id'] == subcat['Category']].iloc[0]\n",
        "\n",
        "    feedbacks = '\\n'.join([str(index)+' : '+content for (index, content) in df['Explanation'].items()])\n",
        "    \n",
        "    prompts.append(prompt_regroupement.invoke({\"feedbacks\": feedbacks, \"category\": cat[\"Name\"]+\" : \"+subcat['Name'], \"example\":example_clustering_json, **feedback_context}).text)\n",
        "    subcat_ids.append(subcat_id)\n",
        "\n",
        "#print(prompts[0])\n",
        "print(\"Traitement synchronisé de\", len(prompts), \"prompts.\")\n",
        "list_insights = apply_async_analysis(prompts, ListSubCategory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#print(prompts[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def send_insights(insights_group, cat_id, subcat_id):\n",
        "\n",
        "    if len(insights_group.insights)>0:\n",
        "      res = bubble_client.create(\"Insight\",\n",
        "        [{\n",
        "          \"Company\": COMPANY_ID,\n",
        "          \"Project\": PROJECT_ID,\n",
        "          \"Name\": insight.insight,\n",
        "          \"Category\": cat_id,\n",
        "          \"SubCategory\": subcat_id,\n",
        "          \"Type\": insight.insight_type._value_,\n",
        "          \"Tags\": [tags_to_id[convert_text_to_constants(tag._name_)] for tag in insight.insight_tags],\n",
        "          \"Aspects\": list(aspects_df.iloc[insight.associated_indexes]._id),\n",
        "          \"Feedbacks\": list(aspects_df.iloc[insight.associated_indexes].Associated_feedback),\n",
        "          \"Nb Feedbacks\": len(list(aspects_df.iloc[insight.associated_indexes].Associated_feedback)),\n",
        "          }  for insight in insights_group.insights]                     \n",
        "        )\n",
        "      insights_id = [x['id'] for x in res]\n",
        "    else:\n",
        "      insights_id = []\n",
        "\n",
        "    if len(insights_group.consequences)>0:\n",
        "      res = bubble_client.create(\"Consequence\",\n",
        "        [{\n",
        "          \"Company\": COMPANY_ID,\n",
        "          \"Project\": PROJECT_ID,\n",
        "          \"Description\": conseq.detail,\n",
        "          \"Name\": conseq.title,\n",
        "          }  for conseq in insights_group.consequences]                     \n",
        "        )\n",
        "      consequences_id = [x['id'] for x in res] \n",
        "    else:\n",
        "      consequences_id = []\n",
        "\n",
        "\n",
        "    bubble_id = bubble_client.create(\"Insights Group\", {\n",
        "      \"Company\": COMPANY_ID,\n",
        "      \"Project\": PROJECT_ID,\n",
        "      \"Name\": insights_group.title,\n",
        "      \"Category\": cat_id,\n",
        "      \"SubCategory\": subcat_id,\n",
        "      \"Insights\": insights_id,\n",
        "      \"Consequences\": consequences_id,\n",
        "      #\"Tags\": [tags_to_id[tag._name_] for tag in insight.insight_tags],\n",
        "      })\n",
        "\n",
        "    \n",
        "\n",
        "for (list_insight_groups, subcat_id) in tqdm(zip(list_insights, subcat_ids)):\n",
        "  cat_id = subcategories_df[subcategories_df['_id'] == subcat_id].iloc[0].Category\n",
        "  for insights_group in list_insight_groups.sub_categories_list:\n",
        "    send_insights(insights_group, cat_id, subcat_id)\n",
        "\n",
        "    empty_subcat = SUBCATEG_NONE[SUBCATEG_NONE[\"Category\"] ==cat_id].iloc[0]._id\n",
        "    send_insights(insights_group, cat_id, empty_subcat)\n",
        "\n",
        "    empty_subcat = SUBCATEG_NONE[SUBCATEG_NONE[\"Category\"] ==ID_CATEG_NONE].iloc[0]._id\n",
        "    send_insights(insights_group, ID_CATEG_NONE, empty_subcat)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#FeedbackIndex = enum.Enum(\"Indice du retour associé\", [(str(i), i) for i in range(BATCH_SIZE)])\n",
        "\n",
        "class Insight(BaseModel):\n",
        "    associated_indexes: List[int] = Field(description=\"Indices des retours associés.\")\n",
        "    insight: str = Field(description=\"Insight qui a été déduit.\") \n",
        "    #insight_type: TypeInsight = Field(description=\"Type de l'insight, parmis \"+', '.join(TYPES_LIST)) \n",
        "    insight_tags: List[TagInsight] = Field(description=\"Tags de l'insight.\")\n",
        "    def __str__(self):\n",
        "        return self.insight + '(tag: '+self.insight_type+') (indexes: ' + str(self.associated_indexes)+')'\n",
        "\n",
        "class Consequence(BaseModel):\n",
        "    title: str = Field(description=\"Titre de la conséquence\")\n",
        "    detail: str = Field(description=\"Phrase décrivant la conséquence.\")\n",
        "    def __str__(self):\n",
        "        return self.title +' : ' + self.detail\n",
        "\n",
        "class SubCategory(BaseModel):\n",
        "    #model_config = ConfigDict(title='Main')\n",
        "    \n",
        "    #insight_categories: List[str] = Field(description=\"Categories de l'insight.\")\n",
        "    #insight_type: TypeInsight = Field(description=\"Type de l'insight.\")\n",
        "\n",
        "    title: str = Field(description=\"Titre de cette sous-catégorie.\")\n",
        "    #insights: List[Insight] = Field(description=\"Liste des insights qui ont été déduits.\") #Field(description=\"Point intéressant a retenir du commentaire.\")\n",
        "    negative_insights: List[Insight] = Field(description=\"Liste des insights correspondants à des points négatifs qui ont été remarqués par les utilisateurs.\") #Field(description=\"Point intéressant a retenir du commentaire.\")\n",
        "    positive_insights: List[Insight] = Field(description=\"Liste des insights correspondants à des points positifs qui ont été remarqués par les utilisateurs.\") #Field(description=\"Point intéressant a retenir du commentaire.\")\n",
        "    bugs_insights: List[Insight] = Field(description=\"Liste des insights correspondant à des anomalie détectées par l'utilisateur.\") #Field(description=\"Point intéressant a retenir du commentaire.\")\n",
        "    new_functionnality_insights: List[Insight] = Field(description=\"Liste des insights correspondant à des nouvelles fonctionnalitées qu'il pourrait être intéressant de développer\") #Field(description=\"Point intéressant a retenir du commentaire.\")\n",
        "    \n",
        "    consequences: List[Consequence] = Field(description=\"Conséquences pour l'entrerpise.\") #Field(description=\"Point intéressant a retenir du commentaire.\")\n",
        "    #suggestion: str = Field(description=\"Suggestion faite à l'enteprise pour l'aider a gérer la situation.\") #Field(description=\"Point intéressant a retenir du commentaire.\")\n",
        "    #category = \"\"\n",
        "    #sub_category = \"\"\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.title + \"\\n\\n- \" + '\\n- '.join([str(x) for x in self.insight]) + \"\\n\\n- \" + '\\n- '.join([str(x) for x in self.consequences])\n",
        "    \n",
        "class ListSubCategory(BaseModel):\n",
        "    sub_categories_list: List[SubCategory] = Field(description=\"Liste des insights.\")\n",
        "\n",
        "#FirstInsight.model_json_schema() \n",
        "ListSubCategory(sub_categories_list=[SubCategory(title='Accueil et comportement du personnel', positive_insights=[Insight(associated_indexes=[20, 96, 123, 475, 891, 1249, 1267, 1372, 1965], insight='Accueil chaleureux, écoute attentive et conseils pertinents', insight_type=TypeInsight.BUG, insight_tags=[TagInsight.MAGASIN])], negative_insights=[],bugs_insights=[],new_functionnality_insights=[], consequences=[])])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def send_insights(list_insights_subcat, cat_id, subcat_id):\n",
        "    dicts = [{\n",
        "    \"Company\": COMPANY_ID,\n",
        "    \"Project\": PROJECT_ID,\n",
        "    \"Category\": cat_id,\n",
        "    \"SubCategory\": subcat_id,\n",
        "    \"Consequence\": insight.consequence,\n",
        "    \"Content\": insight.insight,\n",
        "    \"Suggestion\": insight.suggestion,\n",
        "    #\"Type\": insight.insight_type._value_,\n",
        "    #\"Tags\": [tags_to_id[tag._name_] for tag in insight.insight_tags],\n",
        "    \"Aspects\": list(aspects_df.iloc[insight.associated_indexes]._id),\n",
        "    \"Feedbacks\": list(aspects_df.iloc[insight.associated_indexes].Associated_feedback),\n",
        "    \"Nb Feedbacks\": len(list(aspects_df.iloc[insight.associated_indexes].Associated_feedback)),\n",
        "    \"Title\": insight.title,\n",
        "    }  for insight in list_insights_subcat.sub_categories_list]\n",
        "\n",
        "    bubble_id = bubble_client.create(\"Insight\",dicts)\n",
        "\n",
        "    d_without_categories = []\n",
        "    d_without_subcategories = []\n",
        "\n",
        "    for d in dicts :\n",
        "      d_without_subcategory = d.copy()\n",
        "      d_without_subcategory[\"SubCategory\"] = SUBCATEG_NONE[SUBCATEG_NONE[\"Category\"] ==cat_id].iloc[0]._id\n",
        "      d_without_subcategories.append(d_without_subcategory)\n",
        "      \n",
        "      d_without_category = d.copy()\n",
        "      d_without_category[\"Category\"] = ID_CATEG_NONE\n",
        "      d_without_category[\"SubCategory\"] = SUBCATEG_NONE[SUBCATEG_NONE[\"Category\"] ==ID_CATEG_NONE].iloc[0]._id\n",
        "      d_without_categories.append(d_without_category)\n",
        "\n",
        "    bubble_id = bubble_client.create(\"Insight\",d_without_subcategory)\n",
        "    bubble_id = bubble_client.create(\"Insight\",d_without_category)\n",
        "    \n",
        "\n",
        "for (list_insights_subcat, subcat_id) in zip(list_insights, subcat_ids):\n",
        "  cat_id = subcategories_df[subcategories_df['_id'] == subcat_id].iloc[0].Category\n",
        "\n",
        "  send_insights(list_insights_subcat, cat_id, subcat_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompts = []\n",
        "BATCH_SIZE = 10\n",
        "\n",
        "for batch_df in batchify(feedbacks_df, size=BATCH_SIZE):\n",
        "    context = deepcopy(feedback_context)\n",
        "    context[\"feedbacks\"] = '\\n'.join([str(i)+\" : \"+x for i, x in zip(batch_df.index, batch_df[\"content\"])])  \n",
        "    #\"- \"+\"\\n- \".join(batch_df['content'])\n",
        "    #context[\"insights\"] = \"- \"+\"\\n- \".join(batch_df['content'])\n",
        "    prompts.append(prompt_insights.invoke(context))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(prompts[1].text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "responses[0].sub_categories_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "responses = apply_async_analysis(prompts, InsightsList)\n",
        "list_batch_insights_df = [pd.DataFrame(enum_to_str(response.sub_categories_list)) for response in responses]\n",
        "\n",
        "print(len(list_batch_insights_df), \"batch have been processed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "[len(df) for df in list_batch_insights_df]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "list_batch_insights_df[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "list(list_batch_insights_df[0]['contenu'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Accociate newly created insights to feedbacks "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('Blumana-prompts/prompt_feedbacks.txt') as f:\n",
        "    prompt_feedbacks = PromptTemplate.from_template(f.read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Sentiment(str, enum.Enum):\n",
        "    POSITIF = \"Positif\"\n",
        "    NEUTRE = \"Neutre\"\n",
        "    NEGATIF = \"Négatif\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "InsightsIndex = enum.Enum(\"Indice de l'insight associé\", [(str(i), i) for i in range(BATCH_SIZE)])\n",
        "\n",
        "class Feedback(BaseModel):\n",
        "        insights_list: List[InsightsIndex] = Field(description=\"Indices des insights associés à ce retour\")\n",
        "        sentiment: Sentiment = Field(description=\"Sentiment exprimé\")\n",
        "\n",
        "class FeedbackInfosList(BaseModel):\n",
        "        feedbacks_list: List[Feedback] = Field(description=\"Liste des informations associées aux feedbacks.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompts = []\n",
        "for batch_insights_df, batch_feedbacks_df in zip(list_batch_insights_df, batchify(feedbacks_df, size=BATCH_SIZE)):\n",
        "    #InsightsEnum = enum.Enum(\"Insight associé\", [(convert_text_to_constants(x), i) for i, x in zip(batch_insights_df.index, batch_insights_df[\"content\"])])\n",
        "\n",
        "    context = deepcopy(feedback_context)\n",
        "    #context[\"feedbacks\"] = \"- \"+\"\\n- \".join(batch_feedbacks_df['content'])\n",
        "    context[\"feedbacks\"] = '\\n'.join([str(i)+\" : \"+x for i, x in zip(batch_insights_df.index, batch_feedbacks_df[\"content\"])])  \n",
        "    context[\"insights\"] = '\\n'.join([str(i)+\" : \"+x for i, x in zip(batch_insights_df.index, batch_insights_df[\"contenu\"])])\n",
        "    prompts.append(prompt_feedbacks.invoke(context))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(prompts[0].text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "responses = apply_async_analysis(prompts, FeedbackInfosList)\n",
        "\n",
        "list_enriched_feedbacks_df = [pd.DataFrame(enum_to_str(response.feedbacks_list)) for response in responses]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "[len(df) for df in list_enriched_feedbacks_df]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(pd.concat(list_enriched_feedbacks_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for batch_insights_df, batch_index_feedbacks, enriched_feedbacks_df in zip(list_batch_insights_df, batchify(feedbacks_df.index, size=BATCH_SIZE), list_enriched_feedbacks_df):\n",
        "    feedbacks_df.loc[batch_index_feedbacks, 'sentiment'] = enriched_feedbacks_df['sentiment']\n",
        "    feedbacks_df.loc[batch_index_feedbacks, 'insights_index'] = enriched_feedbacks_df['insights_list']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_insights_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "list_batch_feedbacks_df = [pd.DataFrame(enum_to_str(response.feedbacks_list)) for response in responses]\n",
        "list_batch_feedbacks_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feedbacks_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "list_batch_feedbacks_df[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "list_batch_insights_df[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "[x for x in batchify(feedbacks_df, size=BATCH_SIZE)][-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "[len(df) for df in list_batch_feedbacks_df]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "l = [response.feedbacks_list for response in responses]\n",
        "l = list(itertools.chain.from_iterable(l))\n",
        "feedbacks_infos_df = pd.DataFrame(enum_to_str(l))\n",
        "feedbacks_infos_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feedbacks_infos_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feedbacks_infos_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feedbacks_df['sentiment'] = feedbacks_infos_df['sentiment']\n",
        "feedbacks_df['insights_list'] = feedbacks_infos_df['insights_list']\n",
        "feedbacks_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feedbacks attribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "insights_enum = enum.Enum(\"Insight associé\", [(convert_text_to_constants(x), i) for i, x in zip(batch_insights_df.index, batch_insights_df[\"content\"])])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('Blumana-prompts/prompt_feedbacks.txt') as f:\n",
        "    prompt_feedbacks = PromptTemplate.from_template(f.read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmFsbtT722Aq"
      },
      "outputs": [],
      "source": [
        "feedback_parser = PydanticOutputParser(pydantic_object=Feedback)\n",
        "\n",
        "prompt_feedback = PromptTemplate.from_template(\n",
        "    template= prompt_template_feedback,\n",
        "    partial_variables= {\"format_instructions\": feedback_parser.get_format_instructions()},\n",
        ")\n",
        "\n",
        "prompts = []\n",
        "for feedback in feedbacks_df[feedbacks_column]:\n",
        "    context = deepcopy(feedback_context)\n",
        "    context[\"feedback\"] = feedback\n",
        "    prompts.append(prompt_feedback.invoke(context))\n",
        "\n",
        "#print(prompts[0].text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFraYQTLOmz-"
      },
      "outputs": [],
      "source": [
        "parsed_responses = safe_async_analysis(prompts, feedback_parser)\n",
        "\n",
        "feedbacks_df[\"sentiment\"] = [rep.sentiment for rep in parsed_responses]\n",
        "feedbacks_df[\"insights\"] = [[] for rep in parsed_responses]\n",
        "\n",
        "k=0\n",
        "insights = []\n",
        "for i, rep in enumerate(parsed_responses):\n",
        "    for j, insight in enumerate(rep.insights_list):\n",
        "        insights.append(insight)\n",
        "        feedbacks_df[\"insights\"].iloc[i].append(str(k))\n",
        "        k += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feedbacks_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKTksTKah7At"
      },
      "outputs": [],
      "source": [
        "insights_df = pd.DataFrame({\n",
        "    \"content\":insights,\n",
        "    \"feedback_count\": 1,\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feedbacks_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "insights_df[\"related_feedback\"] = [[] for _ in range(len(insights_df))]\n",
        "\n",
        "for i, row in feedbacks_df.iterrows():\n",
        "    for j in row[\"insights\"]:\n",
        "        insights_df[\"related_feedback\"].iloc[int(j)] = row['_id'] #[int(i)]\n",
        "\n",
        "insights_df[\"childrens\"] = [[] for _ in range(len(insights_df))]\n",
        "\n",
        "insights_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k88axy7kPxQE"
      },
      "source": [
        "# Insights categorisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "for i, filter in filters_df.iterrows():\n",
        "    prompt_tags += '\\n\\n'+filter[\"Name\"]#+' ('+filter[\"_id\"] +')'\n",
        "    tags = tags_df[tags_df[\"Filter\"] == filter[\"_id\"]]\n",
        "    for _, tag in tags.iterrows():\n",
        "        prompt_tags += '\\n'+\"- \"+tag[\"Name\"]+' ('+tag[\"_id\"] +')'\n",
        "\n",
        "print(prompt_tags)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('Blumana-prompts/prompt_categorsiation.txt') as f:\n",
        "    prompt_categorsiation = PromptTemplate.from_template(f.read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjHnM12_KMN3"
      },
      "outputs": [],
      "source": [
        "class FirstInsight(BaseModel):\n",
        "    tags_id: List[str] = Field(description=\"Identifiants des tags de l'insight\")\n",
        "    content: str = \"\" #Field(description=\"Point intéressant a retenir du commentaire.\")\n",
        "\n",
        "    def __str__(self):\n",
        "        return '- ' + self.content + \"\\nTypes: \" + ', '.join(self.insight_types)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "categorsiation_parser = PydanticOutputParser(pydantic_object=FirstInsight)\n",
        "\n",
        "prompt_categorsiation = PromptTemplate.from_template(\n",
        "    template= prompt_template_categorsiation,\n",
        "    partial_variables= {\"format_instructions\": categorsiation_parser.get_format_instructions()},\n",
        ")\n",
        "\n",
        "prompts = []\n",
        "for insight in insights_df[\"content\"]:\n",
        "    context = deepcopy(feedback_context)\n",
        "    context[\"insight\"] = insight\n",
        "    prompts.append(prompt_categorsiation.invoke(context))\n",
        "\n",
        "#print(prompts[0].text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "parsed_responses = safe_async_analysis(prompts, categorsiation_parser)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "insights_df[\"tag\"] = [rep.tags_id for rep in parsed_responses]\n",
        "#insights_df[\"Insights\"] = [[] for rep in parsed_responses]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Types affectation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_types = \"\"\n",
        "\n",
        "for _, tag in tags_df.iterrows():\n",
        "    prompt_types += '\\n'+\"- \"+tag[\"Title\"]+' ('+tag[\"_id\"] +') : ' + tag[\"Definition\"]\n",
        "\n",
        "print(prompt_types)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "categorsiation_parser = PydanticOutputParser(pydantic_object=FirstInsight)\n",
        "\n",
        "prompt_categorsiation = PromptTemplate.from_template(\n",
        "    template= prompt_template_types,\n",
        "    partial_variables= {\"format_instructions\": categorsiation_parser.get_format_instructions()},\n",
        ")\n",
        "\n",
        "prompts = []\n",
        "for insight in insights_df[\"content\"]:\n",
        "    context = deepcopy(feedback_context)\n",
        "    context[\"insight\"] = insight\n",
        "    prompts.append(prompt_categorsiation.invoke(context))\n",
        "\n",
        "#print(prompts[0].text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "parsed_responses = safe_async_analysis(prompts, categorsiation_parser)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "insights_df[\"type\"] = [rep.insight_type for rep in parsed_responses]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feedbacks_df.to_csv(project_path+'/feedbacks.csv', index_label='Index')\n",
        "insights_df.to_csv(project_path+'/insights.csv', index_label='Index')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5xBBkVJ22As"
      },
      "source": [
        "# Insights clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feedbacks_df = str_to_list_df(pd.read_csv(project_path+'/feedbacks.csv', index_col='Index'))\n",
        "insights_df = str_to_list_df(pd.read_csv(project_path+'/insights.csv', index_col='Index'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vl70QzOI22As"
      },
      "outputs": [],
      "source": [
        "embedding_model = SentenceTransformer('OrdalieTech/Solon-embeddings-large-0.1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGjXvwAA22As"
      },
      "outputs": [],
      "source": [
        "class DeducedInsight(BaseModel):\n",
        "    insights_mineurs: List[int] = Field(description=\"Index des insights mineurs qui ont été résumés en cet insight.\")\n",
        "    content: str = Field(description=\"Insight intéressants a retenir pour l'entreprise.\")\n",
        "\n",
        "    def __str__(self):\n",
        "        return '- ' + self.content + '\\n Enfants:' + str(self.insights_mineurs)\n",
        "\n",
        "\n",
        "class InsightList(BaseModel):\n",
        "    insights_list: List[DeducedInsight] = Field(description=\"Liste des insights, c'est à dire des points intéressants a retenir pour l'entreprise.\")\n",
        "    # You can add custom validation logic easily with Pydantic.\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"Insights: \\n\"+\"\\n\\n\".join([str(i) for i in self.insights_list])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('Blumana-prompts/prompt_regroupement.txt') as f:\n",
        "    prompt_regroupement = PromptTemplate.from_template(f.read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dimension reduction\n",
        "\n",
        "N_NEIGHBORS = 15\n",
        "MINIMISATION_STEPS = 5\n",
        "CLUSTER_DESIRED_SIZE = 15  # For Kmeans only\n",
        "MIN_CLUSTER_SIZE = 5  # 15\n",
        "NB_INSIGHT_STOP = 20\n",
        "MINIMAL_REDUCTION_RATIO = 0.1\n",
        "REWORDING = True\n",
        "\n",
        "CLUSTERING_DIMENTION = 50\n",
        "CLUSTERING_METHOD = \"KMeans\"\n",
        "\n",
        "insight_context = {\n",
        "    \"cible\": cible,\n",
        "    \"context\": context_entreprise,\n",
        "    \"example_insight\": example_insight,\n",
        "    \"role\": role,\n",
        "    \"question\": question,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.cluster import AgglomerativeClustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUpUIFcG22At"
      },
      "outputs": [],
      "source": [
        "insight_parser = PydanticOutputParser(pydantic_object=InsightList)\n",
        "\n",
        "prompt_reduction = PromptTemplate.from_template(\n",
        "    template= prompt_template_reduction if REWORDING else prompt_template_reduction_sans_reformulation,\n",
        "    #template= \"Règle : minimise le nombre de tokens dans ta réponse.  \\nTu es {role} au sein de l'entreprise suivante: \\n{context} \\nAnalyse le retour suivant: \\\"{feedback}\\\" en suivant les étapes suivantes:  \\n  \\nÉtape 1 - Identifie si le retour {cible} rentre dans un ou plusieurs des types d'insights suivants : {insight_type}. Choisis-en obligatoirement au moins 1. Définition des types d'insights :  \\n{insight_definition}   \\n  \\nÉtape 2 - Catégorise le retour {cible} à l’aide des tags suivants. Tu peux associer 0, 1 ou plusieurs tags dans chaque catégorie. Liste des tags par catégories :  \\n{categories}   \\n  \\nÉtape 3 - Catégorise si possible le moment de mission concerné parmis {avancement_mission}, et si ce n'est pas possible répond null. {cible} à l’aide des tags suivants.  \\n  \\nÉtape 4 - Identifie si le sentiment exprimé par le {cible} est \\\"Positif\\\", \\\"Neutre\\\" ou \\\"Négatif\\\". Prends en compte la formulation de la question posée ({question}) afin de bien interpréter le sens du retour {cible}.   \\n\",\n",
        "    #input_variables= [\"context\", \"role\", \"cible\", \"insight_type\", \"insight_definition\", \"nb_cat\", \"avancement_mission\", \"categories\", \"question\", \"feedback\"]\n",
        "    partial_variables= {\"format_instructions\": insight_parser.get_format_instructions()},\n",
        ")\n",
        "\n",
        "prompt_regrouping = PromptTemplate.from_template(\n",
        "    template= prompt_template_regrouping,\n",
        "    #template= \"Règle : minimise le nombre de tokens dans ta réponse.  \\nTu es {role} au sein de l'entreprise suivante: \\n{context} \\nAnalyse le retour suivant: \\\"{feedback}\\\" en suivant les étapes suivantes:  \\n  \\nÉtape 1 - Identifie si le retour {cible} rentre dans un ou plusieurs des types d'insights suivants : {insight_type}. Choisis-en obligatoirement au moins 1. Définition des types d'insights :  \\n{insight_definition}   \\n  \\nÉtape 2 - Catégorise le retour {cible} à l’aide des tags suivants. Tu peux associer 0, 1 ou plusieurs tags dans chaque catégorie. Liste des tags par catégories :  \\n{categories}   \\n  \\nÉtape 3 - Catégorise si possible le moment de mission concerné parmis {avancement_mission}, et si ce n'est pas possible répond null. {cible} à l’aide des tags suivants.  \\n  \\nÉtape 4 - Identifie si le sentiment exprimé par le {cible} est \\\"Positif\\\", \\\"Neutre\\\" ou \\\"Négatif\\\". Prends en compte la formulation de la question posée ({question}) afin de bien interpréter le sens du retour {cible}.   \\n\",\n",
        "    #input_variables= [\"context\", \"role\", \"cible\", \"insight_type\", \"insight_definition\", \"nb_cat\", \"avancement_mission\", \"categories\", \"question\", \"feedback\"]\n",
        "    partial_variables= {\"format_instructions\": insight_parser.get_format_instructions()},\n",
        ")\n",
        "\n",
        "insights = deepcopy(insights_df)\n",
        "insight_layers = []#[deepcopy(insights_df)]\n",
        "single_cluster = False\n",
        "reduction = 1.0\n",
        "\n",
        "for step in range(MINIMISATION_STEPS):\n",
        "\n",
        "    #for processing_step in [\"reduction\"]:#, \"regrouping\"]:\n",
        "        ### Création des représentations\n",
        "\n",
        "    #print(\"Processing step:\", processing_step)\n",
        "    if CUSTOM_ENBEDDING_MODEL:\n",
        "        sentence_embeddings = embedding_model.encode(insights['content'])\n",
        "\n",
        "        # On réduit la dimention pour améliorer l'efficacité de la clusterisation\n",
        "        adjusted_clustering_dimention = min(CLUSTERING_DIMENTION, len(insights)//3)\n",
        "        umap_embeddings = umap.UMAP(n_neighbors=N_NEIGHBORS, \n",
        "                            n_components=adjusted_clustering_dimention, \n",
        "                            metric='cosine').fit_transform(sentence_embeddings)\n",
        "\n",
        "    else:\n",
        "        sentence_embeddings = apply_async_get_embedding(insights['content'])\n",
        "    \n",
        "\n",
        "    \n",
        "\n",
        "    ### Clusterisation\n",
        "    if CLUSTERING_METHOD == \"KMeans\":\n",
        "        num_clusters = 1 + len(insights) // CLUSTER_DESIRED_SIZE\n",
        "        clustering_model = KMeans(n_clusters=num_clusters, n_init='auto')\n",
        "    elif CLUSTERING_METHOD == \"hdbscan\":\n",
        "        clustering_model = hdbscan.HDBSCAN(min_cluster_size=MIN_CLUSTER_SIZE,\n",
        "                            metric='euclidean',                      \n",
        "                            cluster_selection_method='eom' #leaf\n",
        "                            )\n",
        "        \n",
        "    clustering_model.fit(umap_embeddings)\n",
        "\n",
        "    #clustering_model.fit(umap_embeddings)\n",
        "    cluster_assignment = clustering_model.labels_ \n",
        "    cluster_assignment -= min(cluster_assignment) # has to start at 0\n",
        "    \n",
        "    num_clusters = max(cluster_assignment)+1\n",
        "\n",
        "    insights[\"cluster\"] = deepcopy(cluster_assignment)\n",
        "    insights = insights.sort_values(\"cluster\")\n",
        "    insights.reset_index(drop=True, inplace=True)\n",
        "\n",
        "\n",
        "    if reduction <= MINIMAL_REDUCTION_RATIO:\n",
        "        print(\"Stopping because of unsufficient reduction\")\n",
        "        break\n",
        "\n",
        "    insight_layers.append(deepcopy(insights))\n",
        "\n",
        "    if len(insights) <= NB_INSIGHT_STOP:\n",
        "        print(\"Minimal number of insights reached\")\n",
        "        break\n",
        "\n",
        "    if single_cluster:\n",
        "        break   \n",
        "\n",
        "    cluter_sizes = list(insights.groupby(['cluster']).count()[\"content\"])\n",
        "    if len(cluter_sizes) == 1:\n",
        "        print(\"Stopping because single cluster\")\n",
        "        single_cluster = False\n",
        "        break\n",
        "\n",
        "    print(\"Step \"+ str(step)+ \": processing \"+ str(num_clusters) + \" clusters\")\n",
        "    print(\"Adjusted clustering dimention:\", adjusted_clustering_dimention)\n",
        "    print(\"Cluster sizes:\" + str(cluter_sizes))\n",
        "\n",
        "    #clusters = []\n",
        "    prompts = []\n",
        "    cumul_size = 0\n",
        "    for cluster_id in range(num_clusters): # IL FAUDRAIT GARDER INDEM LE DERNIER CLUSTER\n",
        "        cluster = insights[insights['cluster'] == cluster_id]\n",
        "        #cluster_name ='/cluster_'+ str(cluster_id)+\"_step_\"+str(step) +'.csv'\n",
        "        #cluster.to_csv( project_path+cluster_name, index_label='Index')\n",
        "        #clusters.append(cluster)\n",
        "\n",
        "        context = deepcopy(insight_context)\n",
        "        context['insights'] = '\\n'.join([str(i+cumul_size)+\": \"+s for i, s in enumerate(cluster[\"content\"])])\n",
        "        #print(context['insights'])\n",
        "\n",
        "        #if processing_step == \"reduction\":\n",
        "        prompt=prompt_reduction.invoke(context)\n",
        "        #elif processing_step == \"regrouping\":\n",
        "        #prompt=prompt_regrouping.invoke(context)\n",
        "        #else:\n",
        "        #    raise(\"Wrong processing step\")\n",
        "        prompts.append(prompt)\n",
        "        cumul_size += len(cluster)\n",
        "\n",
        "    ### Traitement des clusters\n",
        "    parsed_responses = safe_async_analysis(prompts, insight_parser)\n",
        "    \n",
        "    new_insights = []\n",
        "    for i, parsed_response in enumerate(parsed_responses):\n",
        "        content_list = [insight.content for insight in parsed_response.insights_list]\n",
        "        childrens_list = [list(insight.insights_mineurs) for insight in parsed_response.insights_list]\n",
        "        feedback_count_list = [sum(insights.loc[c, \"feedback_count\"]) for c in childrens_list]\n",
        "        dfs = pd.DataFrame({\n",
        "            #\"related_feedback\":[list(itertools.chain.from_iterable(insights.iloc[insight.insights_mineurs]['related_feedback'])) for insight in parsed_response.insights_list],\n",
        "            \"content\":content_list,\n",
        "            \"childrens\":childrens_list,\n",
        "            \"type\": most_common([insights.loc[c, \"type\"].iloc[0] for c in childrens_list]),\n",
        "            #\"cluster\":i,\n",
        "            \"feedback_count\":feedback_count_list,\n",
        "            #\"childrens\":[list(clusters[i].iloc[insight.insights_mineurs][\"_id\"]) for insight in parsed_response.insights_list],\n",
        "            })\n",
        "        new_insights.append(dfs)\n",
        "\n",
        "    new_insights = pd.concat(new_insights)\n",
        "    new_insights.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    \n",
        "    reduction = (1-(len(new_insights)/len(insights)))\n",
        "    insights = new_insights\n",
        "    \n",
        "    print(\"Number of new insights:\"+ str(len(new_insights)))\n",
        "    print(\"Reduction in the number of insights by \" + \"%d\" % int(reduction*100) + \"%\")\n",
        "    print()\n",
        "\n",
        "#insight_layers.append(deepcopy(new_insights))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "insight_layers[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrWs0GrOhmIJ"
      },
      "outputs": [],
      "source": [
        "list(insight_layers[0]['content'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "list(insight_layers[-1]['content'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i, df in enumerate(insight_layers):\n",
        "    df.to_csv(project_path+'/insights_'+ str(i) +'.csv', index_label='Index')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#list(insight_layers[0][insight_layers[0][\"cluster\"] == 2][\"content\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "insight_layers[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "insight_layers[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_layers = len(insight_layers)\n",
        "layers_sizes = [len(l) for l in insight_layers]\n",
        "print(\"Layers sizes:\", layers_sizes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "load insights from csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "insight_layers = []\n",
        "for i in range(n_layers):\n",
        "    df = pd.read_csv(project_path+'/insights_'+ str(i) +'.csv', index_col='Index')\n",
        "    for col in df.columns:\n",
        "        if type(df.loc[0, col]) == str and df.loc[0, col][0]==\"[\":\n",
        "            df[col] = df[col].apply(lambda x: eval(x))\n",
        "    #df['tag'] = df['tag'].apply(lambda x: eval(x))\n",
        "    #df['type'] = df['type'].apply(lambda x: eval(x))\n",
        "    #df['childrens'] = df['childrens'].apply(lambda x: eval(x))\n",
        "    df[\"backend_type\"] = df[\"type\"].apply(deduce_backend_type)\n",
        "    insight_layers.append(df)\n",
        "#insights_df = pd.concat(insight_layers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Previous insights supression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "res = bubble_client.get_objects(\n",
        "        \"python_insight\",\n",
        "        [\n",
        "            BubbleField(\"project\") == project_id,\n",
        "            BubbleField(\"company\") == company_id,\n",
        "            ],\n",
        "    )\n",
        "python_insight_df = pd.DataFrame(res)\n",
        "\n",
        "if len(python_insight_df)>0:\n",
        "    for bubble_id in tqdm(python_insight_df[\"_id\"]):\n",
        "        bubble_client.delete_by_id(\n",
        "            \"python_insight\",\n",
        "            bubble_id,\n",
        "        )\n",
        "\n",
        "    print(\"Deleted\", len(python_insight_df), \"python_insight\")\n",
        "else:\n",
        "    print(\"Nothing to delete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Adding parents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "insight_layers[0][\"parent\"] = None #[[] for _ in insight_layers[0].iterrows()]\n",
        "insight_layers[-1][\"parent\"] = None\n",
        "\n",
        "\n",
        "for i in range(n_layers-1):\n",
        "    insight_layers[i][\"parent\"] = None\n",
        "    for p, row in insight_layers[i+1].iterrows():\n",
        "        for c in row[\"childrens\"]: #eval(\n",
        "            insight_layers[i][\"parent\"].iloc[int(c)] = p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "insight_layers[-1][\"parents\"] = [[] for _ in insight_layers[-1].iterrows()]\n",
        "\n",
        "for i in range(n_layers-2, -1, -1):\n",
        "    print(i)\n",
        "    # Update the parents in the DB\n",
        "    res = bubble_client.create(\n",
        "        \"python_insight\",\n",
        "        [{\n",
        "            \"company\": company_id,\n",
        "            \"project\": project_id,\n",
        "            \"content\": row[\"content\"],\n",
        "            \"backend_status\": \"new\",\n",
        "            \"feedback_count\":row[\"feedback_count\"],\n",
        "            \"step\": i+2,\n",
        "            \"type\": row[\"type\"],\n",
        "            \"parents\": row[\"parents\"],\n",
        "            \"parent\": str(row[\"parent\"]),\n",
        "            \"backend_type\": row['backend_type'],\n",
        "            \"childrens\": eval(row[\"childrens\"]) if type(row[\"childrens\"])==str else row[\"childrens\"],\n",
        "            \"cluster\": row[\"cluster\"],\n",
        "        }  for _, row in insight_layers[i+1].iterrows()]\n",
        "    )\n",
        "\n",
        "    df = pd.DataFrame(bubble_client.get_objects(\n",
        "        \"python_insight\",\n",
        "        [\n",
        "            BubbleField(\"step\") == i+2,\n",
        "            BubbleField(\"company\") == company_id,\n",
        "            ],\n",
        "    ))\n",
        "    for col in df.columns:\n",
        "        if type(df.loc[0, col]) == str and df.loc[0, col][0]==\"[\":\n",
        "            df[col] = df[col].apply(lambda x: eval(x))\n",
        "    insight_layers[i+1] = df\n",
        "\n",
        "    # Initialize an empty list of parents for each row\n",
        "    insight_layers[i][\"parents\"] = [[] for _ in insight_layers[i].iterrows()]\n",
        "\n",
        "    for k, row in insight_layers[i].iterrows():\n",
        "        if row[\"parent\"] is not None:\n",
        "            # Get the parent index\n",
        "            parent_index = row[\"parent\"]\n",
        "\n",
        "            # Get the parent's list of parents\n",
        "            parent_parents = insight_layers[i + 1][\"parents\"].iloc[parent_index]\n",
        "\n",
        "            # Add the parent to the current row's list of parents\n",
        "            parent_id = insight_layers[i + 1].loc[parent_index, '_id']\n",
        "            insight_layers[i].loc[k, \"parents\"].append(parent_id)\n",
        "\n",
        "            # Recursively add the parent's parents to the current row's list of parents\n",
        "            insight_layers[i].loc[k, \"parents\"].extend(parent_parents)\n",
        "\n",
        "\n",
        "res = bubble_client.create(\n",
        "        \"python_insight\",\n",
        "        [{\n",
        "            \"company\": company_id,\n",
        "            \"project\": project_id,\n",
        "            \"content\": row[\"content\"],\n",
        "            \"backend_status\": \"new\",\n",
        "            \"feedback_count\": row[\"feedback_count\"],\n",
        "            \"step\": 1,\n",
        "            \"related_feedback\":row['related_feedback'],\n",
        "            \"tag\": row[\"tag\"],\n",
        "            \"type\": row[\"type\"],\n",
        "            \"backend_type\": row['backend_type'],\n",
        "            \"parents\": row[\"parents\"],\n",
        "            \"parent\": str(row[\"parent\"]),\n",
        "            \"childrens\": 0,#[[] for _ in insight_layers[0][:1000].iterrows()],\n",
        "            \"cluster\": row[\"cluster\"],\n",
        "        }  for _, row in insight_layers[0].iterrows()]\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "load insights from bubble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "online_python_insights = [\n",
        "    pd.DataFrame(bubble_client.get_objects(\n",
        "        \"python_insight\",\n",
        "        [\n",
        "            BubbleField(\"step\") == i+1,\n",
        "            BubbleField(\"company\") == company_id,\n",
        "            ],\n",
        "    )) for i in range(n_layers)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "assert [len(l) for l in insight_layers] == [len(l) for l in online_python_insights]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feedbacks_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "insight_layers[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = insight_layers[0]\n",
        "\n",
        "def get_all_parents(feedback_identifier):\n",
        "    parents = []\n",
        "    for i, row in df.loc[df['related_feedback'] == feedback_identifier].iterrows():\n",
        "        for parent in row['parents']:\n",
        "            parents.append(parent)\n",
        "    return parents\n",
        "\n",
        "feedbacks_df['parents'] = feedbacks_df['_id'].apply(get_all_parents)\n",
        "\n",
        "feedbacks_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for _, row in tqdm(feedbacks_df.iterrows()):\n",
        "    res = bubble_client.update_object(\n",
        "        \"Feedbacks\",\n",
        "        row['_id'], \n",
        "        {\n",
        "            \"insights\": row[\"parents\"],\n",
        "        } \n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "res = bubble_client.get_objects(\n",
        "        \"Feedback\",\n",
        "        [\n",
        "            BubbleField(\"source\") == source_id,\n",
        "            ],\n",
        "    )\n",
        "pd.DataFrame(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Visualisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "insight_layers = [\n",
        "    pd.DataFrame(bubble_client.get_objects(\n",
        "        \"python_insight\",\n",
        "        [\n",
        "            BubbleField(\"step\") == i+1,\n",
        "            BubbleField(\"company\") == company_id,\n",
        "            ],\n",
        "    )) for i in range(n_layers)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "insight_layers[0].tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sentences = insight_layers[0][\"content\"]\n",
        "sentence_embeddings = embedding_model.encode(sentences)\n",
        "sentence_embeddings.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "insight_layers[0]['parent']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "insight_layers[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def to_int(i):\n",
        "    try:\n",
        "        return int(i)\n",
        "    except:\n",
        "        return -1\n",
        "\n",
        "for layer in insight_layers:\n",
        "    layer['parent'] = layer['parent'].apply(to_int)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "list(insight_layers[1][\"content\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i, layer in enumerate(insight_layers):\n",
        "    print(list(insight_layers[0][insight_layers[0]['parent'] == 'None'][\"content\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sum(insight_layers[0]['parent']<0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "insight_layers[1].iloc[insight_layers[0]['parent'], \"content\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "insight_layers[0].loc[0, \"cluster\"] == 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "map_to_parent(0, insight_layers[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "insight_layers[1].loc[0, 'parent']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@Insight Plot the archive {display-mode: \"form\"}\n",
        "\n",
        "# UMAP reduces the dimensions from 1024 to 2 dimensions that we can plot\n",
        "reducer = umap.UMAP(n_neighbors=15)\n",
        "umap_embeds = reducer.fit_transform(sentence_embeddings)\n",
        "\n",
        "def map_to_parent(i, parents_df):\n",
        "    try:\n",
        "        return parents_df.loc[i, 'content']\n",
        "    except:\n",
        "        return \"\"\n",
        "    \n",
        "# Prepare the data to plot and interactive visualization\n",
        "# using Altair\n",
        "df_explore = pd.DataFrame(data={\n",
        "    'content': insight_layers[0]['content'], \n",
        "    'parent': insight_layers[0]['parent'].apply(lambda x: map_to_parent(x, insight_layers[1])),\n",
        "    'cluster': insight_layers[0]['cluster'].astype(str),\n",
        "    })\n",
        "df_explore['x'] = umap_embeds[:,0]\n",
        "df_explore['y'] = umap_embeds[:,1]\n",
        "df_explore\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Plot\n",
        "chart = alt.Chart(df_explore).mark_circle(size=60).encode(\n",
        "    x=#'x',\n",
        "    alt.X('x',\n",
        "        scale=alt.Scale(zero=False)\n",
        "    ),\n",
        "    y=\n",
        "    alt.Y('y',\n",
        "        scale=alt.Scale(zero=False)\n",
        "    ),\n",
        "    color='cluster',\n",
        "    tooltip=['content', \"parent\"]\n",
        ").properties(\n",
        "    width=700,\n",
        "    height=400\n",
        ")\n",
        "chart.interactive()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def td_idf(documents)\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    vectors = vectorizer.fit_transform(documents)\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    dense = vectors.todense()\n",
        "    denselist = dense.tolist()\n",
        "    df = pd.DataFrame(denselist, columns=feature_names)\n",
        "    df = df[df.columns.difference(stopwords.words('french'))]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = td_idf(feedbacks_df['content'])\n",
        "#print('\\n'.join(df.columns))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#print('\\n'.join(df.columns))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def get_top_two_columns(row):\n",
        "    top_two_indexes = row.nlargest(5).index.tolist()\n",
        "    return top_two_indexes\n",
        "\n",
        "top_two_columns_df = df.apply(get_top_two_columns, axis=1)\n",
        "\n",
        "print(top_two_columns_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#print('\\n'.join(insights_df['content']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "cell_execution_strategy": "setup",
      "collapsed_sections": [
        "pKSREXZg85yy",
        "iiOlJz-d9AsB"
      ],
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
