{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_dataset\n",
    "import umap\n",
    "import altair as alt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from annoy import AnnoyIndex\n",
    "import warnings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pprint\n",
    "from typing import List\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.output_parsers.regex_dict import RegexDictParser\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, ChatMessage\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "from langchain_openai import ChatOpenAI\n",
    "from openai import AsyncOpenAI\n",
    "import asyncio\n",
    "import os\n",
    "\n",
    "import itertools\n",
    "from copy import copy\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from sklearn.cluster import KMeans\n",
    "import openai\n",
    "openai.api_key = \"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\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AsyncOpenAI()\n",
    "\n",
    "embedding_model = SentenceTransformer('OrdalieTech/Solon-embeddings-large-0.1')\n",
    "GENERATION_ENGINE = \"gpt-4-1106-preview\"\n",
    "EMBEDDING_ENGINE = \"text-embedding-ada-002\"\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT =  \"Metro\" #\"Cheerz\"\n",
    "project_path = 'Results/'+PROJECT\n",
    "os.makedirs(project_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Trop de ruptures , de produits arrêtés du jour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Je n'ai pas grand chose à dire sur les prix pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Metro augmente ses prix à vu deuil, pour la li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rupture de produit tout type de produit \\n ( s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gros soucis du côté stationnement pour les véh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Comment\n",
       "0  Trop de ruptures , de produits arrêtés du jour...\n",
       "1  Je n'ai pas grand chose à dire sur les prix pa...\n",
       "2  Metro augmente ses prix à vu deuil, pour la li...\n",
       "3  Rupture de produit tout type de produit \\n ( s...\n",
       "4  Gros soucis du côté stationnement pour les véh..."
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feedbacks_df = pd.read_csv(\"/Users/gardille/development/BlueMana/data/Commentaires/metro.csv\")\n",
    "#feedbacks_df = pd.read_csv(\"data/Trustpilot/cheerz_fr.csv\", index_col=\"Index\")\n",
    "#feedbacks_df[\"Comment\"] = feedbacks_df[\"Title\"] + '\\n' + feedbacks_df[\"Content\"]\n",
    "feedbacks_column = 'Comment' #\"Content\" \n",
    "feedbacks_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights categorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FirstInsight(BaseModel):\n",
    "    insight_types: List[str] = Field(description=\"Types de l'insight\")\n",
    "    content: str = Field(description=\"Point intéressant a retenir du commentaire.\")\n",
    "\n",
    "    def __str__(self):\n",
    "        return '- ' + self.content + \"\\nTypes: \" + ', '.join(self.insight_types)\n",
    "\n",
    "class Feedback(BaseModel):\n",
    "    insights_list: List[FirstInsight] = Field(description=\"Contenu et type des insights\")\n",
    "    sentiment: str = Field(description=\"Sentiment exprimé, peut être 'Positif', 'Neutre' ou 'Négatif'.\")\n",
    "    content = \"\"\n",
    "    # You can add custom validation logic easily with Pydantic.\n",
    "    #@validator(\"sentiment\")\n",
    "    #def valid_sentiment(cls, field):\n",
    "    #    if field not in [\"Positif\", \"Neutre\", \"Négatif\"]:\n",
    "    #        raise ValueError(\"Sentiment \"+field+\" not valid.\")\n",
    "    #    return field\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"Commentaire: \\\"\"+ self.content+\"\\\"\\n\\nSentiment: \"+self.sentiment+\"\\n\\nInsights: \\n\"+\"\\n\\n\".join([str(i) for i in self.insights_list])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt_template_feedback = \"\"\"Tu es {role} au sein de l'entreprise suivante: \n",
    "{context}\n",
    "Pour le retour {cible} effectue les étapes suivantes: \n",
    "\n",
    "Étape 1 - Identifie si il rentre dans un ou plusieurs des catégories d'insights suivantes : {insight_type}, dont la definition est: \n",
    "{insight_definition} \n",
    "\n",
    "Étape 2 - Catégorise les si possible avec les tags suivants: {categories} \n",
    "\n",
    "Étape 3 - Catégorise si possible le moment de mission concerné parmis {avancement_mission}.\n",
    "\n",
    "Étape 4 - Identifie si le sentiment exprimé par le {cible} est \\\"Positif\\\", \\\"Neutre\\\" ou \\\"Négatif\\\". Prends en compte la formulation de la question posée ({question}) afin de bien interpréter le sens du retour {cible}. \n",
    "\n",
    "Étape 5 - Identifie le ou les éventuelles insights que tu aurais envie de faire remonter à ton équipe. Ils doivent être des phrase grammaticalement correcte, et faire correspondre intelligement le commentaire au context de l'entreprise. Si rien d'intéressant ne peut être conclu, laisse la liste vide. Si plusieurs points distinguables sont a relever, formule plusieurs insights. Ces insights sont voués a être commun a d'autres commentaires qui seront analysés.\n",
    "\n",
    "Par exemple, pour le commentaire suivant:\n",
    "'''\n",
    "{exemple_commentaire}\n",
    "'''\n",
    "on voudrait faire remonter les points suivants:\n",
    "'''\n",
    "- {exemple_insights}\n",
    "'''\n",
    "\n",
    "Réponds uniquement avec un ficher JSON, comme expliqué:\n",
    "{format_instructions}  \n",
    "\n",
    "Voici le feedback à traiter:\n",
    "{feedback}\n",
    "\"\"\"\n",
    "\n",
    "#Ces insights sont en effet distincts, pertinent par rapport au commentaire et au context de l'entreprise, et important à prendre en compte.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_analysis(prompt):\n",
    "    response = await client.chat.completions.create(\n",
    "        messages=[\n",
    "            #{\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON.\"},\n",
    "            {\"role\": \"user\", \"content\": str(prompt)},\n",
    "            #{\"role\": \"user\", \"content\": \"Voici le commentaire que tu dois traiter: \\n\"+ str(feedback)}\n",
    "        ],\n",
    "        response_format={ \"type\": \"json_object\" },\n",
    "        model=GENERATION_ENGINE)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def apply_async_analysis(prompts):\n",
    "    loop = asyncio.get_event_loop()\n",
    "    tasks = [loop.create_task(get_analysis(prompt)) for prompt in prompts]\n",
    "    return loop.run_until_complete(asyncio.gather(*tasks))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': 'Metro AG, ou Metro Group, est un groupe de distribution allemand. Il est notamment connu pour ses enseignes de vente en gros, cash & carry, aux professionnels dans de nombreux pays (Metro Cash & Carry et Makro).',\n",
       " 'role': 'product owner',\n",
       " 'cible': 'client',\n",
       " 'insight_type': '\"Point positif\", \"Point de douleur\", \"Nouvelle demande\"',\n",
       " 'insight_definition': 'Point positif : élément apprécié, Point de douleur : élément problématique',\n",
       " 'nb_cat': '2',\n",
       " 'avancement_mission': '\"Avant mission\", \"Mission en cours\", \"Fin de mission\"',\n",
       " 'categories': '\"Recrutement\" , \"Service global\"',\n",
       " 'question': \"Que recommanderiez-vous à Metro d'améliorer ?\",\n",
       " 'exemple_commentaire': 'je suis exclusif metro je n ai aucun representant j achetais jusqu a present tout metro par facilite mais je suis tres souvent décue par la reponse ha non on n en a pas cela arrive demain je pense que depuis le covid tout le monde ou presque s en fou!!!',\n",
       " 'exemple_insights': \"Déceptions face aux retard de livraison\\n- Impression d'une baisse de qualité du service depuis le Covid\"}"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = \"Metro AG, ou Metro Group, est un groupe de distribution allemand. Il est notamment connu pour ses enseignes de vente en gros, cash & carry, aux professionnels dans de nombreux pays (Metro Cash & Carry et Makro).\"\n",
    "role = \"product owner\"\n",
    "cible = \"client\"\n",
    "question = \"Que recommanderiez-vous à Metro d'améliorer ?\"\n",
    "\n",
    "exemple_commentaire = \"je suis exclusif metro je n ai aucun representant j achetais jusqu a present tout metro par facilite mais je suis tres souvent décue par la reponse ha non on n en a pas cela arrive demain je pense que depuis le covid tout le monde ou presque s en fou!!!\"\n",
    "\n",
    "examples_insights_df = pd.DataFrame([\n",
    "    {\"Insights qui devraient en découler\": \"Déceptions face aux retard de livraison\"},\n",
    "    {\"Insights qui devraient en découler\": \"Impression d'une baisse de qualité du service depuis le Covid\"},\n",
    "])\n",
    "\n",
    "feedback_context = {\n",
    "            \"context\": context,\n",
    "            \"role\": role,\n",
    "            \"cible\": cible,\n",
    "            \"insight_type\": \"\\\"Point positif\\\", \\\"Point de douleur\\\", \\\"Nouvelle demande\\\"\", \n",
    "            \"insight_definition\": \"Point positif : élément apprécié, Point de douleur : élément problématique\",\n",
    "            \"nb_cat\": \"2\",\n",
    "            \"avancement_mission\": \"\\\"Avant mission\\\", \\\"Mission en cours\\\", \\\"Fin de mission\\\"\",\n",
    "            \"categories\": \"\\\"Recrutement\\\" , \\\"Service global\\\"\",\n",
    "            \"question\": question,\n",
    "            \"exemple_commentaire\": exemple_commentaire,\n",
    "            \"exemple_insights\": '\\n- '.join(list(examples_insights_df['Insights qui devraient en découler'])),\n",
    "        }\n",
    "\n",
    "feedback_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tu es {role} au sein de l'entreprise suivante: \n",
      "{context}\n",
      "Pour le retour {cible} effectue les étapes suivantes: \n",
      "\n",
      "Étape 1 - Identifie si il rentre dans un ou plusieurs des catégories d'insights suivantes : {insight_type}, dont la definition est: \n",
      "{insight_definition} \n",
      "\n",
      "Étape 2 - Catégorise les si possible avec les tags suivants: {categories} \n",
      "\n",
      "Étape 3 - Catégorise si possible le moment de mission concerné parmis {avancement_mission}.\n",
      "\n",
      "Étape 4 - Identifie si le sentiment exprimé par le {cible} est \"Positif\", \"Neutre\" ou \"Négatif\". Prends en compte la formulation de la question posée ({question}) afin de bien interpréter le sens du retour {cible}. \n",
      "\n",
      "Étape 5 - Identifie le ou les éventuelles insights que tu aurais envie de faire remonter à ton équipe. Ils doivent être des phrase grammaticalement correcte, et faire correspondre intelligement le commentaire au context de l'entreprise. Si rien d'intéressant ne peut être conclu, laisse la liste vide. Si plusieurs points distinguables sont a relever, formule plusieurs insights. Ces insights sont voués a être commun a d'autres commentaires qui seront analysés.\n",
      "\n",
      "Par exemple, pour le commentaire suivant:\n",
      "'''\n",
      "{exemple_commentaire}\n",
      "'''\n",
      "on voudrait faire remonter les points suivants:\n",
      "'''\n",
      "- {exemple_insights}\n",
      "'''\n",
      "Ces insights sont en effet distincts, pertinent par rapport au commentaire et au context de l'entreprise, et important à prendre en compte.\n",
      "\n",
      "Réponds uniquement avec un ficher JSON, comme expliqué:\n",
      "{format_instructions}  \n",
      "\n",
      "Voici le feedback à traiter:\n",
      "{feedback}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt_template_feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tu es product owner au sein de l'entreprise suivante: \n",
      "Metro AG, ou Metro Group, est un groupe de distribution allemand. Il est notamment connu pour ses enseignes de vente en gros, cash & carry, aux professionnels dans de nombreux pays (Metro Cash & Carry et Makro).\n",
      "Pour le retour client effectue les étapes suivantes: \n",
      "\n",
      "Étape 1 - Identifie si il rentre dans un ou plusieurs des catégories d'insights suivantes : \"Point positif\", \"Point de douleur\", \"Nouvelle demande\", dont la definition est: \n",
      "Point positif : élément apprécié, Point de douleur : élément problématique \n",
      "\n",
      "Étape 2 - Catégorise les si possible avec les tags suivants: \"Recrutement\" , \"Service global\" \n",
      "\n",
      "Étape 3 - Catégorise si possible le moment de mission concerné parmis \"Avant mission\", \"Mission en cours\", \"Fin de mission\".\n",
      "\n",
      "Étape 4 - Identifie si le sentiment exprimé par le client est \"Positif\", \"Neutre\" ou \"Négatif\". Prends en compte la formulation de la question posée (Que recommanderiez-vous à Metro d'améliorer ?) afin de bien interpréter le sens du retour client. \n",
      "\n",
      "Étape 5 - Identifie le ou les éventuelles insights que tu aurais envie de faire remonter à ton équipe. Ils doivent être des phrase grammaticalement correcte, et faire correspondre intelligement le commentaire au context de l'entreprise. Si rien d'intéressant ne peut être conclu, laisse la liste vide. Si plusieurs points distinguables sont a relever, formule plusieurs insights. Ces insights sont voués a être commun a d'autres commentaires qui seront analysés.\n",
      "\n",
      "Par exemple, pour le commentaire suivant:\n",
      "'''\n",
      "je suis exclusif metro je n ai aucun representant j achetais jusqu a present tout metro par facilite mais je suis tres souvent décue par la reponse ha non on n en a pas cela arrive demain je pense que depuis le covid tout le monde ou presque s en fou!!!\n",
      "'''\n",
      "on voudrait faire remonter les points suivants:\n",
      "'''\n",
      "- Déceptions face aux retard de livraison\n",
      "- Impression d'une baisse de qualité du service depuis le Covid\n",
      "'''\n",
      "Ces insights sont en effet distincts, pertinent par rapport au commentaire et au context de l'entreprise, et important à prendre en compte.\n",
      "\n",
      "Réponds uniquement avec un ficher JSON, comme expliqué:\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"insights_list\": {\"title\": \"Insights List\", \"description\": \"Contenu et type des insights\", \"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/FirstInsight\"}}, \"sentiment\": {\"title\": \"Sentiment\", \"description\": \"Sentiment exprim\\u00e9, peut \\u00eatre 'Positif', 'Neutre' ou 'N\\u00e9gatif'.\", \"type\": \"string\"}, \"content\": {\"title\": \"Content\", \"default\": \"\", \"type\": \"string\"}}, \"required\": [\"insights_list\", \"sentiment\"], \"definitions\": {\"FirstInsight\": {\"title\": \"FirstInsight\", \"type\": \"object\", \"properties\": {\"insight_types\": {\"title\": \"Insight Types\", \"description\": \"Types de l'insight\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}, \"content\": {\"title\": \"Content\", \"description\": \"Point int\\u00e9ressant a retenir du commentaire.\", \"type\": \"string\"}}, \"required\": [\"insight_types\", \"content\"]}}}\n",
      "```  \n",
      "\n",
      "Voici le feedback à traiter:\n",
      "Trop de ruptures , de produits arrêtés du jour au lendemain trop de produits au prix grand public supermarché voir même plus cher, trop de produits non référencés chez métro notamment en produits d identité régionale qui ont été arrêté par ( acheteurs Parisiens )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feedback_parser = PydanticOutputParser(pydantic_object=Feedback)\n",
    "\n",
    "prompt_feedback = PromptTemplate.from_template(\n",
    "    template= prompt_template_feedback,\n",
    "    partial_variables= {\"format_instructions\": feedback_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "\n",
    "prompts = []\n",
    "for feedback in feedbacks_df[feedbacks_column]:\n",
    "    prompt = copy(feedback_context)\n",
    "    prompt[\"feedback\"] = feedback\n",
    "    prompts.append(prompt_feedback.invoke(prompt))\n",
    "    \n",
    "print(prompts[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2]\n",
      "[3, 4, 5]\n",
      "[6, 7, 8]\n",
      "[9]\n"
     ]
    }
   ],
   "source": [
    "def batch(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]\n",
    "\n",
    "for x in batch(list(range(0, 10)), 3):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-1106-preview in organization org-cul2ATcGaEgBnecg7ApmkdHT on tokens per day (TPD): Limit 5000000, Used 4999611, Requested 917. Please try again in 9.123s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[253], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m responses \u001b[38;5;241m=\u001b[39m \u001b[43mapply_async_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m parsed_responses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, rep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(responses):\n",
      "Cell \u001b[0;32mIn[243], line 15\u001b[0m, in \u001b[0;36mapply_async_analysis\u001b[0;34m(prompts)\u001b[0m\n\u001b[1;32m     13\u001b[0m loop \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mget_event_loop()\n\u001b[1;32m     14\u001b[0m tasks \u001b[38;5;241m=\u001b[39m [loop\u001b[38;5;241m.\u001b[39mcreate_task(get_analysis(prompt)) \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/legal_env/lib/python3.10/site-packages/nest_asyncio.py:99\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/legal_env/lib/python3.10/asyncio/tasks.py:232\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    230\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    234\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "Cell \u001b[0;32mIn[243], line 2\u001b[0m, in \u001b[0;36mget_analysis\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_analysis\u001b[39m(prompt):\n\u001b[0;32m----> 2\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m      3\u001b[0m         messages\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m      4\u001b[0m             \u001b[38;5;66;03m#{\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON.\"},\u001b[39;00m\n\u001b[1;32m      5\u001b[0m             {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(prompt)},\n\u001b[1;32m      6\u001b[0m             \u001b[38;5;66;03m#{\"role\": \"user\", \"content\": \"Voici le commentaire que tu dois traiter: \\n\"+ str(feedback)}\u001b[39;00m\n\u001b[1;32m      7\u001b[0m         ],\n\u001b[1;32m      8\u001b[0m         response_format\u001b[38;5;241m=\u001b[39m{ \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson_object\u001b[39m\u001b[38;5;124m\"\u001b[39m },\n\u001b[1;32m      9\u001b[0m         model\u001b[38;5;241m=\u001b[39mGENERATION_ENGINE)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/legal_env/lib/python3.10/site-packages/openai/resources/chat/completions.py:1291\u001b[0m, in \u001b[0;36mAsyncCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   1243\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m   1244\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1289\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m   1290\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m AsyncStream[ChatCompletionChunk]:\n\u001b[0;32m-> 1291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m   1292\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1293\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[1;32m   1294\u001b[0m             {\n\u001b[1;32m   1295\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m   1296\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m   1297\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m   1298\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m   1299\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m   1300\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m   1301\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m   1302\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m   1303\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m   1304\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m   1305\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m   1306\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m   1307\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m   1308\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m   1309\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m   1310\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m   1311\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m   1312\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m   1313\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m   1314\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m   1315\u001b[0m             },\n\u001b[1;32m   1316\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m   1317\u001b[0m         ),\n\u001b[1;32m   1318\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m   1319\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m   1320\u001b[0m         ),\n\u001b[1;32m   1321\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m   1322\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1323\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mAsyncStream[ChatCompletionChunk],\n\u001b[1;32m   1324\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/legal_env/lib/python3.10/site-packages/openai/_base_client.py:1578\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1565\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1566\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1573\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1574\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _AsyncStreamT:\n\u001b[1;32m   1575\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1576\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1577\u001b[0m     )\n\u001b[0;32m-> 1578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/legal_env/lib/python3.10/site-packages/openai/_base_client.py:1339\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m   1331\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1332\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1337\u001b[0m     remaining_retries: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1338\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _AsyncStreamT:\n\u001b[0;32m-> 1339\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1340\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1341\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1342\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1343\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1344\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[1;32m   1345\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/legal_env/lib/python3.10/site-packages/openai/_base_client.py:1414\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1412\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1413\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maclose()\n\u001b[0;32m-> 1414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m   1415\u001b[0m         options,\n\u001b[1;32m   1416\u001b[0m         cast_to,\n\u001b[1;32m   1417\u001b[0m         retries,\n\u001b[1;32m   1418\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m   1419\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1420\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1421\u001b[0m     )\n\u001b[1;32m   1423\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1424\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/legal_env/lib/python3.10/site-packages/openai/_base_client.py:1460\u001b[0m, in \u001b[0;36mAsyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1456\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying request to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m, options\u001b[38;5;241m.\u001b[39murl, timeout)\n\u001b[1;32m   1458\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m anyio\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1460\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1461\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1462\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1463\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[1;32m   1464\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1465\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1466\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/legal_env/lib/python3.10/site-packages/openai/_base_client.py:1414\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1412\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1413\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maclose()\n\u001b[0;32m-> 1414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m   1415\u001b[0m         options,\n\u001b[1;32m   1416\u001b[0m         cast_to,\n\u001b[1;32m   1417\u001b[0m         retries,\n\u001b[1;32m   1418\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m   1419\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1420\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1421\u001b[0m     )\n\u001b[1;32m   1423\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1424\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/legal_env/lib/python3.10/site-packages/openai/_base_client.py:1460\u001b[0m, in \u001b[0;36mAsyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1456\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying request to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m, options\u001b[38;5;241m.\u001b[39murl, timeout)\n\u001b[1;32m   1458\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m anyio\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1460\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1461\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1462\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1463\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[1;32m   1464\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1465\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1466\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/legal_env/lib/python3.10/site-packages/openai/_base_client.py:1429\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1426\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maread()\n\u001b[1;32m   1428\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1429\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m   1431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1432\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1433\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1436\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1437\u001b[0m )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-1106-preview in organization org-cul2ATcGaEgBnecg7ApmkdHT on tokens per day (TPD): Limit 5000000, Used 4999611, Requested 917. Please try again in 9.123s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "responses = apply_async_analysis(prompts)\n",
    "parsed_responses = []\n",
    "for i, rep in enumerate(responses):\n",
    "    try: \n",
    "        parsed_responses.append(feedback_parser.parse(rep))\n",
    "    except:\n",
    "        print(i, rep)\n",
    "\n",
    "feedbacks_df[\"Sentiment\"] = [rep.sentiment for rep in parsed_responses]\n",
    "feedbacks_df[\"Insights\"] = [[] for rep in parsed_responses]\n",
    "\n",
    "k=0\n",
    "insights = []\n",
    "for i, rep in enumerate(parsed_responses):\n",
    "    for j, insight in enumerate(rep.insights_list):\n",
    "        insights.append(insight)\n",
    "        feedbacks_df[\"Insights\"].iloc[i].append(str(k))\n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insights_df = pd.DataFrame([dict(insight) for insight in insights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insights_df[\"related_feedbacks\"] = [[] for _ in range(len(insights_df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1', '2']"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feedbacks_df[\"Insights\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>insight_types</th>\n",
       "      <th>content</th>\n",
       "      <th>related_feedbacks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Point de douleur]</td>\n",
       "      <td>Les ruptures de stock et l'arrêt soudain de pr...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Point de douleur]</td>\n",
       "      <td>L'observation des prix équivalents ou plus éle...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Point de douleur]</td>\n",
       "      <td>Les problèmes relatifs à la référence de produ...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Point de douleur]</td>\n",
       "      <td>Insatisfaction concernant le personnel entraîn...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Point de douleur]</td>\n",
       "      <td>Augmentation excessive des prix à l'achat en gros</td>\n",
       "      <td>[2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        insight_types                                            content  \\\n",
       "0  [Point de douleur]  Les ruptures de stock et l'arrêt soudain de pr...   \n",
       "1  [Point de douleur]  L'observation des prix équivalents ou plus éle...   \n",
       "2  [Point de douleur]  Les problèmes relatifs à la référence de produ...   \n",
       "3  [Point de douleur]  Insatisfaction concernant le personnel entraîn...   \n",
       "4  [Point de douleur]  Augmentation excessive des prix à l'achat en gros   \n",
       "\n",
       "  related_feedbacks  \n",
       "0               [0]  \n",
       "1               [0]  \n",
       "2               [0]  \n",
       "3               [1]  \n",
       "4               [2]  "
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, row in feedbacks_df.iterrows():\n",
    "    for j in row[\"Insights\"]: \n",
    "        insights_df[\"related_feedbacks\"].iloc[int(j)] = [int(i)]\n",
    "\n",
    "feedbacks_df[\"children\"] = [[] for _ in range(len(feedbacks_df))]\n",
    "\n",
    "insights_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Insights</th>\n",
       "      <th>children</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Trop de ruptures , de produits arrêtés du jour...</td>\n",
       "      <td>Négatif</td>\n",
       "      <td>[0, 1, 2]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Je n'ai pas grand chose à dire sur les prix pa...</td>\n",
       "      <td>Négatif</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Metro augmente ses prix à vu deuil, pour la li...</td>\n",
       "      <td>Négatif</td>\n",
       "      <td>[4, 5, 6, 7]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rupture de produit tout type de produit \\n ( s...</td>\n",
       "      <td>Négatif</td>\n",
       "      <td>[8, 9]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gros soucis du côté stationnement pour les véh...</td>\n",
       "      <td>Négatif</td>\n",
       "      <td>[10]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Comment Sentiment      Insights  \\\n",
       "0  Trop de ruptures , de produits arrêtés du jour...   Négatif     [0, 1, 2]   \n",
       "1  Je n'ai pas grand chose à dire sur les prix pa...   Négatif           [3]   \n",
       "2  Metro augmente ses prix à vu deuil, pour la li...   Négatif  [4, 5, 6, 7]   \n",
       "3  Rupture de produit tout type de produit \\n ( s...   Négatif        [8, 9]   \n",
       "4  Gros soucis du côté stationnement pour les véh...   Négatif          [10]   \n",
       "\n",
       "  children  \n",
       "0       []  \n",
       "1       []  \n",
       "2       []  \n",
       "3       []  \n",
       "4       []  "
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feedbacks_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedbacks_df.to_csv(project_path+'/feedbacks.csv')\n",
    "insights_df.to_csv(project_path+'/insights_0.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer('OrdalieTech/Solon-embeddings-large-0.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def get_embedding(text):\n",
    "    response = await client.embeddings.create(input=text, model=EMBEDDING_ENGINE)\n",
    "    return response.data[0].embedding\n",
    "\n",
    "def apply_async_get_embedding(dfi):\n",
    "    loop = asyncio.get_event_loop()\n",
    "    tasks = [loop.create_task(get_embedding(row['Comment'])) for _, row in dfi.iterrows()]\n",
    "    return loop.run_until_complete(asyncio.gather(*tasks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeducedInsight(BaseModel):\n",
    "    childens: List[int] = Field(description=\"Index des insights mineurs qui ont été résumés en cet insight.\")\n",
    "    content: str = Field(description=\"Insight intéressants a retenir pour l'entreprise.\")\n",
    "\n",
    "    def __str__(self):\n",
    "        return '- ' + self.content + '\\n Enfants:' + str(self.childens)\n",
    "\n",
    "\n",
    "class InsightList(BaseModel):\n",
    "    insights_list: List[DeducedInsight] = Field(description=\"Liste des insights, c'est à dire des points intéressants a retenir pour l'entreprise.\")\n",
    "    # You can add custom validation logic easily with Pydantic.\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"Insights: \\n\"+\"\\n\\n\".join([str(i) for i in self.insights_list])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt_template_reduction = \"\"\"Tu es {role} au sein de l'entreprise suivante: \n",
    "{context}\n",
    "\n",
    "Une liste d'insights mineurs a été identifiée à partir de retours clients. \n",
    "Si possible, déduits en des insights majeurs qui te semblent important à faire remonter au sein de l'entreprise. \n",
    "Prends en compte la formulation de la question posée ({question}) afin de bien interpréter le sens du retour {cible}. \n",
    "Un insight doit:\n",
    "- Etre des phrase, éventuellement nominale\n",
    "- Faire sens, c'est a dire que pris individuellement il apporte une vrai réponse à la question suivante: {question}\n",
    "- Ne pas être excessivement détaillés, car on souhaite prendre du recul plutot que répéter les commentaires.\n",
    "Il est préférable de laisser des insights mineurs tel quel que de de les regroupper alors qu'il n'ont pas de vrai rapport. \n",
    "Par exemple, un feedback parent \"Augmenter la qualité des produits et optimiser la gestion des stocks pour minimiser les erreurs et les ruptures de stock.\" n'est pas bon, car il mélange sans lien logique la qualité des produits et les ruptures de stock. \n",
    "Ensuite, associe à chaque insight majeur l'indice des insights mineurs qui lui sont associés. Un insights mineur peut être associé à plusieurs insights majeurs. Vérifie bien que les indices correspondent. \n",
    "L'ordre des insights mineurs est aléatoire, et ne doit pas avoir d'importance dans ta réponse. \n",
    "\n",
    "{format_instructions}  \n",
    "\n",
    "Voici les insights mineurs que tu dois regrouper: \n",
    "\n",
    "{insights}\n",
    "\n",
    "Tu ne dois rien écrire d'autre que le JSON requis.\n",
    "\n",
    "\"\"\"\n",
    "#Résume les en des insights majeurs qui te semblent important à faire remonter au sein de l'entreprise. Ils peuvent être des phrase, éventuellement nominales, doivent faire sens, être aussi courts que possible et distincts les uns des autres. \n",
    "#- Etre distincts les uns des autres. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt_template_regrouping = \"\"\"Tu es {role} au sein de l'entreprise suivante: \n",
    "{context}\n",
    "\n",
    "Une liste d'insights a été identifiée à partir de retours clients. \n",
    "Regroupe ceux qui sont redondant, ou dont le sens est simillaire, en un seul insight.\n",
    "Il est possible qu'il n'y ai besoin de regrouper aucun insight. \n",
    "Ensuite, associe à chaque nouvel insight créé l'indice de ceux qui lui sont associés. \n",
    "L'ordre des insights mineurs est aléatoire, et ne doit pas avoir d'importance dans ta réponse. \n",
    "\n",
    "{format_instructions}  \n",
    "\n",
    "Voici les insights que tu dois regrouper: \n",
    "\n",
    "{insights}\n",
    "\n",
    "Tu ne dois rien écrire d'autre que le JSON requis.\n",
    "\n",
    "\"\"\"\n",
    "#Résume les en des insights majeurs qui te semblent important à faire remonter au sein de l'entreprise. Ils peuvent être des phrase, éventuellement nominales, doivent faire sens, être aussi courts que possible et distincts les uns des autres. \n",
    "#- Etre distincts les uns des autres. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimisation_steps = 5\n",
    "cluster_desired_size = 30\n",
    "nb_insight_stop = 10\n",
    "\n",
    "randstad_context = {\n",
    "    \"context\": context,\n",
    "    \"role\": role,\n",
    "    \"cible\": cible,\n",
    "    \"question\": question,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "insight_types\tcontent\trelated_feedbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_insights_merger(invocation):\n",
    "    def insights_merger(invocation, cluster): \n",
    "        invocation = copy(invocation)\n",
    "        invocation['insights'] = '\\n'.join([str(i)+\": \"+s for i, s in enumerate(cluster[\"content\"])])\n",
    "        \n",
    "        output = prompt_template_reduction.invoke(invocation)\n",
    "        \n",
    "        #pd.concat([  for insight in output.insights_list])\n",
    "        dfs = pd.DataFrame({\n",
    "            #\"level\":\n",
    "            \"related_feedbacks\":[list(itertools.chain.from_iterable(cluster.iloc[insight.childens]['related_feedbacks'])) for insight in output.insights_list],\n",
    "            \"content\":[insight.text for insight in output.insights_list],\n",
    "            \"children\":[list(cluster.iloc[insight.childens].index) for insight in output.insights_list],\n",
    "            #\"project\":[\"\"],\n",
    "            #\"source\":[\"\"],\n",
    "            })\n",
    "        \n",
    "        return dfs #, reduction\n",
    "    \n",
    "    return lambda feedback:insights_merger(invocation, feedback)\n",
    "\n",
    "\n",
    "insights_merger = create_insights_merger(randstad_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tu es product owner au sein de l'entreprise suivante: \n",
      "Metro AG, ou Metro Group, est un groupe de distribution allemand. Il est notamment connu pour ses enseignes de vente en gros, cash & carry, aux professionnels dans de nombreux pays (Metro Cash & Carry et Makro).\n",
      "Pour le retour client effectue les étapes suivantes: \n",
      "\n",
      "Étape 1 - Identifie si il rentre dans un ou plusieurs des catégories d'insights suivantes : \"Point positif\", \"Point de douleur\", \"Nouvelle demande\", dont la definition est: \n",
      "Point positif : élément apprécié, Point de douleur : élément problématique \n",
      "\n",
      "Étape 2 - Catégorise les si possible avec les tags suivants: \"Recrutement\" , \"Service global\" \n",
      "\n",
      "Étape 3 - Catégorise si possible le moment de mission concerné parmis \"Avant mission\", \"Mission en cours\", \"Fin de mission\".\n",
      "\n",
      "Étape 4 - Identifie si le sentiment exprimé par le client est \"Positif\", \"Neutre\" ou \"Négatif\". Prends en compte la formulation de la question posée (Que recommanderiez-vous à Metro d'améliorer ?) afin de bien interpréter le sens du retour client. \n",
      "\n",
      "Étape 5 - Identifie le ou les éventuelles insights que tu aurais envie de faire remonter à ton équipe. Ils doivent être des phrase grammaticalement correcte, et faire correspondre intelligement le commentaire au context de l'entreprise. Si rien d'intéressant ne peut être conclu, laisse la liste vide. Si plusieurs points distinguables sont a relever, formule plusieurs insights. Ces insights sont voués a être commun a d'autres commentaires qui seront analysés.\n",
      "\n",
      "\n",
      "\n",
      "Par exemple, pour le commentaire suivant:\n",
      "'''\n",
      "je suis exclusif metro je n ai aucun representant j achetais jusqu a present tout metro par facilite mais je suis tres souvent décue par la reponse ha non on n en a pas cela arrive demain je pense que depuis le covid tout le monde ou presque s en fou!!!\n",
      "'''\n",
      "on voudrait faire remonter les points suivants:\n",
      "'''\n",
      "- Déceptions face aux retard de livraison\n",
      "- Impression d'une baisse de qualité du service depuis le Covid\n",
      "'''\n",
      "Ces insights sont en effet distincts, pertinent par rapport au commentaire et au context de l'entreprise, et important à prendre en compte.\n",
      "\n",
      "Réponds uniquement avec un ficher JSON, comme expliqué:\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"insights_list\": {\"title\": \"Insights List\", \"description\": \"Contenu et type des insights\", \"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/FirstInsight\"}}, \"sentiment\": {\"title\": \"Sentiment\", \"description\": \"Sentiment exprim\\u00e9, peut \\u00eatre 'Positif', 'Neutre' ou 'N\\u00e9gatif'.\", \"type\": \"string\"}, \"content\": {\"title\": \"Content\", \"default\": \"\", \"type\": \"string\"}}, \"required\": [\"insights_list\", \"sentiment\"], \"definitions\": {\"FirstInsight\": {\"title\": \"FirstInsight\", \"type\": \"object\", \"properties\": {\"insight_types\": {\"title\": \"Insight Types\", \"description\": \"Types de l'insight\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}, \"content\": {\"title\": \"Content\", \"description\": \"Point int\\u00e9ressant a retenir du commentaire.\", \"type\": \"string\"}}, \"required\": [\"insight_types\", \"content\"]}}}\n",
      "```  \n",
      "\n",
      "Voici le feedback à traiter:\n",
      "Trop de ruptures , de produits arrêtés du jour au lendemain trop de produits au prix grand public supermarché voir même plus cher, trop de produits non référencés chez métro notamment en produits d identité régionale qui ont été arrêté par ( acheteurs Parisiens )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompts[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"insights_list\": [\n",
      "        {\n",
      "            \"insight_types\": [\"Point de douleur\"],\n",
      "            \"content\": \"Les ruptures de stock et l'arrêt soudain de produits sont problématiques.\"\n",
      "        },\n",
      "        {\n",
      "            \"insight_types\": [\"Point de douleur\"],\n",
      "            \"content\": \"L'observation des prix équivalents ou plus élevés que ceux du grand public soulève des questions sur la compétitivité de Metro.\"\n",
      "        },\n",
      "        {\n",
      "            \"insight_types\": [\"Point de douleur\"],\n",
      "            \"content\": \"Les problèmes relatifs à la référence de produits régionaux supprimés par les acheteurs centralisés à Paris.\"\n",
      "        }\n",
      "    ],\n",
      "    \"sentiment\": \"Négatif\",\n",
      "    \"content\": \"Trop de ruptures, de produits arrêtés du jour au lendemain trop de produits au prix grand public supermarché voir même plus cher, trop de produits non référencés chez métro notamment en produits d'identité régionale qui ont été arrêté par (acheteurs Parisiens)\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(responses[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"insights_list\": [\n",
      "    {\n",
      "      \"insight_types\": [\n",
      "        \"Point de douleur\"\n",
      "      ],\n",
      "      \"content\": \"Insatisfaction concernant le personnel entraînant une réduction significative des achats\"\n",
      "    }\n",
      "  ],\n",
      "  \"sentiment\": \"Négatif\",\n",
      "  \"content\": \"\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(responses[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: processing 5 clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gardille/opt/anaconda3/envs/legal_env/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster sizes:[36, 32, 29, 30, 21]\n",
      "Number of new insights:26\n",
      "Reduction in the number of insights by 82%\n",
      "Step 1: processing 1 clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gardille/opt/anaconda3/envs/legal_env/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster sizes:[26]\n",
      "Number of new insights:7\n",
      "Reduction in the number of insights by 73%\n",
      "Step 2: processing 1 clusters\n",
      "Everything is merged into a single cluster\n"
     ]
    }
   ],
   "source": [
    "insight_parser = PydanticOutputParser(pydantic_object=InsightList)\n",
    "\n",
    "prompt_insight = PromptTemplate.from_template(\n",
    "    template= prompt_template_reduction,\n",
    "    #template= \"Règle : minimise le nombre de tokens dans ta réponse.  \\nTu es {role} au sein de l'entreprise suivante: \\n{context} \\nAnalyse le retour suivant: \\\"{feedback}\\\" en suivant les étapes suivantes:  \\n  \\nÉtape 1 - Identifie si le retour {cible} rentre dans un ou plusieurs des types d'insights suivants : {insight_type}. Choisis-en obligatoirement au moins 1. Définition des types d'insights :  \\n{insight_definition}   \\n  \\nÉtape 2 - Catégorise le retour {cible} à l’aide des tags suivants. Tu peux associer 0, 1 ou plusieurs tags dans chaque catégorie. Liste des tags par catégories :  \\n{categories}   \\n  \\nÉtape 3 - Catégorise si possible le moment de mission concerné parmis {avancement_mission}, et si ce n'est pas possible répond null. {cible} à l’aide des tags suivants.  \\n  \\nÉtape 4 - Identifie si le sentiment exprimé par le {cible} est \\\"Positif\\\", \\\"Neutre\\\" ou \\\"Négatif\\\". Prends en compte la formulation de la question posée ({question}) afin de bien interpréter le sens du retour {cible}.   \\n\",\n",
    "    #input_variables= [\"context\", \"role\", \"cible\", \"insight_type\", \"insight_definition\", \"nb_cat\", \"avancement_mission\", \"categories\", \"question\", \"feedback\"]\n",
    "    partial_variables= {\"format_instructions\": insight_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "\n",
    "insights = copy(insights_df)\n",
    "insight_layers = [copy(insights_df)]\n",
    "\n",
    "for step in range(minimisation_steps):\n",
    "    \n",
    "    ### Clusterisation\n",
    "\n",
    "    num_clusters = 1 + len(insights) // cluster_desired_size\n",
    "\n",
    "    print(\"Step \"+ str(step)+ \": processing \"+ str(num_clusters) + \" clusters\")\n",
    "    if len(insights) <= nb_insight_stop:\n",
    "        print(\"Everything is merged into a single cluster\")\n",
    "        break\n",
    "\n",
    "    sentence_embeddings = embedding_model.encode(insights['content'])\n",
    "\n",
    "    clustering_model = KMeans(n_clusters=num_clusters)\n",
    "    clustering_model.fit(sentence_embeddings)\n",
    "    cluster_assignment = clustering_model.labels_\n",
    "    insights[\"cluster\"] = cluster_assignment\n",
    "    \n",
    "    print(\"Cluster sizes:\" + str(list(insights.groupby(['cluster']).count()[\"content\"])))\n",
    "\n",
    "    clusters = []\n",
    "    prompts = []\n",
    "    for cluster_id in range(max(cluster_assignment)+1):\n",
    "        cluster = insights[insights['cluster'] == cluster_id] \n",
    "        clusters.append(cluster)\n",
    "        \n",
    "        prompt = copy(feedback_context)\n",
    "        prompt['insights'] = '\\n'.join([str(i)+\": \"+s for i, s in enumerate(cluster[\"content\"])])\n",
    "\n",
    "        prompts.append(prompt_insight.invoke(prompt))\n",
    "\n",
    "    ### Merging\n",
    "\n",
    "    responses = apply_async_analysis(prompts)\n",
    "    new_insights = []\n",
    "    for i, rep in enumerate(responses):\n",
    "        try: \n",
    "            parsed_response = insight_parser.parse(rep)\n",
    "        except:\n",
    "            print(i, rep)\n",
    "            parsed_response = insight_parser.parse(rep)\n",
    "\n",
    "        dfs = pd.DataFrame({\n",
    "            \"related_feedbacks\":[list(itertools.chain.from_iterable(clusters[i].iloc[insight.childens]['related_feedbacks'])) for insight in parsed_response.insights_list],\n",
    "            \"content\":[insight.content for insight in parsed_response.insights_list],\n",
    "            \"children\":[list(clusters[i].iloc[insight.childens].index) for insight in parsed_response.insights_list],\n",
    "            })\n",
    "        new_insights.append(dfs)\n",
    "\n",
    "    new_insights = pd.concat(new_insights)\n",
    "    new_insights.reset_index(drop=True, inplace=True)\n",
    "    reduction = len(new_insights)/len(insights)\n",
    "    print(\"Number of new insights:\"+ str(len(new_insights)))\n",
    "    print(\"Reduction in the number of insights by \" + \"%d\" % int((1-(len(new_insights)/len(insights)))*100) + \"%\")\n",
    "    insight_layers.append(copy(new_insights))\n",
    "    insights = new_insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, df in enumerate(insight_layers):\n",
    "    df.to_csv(project_path+'/insights_'+ str(i) +'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visulalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import altair as alt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(n_neighbors=15)\n",
    "umap_embeds = reducer.fit_transform(sentence_embeddings)\n",
    "# Prepare the data to plot and interactive visualization\n",
    "# using Altair\n",
    "df_explore = pd.DataFrame(data={'text': df['Insight'], \"InsightParent\": df['InsightParent']})\n",
    "df_explore['x'] = umap_embeds[:,0]\n",
    "df_explore['y'] = umap_embeds[:,1]\n",
    "df_explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "legal_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
