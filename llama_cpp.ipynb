{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_path = \"/media/maitre/HDD1/Models/\"\n",
    "model_name = \"mixtral-8x7b-instruct-v0.1.Q3_K_M.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python[server] --upgrade --force-reinstall --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\n",
      "llama_model_loader: loaded meta data with 26 key-value pairs and 995 tensors from /media/maitre/HDD1/Models/mixtral-8x7b-instruct-v0.1.Q3_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mixtral-8x7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:                         llama.expert_count u32              = 8\n",
      "llama_model_loader: - kv  10:                    llama.expert_used_count u32              = 2\n",
      "llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  12:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  13:                          general.file_type u32              = 12\n",
      "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:   32 tensors\n",
      "llama_model_loader: - type q8_0:   64 tensors\n",
      "llama_model_loader: - type q3_K:  801 tensors\n",
      "llama_model_loader: - type q4_K:   32 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 8\n",
      "llm_load_print_meta: n_expert_used    = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q3_K - Medium\n",
      "llm_load_print_meta: model params     = 46.70 B\n",
      "llm_load_print_meta: model size       = 18.96 GiB (3.49 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mixtral-8x7b-instruct-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.76 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =    53.71 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size = 19365.55 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host input buffer size   =     9.01 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   125.98 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =     8.80 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 3\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '12', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'llama.expert_count': '8', 'llama.context_length': '32768', 'general.name': 'mistralai_mixtral-8x7b-instruct-v0.1', 'llama.expert_used_count': '2'}\n",
      "Using chat template: {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\n",
      "Using chat eos_token: \n",
      "Using chat bos_token: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-430a8e18-c841-440b-9314-bd4140ad769d', 'object': 'text_completion', 'created': 1707237530, 'model': '/media/maitre/HDD1/Models/mixtral-8x7b-instruct-v0.1.Q3_K_M.gguf', 'choices': [{'text': 'Q: Name the planets in the solar system? A: 1) Mercury, 2) Venus, 3) Earth, 4) Mars, 5) Jupiter, 6) Saturn', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 14, 'completion_tokens': 32, 'total_tokens': 46}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1936.40 ms\n",
      "llama_print_timings:      sample time =       8.61 ms /    32 runs   (    0.27 ms per token,  3717.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1936.37 ms /    14 tokens (  138.31 ms per token,     7.23 tokens per second)\n",
      "llama_print_timings:        eval time =     802.53 ms /    31 runs   (   25.89 ms per token,    38.63 tokens per second)\n",
      "llama_print_timings:       total time =    2799.14 ms /    45 tokens\n"
     ]
    }
   ],
   "source": [
    "llm = Llama(\n",
    "      model_path=models_path+model_name,\n",
    "      n_gpu_layers=-1, # Uncomment to use GPU acceleration\n",
    "      # seed=1337, # Uncomment to set a specific seed\n",
    "      # n_ctx=2048, # Uncomment to increase the context window\n",
    ")\n",
    "output = llm(\n",
    "      \"Q: Name the planets in the solar system? A: \", # Prompt\n",
    "      max_tokens=32, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
    "      stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
    "      echo=True # Echo the prompt back in the output\n",
    ") # Generate a completion, can also call create_completion\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\n",
      "llama_model_loader: loaded meta data with 26 key-value pairs and 995 tensors from /media/maitre/HDD1/Models/mixtral-8x7b-instruct-v0.1.Q3_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mixtral-8x7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:                         llama.expert_count u32              = 8\n",
      "llama_model_loader: - kv  10:                    llama.expert_used_count u32              = 2\n",
      "llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  12:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  13:                          general.file_type u32              = 12\n",
      "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:   32 tensors\n",
      "llama_model_loader: - type q8_0:   64 tensors\n",
      "llama_model_loader: - type q3_K:  801 tensors\n",
      "llama_model_loader: - type q4_K:   32 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 8\n",
      "llm_load_print_meta: n_expert_used    = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q3_K - Medium\n",
      "llm_load_print_meta: model params     = 46.70 B\n",
      "llm_load_print_meta: model size       = 18.96 GiB (3.49 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mixtral-8x7b-instruct-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.76 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =    53.71 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size = 19365.55 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host input buffer size   =     9.01 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   125.98 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =     8.80 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 3\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '12', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'llama.expert_count': '8', 'llama.context_length': '32768', 'general.name': 'mistralai_mixtral-8x7b-instruct-v0.1', 'llama.expert_used_count': '2'}\n",
      "\n",
      "llama_print_timings:        load time =     390.67 ms\n",
      "llama_print_timings:      sample time =     124.95 ms /   480 runs   (    0.26 ms per token,  3841.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =     390.62 ms /    32 tokens (   12.21 ms per token,    81.92 tokens per second)\n",
      "llama_print_timings:        eval time =   12520.37 ms /   479 runs   (   26.14 ms per token,    38.26 tokens per second)\n",
      "llama_print_timings:       total time =   13961.41 ms /   511 tokens\n"
     ]
    }
   ],
   "source": [
    "llm = Llama(model_path=models_path+model_name, n_gpu_layers=-1, chat_format=\"llama-2\")  # Set chat_format according to the model you are using\n",
    "response = llm.create_chat_completion(\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a story writing assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a story about llamas.\"\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-f9645834-f843-4686-9c5b-e30f164eefe4',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1707237630,\n",
       " 'model': '/media/maitre/HDD1/Models/mixtral-8x7b-instruct-v0.1.Q3_K_M.gguf',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': ' Once upon a time, in the vast and rugged terrain of the Andes Mountains, there lived a group of playful and mischievous llamas. These lovable creatures were known for their long necks, beautiful woolly coats, and their unique ability to carry heavy loads on their backs. They spent their days roaming the mountainsides, nibbling on the lush green grass and wildflowers that covered the slopes.\\n\\nThe leader of the llama herd was a wise and gentle llama named Luna. She was respected by all the other llamas for her knowledge of the mountains and her ability to find the best grazing spots. Luna was also known for her striking coat, which was a beautiful mix of white and gray.\\n\\nOne day, Luna received a vision from the Great Spirit of the Andes. The vision showed that a terrible drought was coming to the mountains, and that the llama herd would have to leave their home to find food and water. Luna knew that she had to lead her herd to safety, but she also knew that the journey would be long and difficult.\\n\\nUndeterred, Luna gathered her herd and set off on the journey. They traveled for days, crossing raging rivers and steep mountain passes. Along the way, they encountered many dangers, including fierce jaguars and eagles looking for their next meal. But Luna was a skilled leader, and she was able to guide her herd safely through each challenge.\\n\\nAfter many days of travel, the llama herd finally reached the lush and verdant valley that Luna had seen in her vision. They found plenty of food and water, and they were able to rest and recover from their long journey.\\n\\nThe herd was grateful to Luna for leading them to safety, and they celebrated her bravery and wisdom. From that day on, Luna was known as the greatest leader of all the llama herds in the Andes, and her legend was passed down from generation to generation.\\n\\nAnd so, the llamas lived happily ever after in their new home, free to roam the valley and enjoy the bounty of the earth. They were a testament to the power of leadership, perseverance, and'},\n",
       "   'finish_reason': 'length'}],\n",
       " 'usage': {'prompt_tokens': 32, 'completion_tokens': 480, 'total_tokens': 512}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Once upon a time, in the vast and rugged terrain of the Andes Mountains, there lived a group of playful and mischievous llamas. These lovable creatures were known for their long necks, beautiful woolly coats, and their unique ability to carry heavy loads on their backs. They spent their days roaming the mountainsides, nibbling on the lush green grass and wildflowers that covered the slopes.\n",
      "\n",
      "The leader of the llama herd was a wise and gentle llama named Luna. She was respected by all the other llamas for her knowledge of the mountains and her ability to find the best grazing spots. Luna was also known for her striking coat, which was a beautiful mix of white and gray.\n",
      "\n",
      "One day, Luna received a vision from the Great Spirit of the Andes. The vision showed that a terrible drought was coming to the mountains, and that the llama herd would have to leave their home to find food and water. Luna knew that she had to lead her herd to safety, but she also knew that the journey would be long and difficult.\n",
      "\n",
      "Undeterred, Luna gathered her herd and set off on the journey. They traveled for days, crossing raging rivers and steep mountain passes. Along the way, they encountered many dangers, including fierce jaguars and eagles looking for their next meal. But Luna was a skilled leader, and she was able to guide her herd safely through each challenge.\n",
      "\n",
      "After many days of travel, the llama herd finally reached the lush and verdant valley that Luna had seen in her vision. They found plenty of food and water, and they were able to rest and recover from their long journey.\n",
      "\n",
      "The herd was grateful to Luna for leading them to safety, and they celebrated her bravery and wisdom. From that day on, Luna was known as the greatest leader of all the llama herds in the Andes, and her legend was passed down from generation to generation.\n",
      "\n",
      "And so, the llamas lived happily ever after in their new home, free to roam the valley and enjoy the bounty of the earth. They were a testament to the power of leadership, perseverance, and\n"
     ]
    }
   ],
   "source": [
    "rep = response['choices'][0]['message']['content']\n",
    "print(rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 26 key-value pairs and 995 tensors from /media/maitre/HDD1/Models/mixtral-8x7b-instruct-v0.1.Q3_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mixtral-8x7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:                         llama.expert_count u32              = 8\n",
      "llama_model_loader: - kv  10:                    llama.expert_used_count u32              = 2\n",
      "llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  12:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  13:                          general.file_type u32              = 12\n",
      "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:   32 tensors\n",
      "llama_model_loader: - type q8_0:   64 tensors\n",
      "llama_model_loader: - type q3_K:  801 tensors\n",
      "llama_model_loader: - type q4_K:   32 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 8\n",
      "llm_load_print_meta: n_expert_used    = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q3_K - Medium\n",
      "llm_load_print_meta: model params     = 46.70 B\n",
      "llm_load_print_meta: model size       = 18.96 GiB (3.49 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mixtral-8x7b-instruct-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.38 MiB\n",
      "llm_load_tensors: offloading 30 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 30/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size = 19419.27 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =     9.01 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   125.98 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '12', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'llama.expert_count': '8', 'llama.context_length': '32768', 'general.name': 'mistralai_mixtral-8x7b-instruct-v0.1', 'llama.expert_used_count': '2'}\n",
      "\n",
      "llama_print_timings:        load time =   20853.62 ms\n",
      "llama_print_timings:      sample time =      22.22 ms /    76 runs   (    0.29 ms per token,  3419.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =   20853.32 ms /   436 tokens (   47.83 ms per token,    20.91 tokens per second)\n",
      "llama_print_timings:        eval time =   17770.86 ms /    75 runs   (  236.94 ms per token,     4.22 tokens per second)\n",
      "llama_print_timings:       total time =   38798.85 ms /   511 tokens\n"
     ]
    }
   ],
   "source": [
    "llm = Llama(\n",
    "    model_path=models_path+model_name, \n",
    "    n_gpu_layers=-1, \n",
    "    chat_format=\"llama-2\"\n",
    ")  # Set chat_format according to the model you are using\n",
    "\n",
    "\n",
    "\n",
    "response = llm.create_chat_completion(\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a story writing assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ],\n",
    "    response_format={\n",
    "        \"type\": \"json_object\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The author of this story appears to have an appreciation for nature, as evidenced by the detailed and loving description of the Peruvian plateaus and the llama herd. The focus on community and teamwork within the herd suggests that the author values cooperation and collective action. Luna's leadership role and her selfless actions to rescue the young llama indicate\n"
     ]
    }
   ],
   "source": [
    "rep = response['choices'][0]['message']['content']\n",
    "print(rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\n",
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from /home/maitre/Downloads/llama-2-7b.Q5_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q5_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 4.45 GiB (5.68 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "llm_load_tensors: offloading 28 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 28/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  4560.87 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  3818.00 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 3584\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:  CUDA_Host KV buffer size =   224.00 MiB\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =  1568.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1792.00 MiB, K (f16):  896.00 MiB, V (f16):  896.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host input buffer size   =    15.28 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   285.43 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =   277.60 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 5\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '32', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '17'}\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "llm = Llama(model_path=\"/home/maitre/Downloads/llama-2-7b.Q5_K_M.gguf\", \n",
    "            n_gpu_layers=28, n_threads=6, n_ctx=3584, n_batch=521, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "from mistralai.client import MistralClient\n",
    "from mistralai.models.chat_completion import ChatMessage\n",
    "from pydantic import BaseModel, ValidationError, ConfigDict\n",
    "from typing import Type, Optional\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from llama_cpp import Llama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python3 -m llama_cpp.server --model /media/maitre/HDD1/Models/mixtral-8x7b-instruct-v0.1.Q3_K_M.gguf --n_gpu_layers -1 --chat_format llama-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_path = \"/media/maitre/HDD1/Models/\"\n",
    "model_name = \"mixtral-8x7b-instruct-v0.1.Q3_K_M.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MistralLanguageModel:\n",
    "    def __init__(self, api_key=None,\n",
    "                 model=\"mistral-tiny\", local=True):\n",
    "\n",
    "        if local:\n",
    "            self.llm = Llama(\n",
    "                model_path=models_path+model_name, \n",
    "                n_gpu_layers=-1, \n",
    "                chat_format=\"llama-2\",\n",
    "                n_ctx=2048\n",
    "                )\n",
    "            self.chat = self.llm.create_chat_completion\n",
    "        else:\n",
    "            self.model = model\n",
    "            self.api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
    "            self.chat = MistralClient(api_key=self.api_key).chat\n",
    "\n",
    "    def generate(self, prompt: str,\n",
    "                 output_format: Optional[Type[BaseModel]] = None,\n",
    "                 max_tokens: int = None):\n",
    "\n",
    "        #retry_delay = 0.1\n",
    "\n",
    "        #while True:\n",
    "        #\"    try:\n",
    "        system_message = \"You are a helpful assistant.\"\n",
    "        if output_format:\n",
    "            system_message += f\"\\nRespond with respect to the following JSON format: {output_format.model_json_schema()}\" # noqa\n",
    "\n",
    "        print(\"system_message\")\n",
    "        print(system_message)\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "            #ChatMessage(role=\"system\", content=system_message),\n",
    "            #ChatMessage(role=\"user\", content=prompt)\n",
    "        ]\n",
    "        params = {\n",
    "            #\"model\": self.model,\n",
    "            \"messages\": messages,\n",
    "            \"response_format\":{\n",
    "                \"type\": \"json_object\",\n",
    "            },\n",
    "            #\"temperature\": self.temperature\n",
    "        }\n",
    "\n",
    "        if max_tokens is not None:\n",
    "            params[\"max_tokens\"] = max_tokens\n",
    "\n",
    "        response = self.chat(**params)\n",
    "        print(\"response\")\n",
    "        print(response)\n",
    "        return response\n",
    "        response_content = response['choices'][0]['message']['content']\n",
    "\n",
    "        if output_format:\n",
    "            if self._is_valid_json_for_model(response_content,\n",
    "                                                output_format):\n",
    "                \n",
    "                return response_content\n",
    "        else:\n",
    "            return response_content\n",
    "\n",
    "        #    except Exception:\n",
    "        #        print(f\"Hit rate limit. Retrying in {retry_delay} seconds.\")\n",
    "        #        time.sleep(retry_delay)\n",
    "        #        retry_delay *= 2\n",
    "\n",
    "    def _model_structure_repr(self, model: Type[BaseModel]) -> str:\n",
    "        fields = model.__annotations__\n",
    "        return ', '.join(f'{key}: {value}' for key, value in fields.items())\n",
    "\n",
    "\n",
    "    def _is_valid_json_for_model(self, text: str, model: Type[BaseModel]) -> bool: # noqa\n",
    "        \"\"\"\n",
    "        Check if a text is valid JSON and if it respects the provided BaseModel. # noqa\n",
    "        \"\"\"\n",
    "        model.model_config = ConfigDict(strict=True)\n",
    "\n",
    "        try:\n",
    "            parsed_data = json.loads(text)\n",
    "            model(**parsed_data)\n",
    "            return True\n",
    "        except (json.JSONDecodeError, ValidationError):\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 26 key-value pairs and 995 tensors from /media/maitre/HDD1/Models/mixtral-8x7b-instruct-v0.1.Q3_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mixtral-8x7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:                         llama.expert_count u32              = 8\n",
      "llama_model_loader: - kv  10:                    llama.expert_used_count u32              = 2\n",
      "llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  12:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  13:                          general.file_type u32              = 12\n",
      "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:   32 tensors\n",
      "llama_model_loader: - type q8_0:   64 tensors\n",
      "llama_model_loader: - type q3_K:  801 tensors\n",
      "llama_model_loader: - type q4_K:   32 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 8\n",
      "llm_load_print_meta: n_expert_used    = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q3_K - Medium\n",
      "llm_load_print_meta: model params     = 46.70 B\n",
      "llm_load_print_meta: model size       = 18.96 GiB (3.49 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mixtral-8x7b-instruct-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.76 MiB\n",
      "ggml_backend_cuda_buffer_type_alloc_buffer: allocating 19365.55 MiB on device 0: cudaMalloc failed: out of memory\n",
      "llama_model_load: error loading model: failed to allocate buffer\n",
      "llama_load_model_from_file: failed to load model\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = MistralLanguageModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_dataset\n",
    "import umap\n",
    "import altair as alt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List\n",
    "import enum\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.output_parsers.regex_dict import RegexDictParser\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, ChatMessage\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from pydantic import BaseModel, Field, validator, create_model\n",
    "from openai import AsyncOpenAI, OpenAI\n",
    "import asyncio\n",
    "import os\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import umap.umap_ as umap\n",
    "#import umap\n",
    "import hdbscan\n",
    "\n",
    "from typing import Literal, Union\n",
    "from pydantic.config import ConfigDict\n",
    "\n",
    "from src.utilities import *\n",
    "\n",
    "import openai\n",
    "import instructor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Tu es analyste marketing au sein de l'entreprise suivante:\n",
    "METRO est une enseigne rserve aux professionnels des mtiers de bouche (restaurateurs, traiteurs, bouchers, boulangers) qui vend des produits alimentaires et non alimentaires adapts  leur activit, en magasins (appels entrepts ou halles) et en livraison. \n",
    "\n",
    "Tu as men une enqute auprs des client de l'entreprise, et a rcupr une liste de retours. \n",
    "Tu es charg de faire remonter auprs de l'entreprise les conlusion de ton enqute, c'est  dire les insights que tu as dduit de l'analyse de ces retours.\n",
    "\n",
    "Effectue les tapes suivantes:\n",
    "\n",
    "tape 1 - Identification des insights\n",
    "Identifie les insights  faire remonter auprs de ton quipe.\n",
    "Voici les contraintes que les insights doivent respecter:\n",
    "- Une personne de ton quipe qui lit un insight doit pouvoir en comprendre le sens, sans qu'il y ait d'ambiguit.\n",
    "- Un insight doit tre aussi court que possible, tout en restant parfaitement comprhensible et pertinent.\n",
    "- N'ajoute pas de bouts de phrases unitiles, comme la consquence quand celle ci est vidente. Par exemple, inutile d'ajouter des bouts de phrase comme \"..., ce qui entrane un intrt moindre pour l'enseigne\"\n",
    "- Les insights apportent des informations distinctes.\n",
    "- Un insight est une information que tu trouves rellement intressante, pas une simple reformulation du retour client.\n",
    "\n",
    "tape 2 - Indiquer quels retours sont  l'origine de chaque feedback  \n",
    "Associe  chaque insight la liste des indices des retours qui lui sont associs.\n",
    "Par exemple, le retour a pour indice ne nombre 10:\n",
    "'''\n",
    "10 : ras\n",
    "'''\n",
    "\n",
    "Voici les retours  traiter:\n",
    "              \n",
    "10 : pas assez de promos pour petite quantit achets, volume trop lev\n",
    "11 : mettre en place des points fidlits en\n",
    "12 : il serait souhaitable de remettre de l assortiment en livraison car depuis la rduction des produits nous ne commandons que 10% de produits dispo sur site abonns livraison soit 90 % de moins sur 2 entits\n",
    " Egalement ce dispositif achetez plus n est pas dispo en livraison\n",
    "13 : Produits pas trs rguliers en qualit et en fracheur (salades icebergs et framboises fraches) Pourquoi les prix de chez mtro sont plus chers que les supermarchs ? Bananes, framboises fraches, icebergs, tomates ? Je ne trouve pas cela normal\n",
    "14 : que le nouveau directeur de metro caen apprenne a dire bonjour !!!!\n",
    "\n",
    "Si un retours n'est pas trs intressant, il ne doit pas faire remonter d'insight.\n",
    "L'ordre des retours est alatoire, et ne doit pas avoir d'impact sur ton analyse.\n",
    "Le nombre d'insights dpend uniquement de la quantit d'informations qui te parait important de faire remonter, et pas du nombre de retours client.\n",
    "Retourne la liste des insights que tu as dduis. Chaque insight a donc un contenu et une liste contenant les indices des retours associs.\n",
    "Vrifie que la liste des indices des retours associ  chaque insight correspond bien.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_facile = \"\"\"\n",
    "Tu es analyste marketing au sein de l'entreprise suivante:\n",
    "METRO est une enseigne rserve aux professionnels des mtiers de bouche (restaurateurs, traiteurs, bouchers, boulangers) qui vend des produits alimentaires et non alimentaires adapts  leur activit, en magasins (appels entrepts ou halles) et en livraison. \n",
    "\n",
    "Tu as men une enqute auprs des client de l'entreprise, et a rcupr une liste de retours. \n",
    "Tu es charg de faire remonter auprs de l'entreprise les conlusion de ton enqute, c'est  dire les insights que tu as dduit de l'analyse de ces retours.\n",
    "\n",
    "Effectue les tapes suivantes:\n",
    "\n",
    "tape 1 - Identification des insights\n",
    "Identifie un insights  faire remonter auprs de ton quipe.\n",
    "Voici les contraintes que l'insights doit respecter:\n",
    "- Une personne de ton quipe qui lit un insight doit pouvoir en comprendre le sens, sans qu'il y ait d'ambiguit.\n",
    "- Un insight doit tre aussi court que possible, tout en restant parfaitement comprhensible et pertinent.\n",
    "- N'ajoute pas de bouts de phrases unitiles, comme la consquence quand celle ci est vidente. Par exemple, inutile d'ajouter des bouts de phrase comme \"..., ce qui entrane un intrt moindre pour l'enseigne\"\n",
    "- Les insights apportent des informations distinctes.\n",
    "- Un insight est une information que tu trouves rellement intressante, pas une simple reformulation du retour client.\n",
    "\n",
    "tape 2 - Indiquer quels retours sont  l'origine de ton feedback  \n",
    "Associe  chaque insight la liste des indices des retours qui lui sont associs.\n",
    "Par exemple, le retour a pour indice ne nombre 10:\n",
    "'''\n",
    "10 : ras\n",
    "'''\n",
    "\n",
    "Voici le retours  traiter:\n",
    "              \n",
    "10 : pas assez de promos pour petite quantit achets, volume trop lev\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FirstInsight(BaseModel):\n",
    "    #model_config = ConfigDict(title='Main')\n",
    "    \n",
    "    associated_indexes: List[int] = Field(description=\"Indices des retours associs.\")\n",
    "    #titre: str = Field(description=\"Titre de l'insight.\")\n",
    "    contenu: str = Field(description=\"Insight qui a t dduit de l'analyse des retours.\") #Field(description=\"Point intressant a retenir du commentaire.\")\n",
    "\n",
    "    def __str__(self):\n",
    "        return '- ' + self.content + \"\\nTypes: \" + ', '.join(self.insight_types)\n",
    "    \n",
    "class InsightsList(BaseModel):\n",
    "    insights_list: List[FirstInsight] = Field(description=\"Liste des insights.\")\n",
    "\n",
    "output_format = InsightsList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system_message\n",
      "You are a helpful assistant.\n",
      "Respond with respect to the following JSON format: {'$defs': {'FirstInsight': {'properties': {'associated_indexes': {'description': 'Indices des retours associs.', 'items': {'type': 'integer'}, 'title': 'Associated Indexes', 'type': 'array'}, 'contenu': {'description': \"Insight qui a t dduit de l'analyse des retours.\", 'title': 'Contenu', 'type': 'string'}}, 'required': ['associated_indexes', 'contenu'], 'title': 'FirstInsight', 'type': 'object'}}, 'properties': {'insights_list': {'description': 'Liste des insights.', 'items': {'$ref': '#/$defs/FirstInsight'}, 'title': 'Insights List', 'type': 'array'}}, 'required': ['insights_list'], 'title': 'InsightsList', 'type': 'object'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      "{'id': 'chatcmpl-2bcec634-4eba-46d8-8e60-e83719a84ac5', 'object': 'chat.completion', 'created': 1707243351, 'model': '/media/maitre/HDD1/Models/mixtral-8x7b-instruct-v0.1.Q3_K_M.gguf', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': ' tape 1 - Identification des insights :\\n\\nInsight 1 :\\nContenu : Les clients souhaitent bnficier de promotions adaptes aux petites quantits achetes, car ils estiment que le volume actuel est trop lev.\\n\\nInsight 2 :\\nContenu : Les clients demandent la mise en place d\\'un programme de fidlit.\\n\\nInsight 3 :\\nContenu : Les clients constatent une diminution de l\\'assortiment en livraison et aimeraient que celui-ci soit rtabli, en particulier l\\'offre \"Achetez plus\".\\n\\nInsight 4 :\\nContenu : Les clients peroivent une irrgularit dans la qualit et la fracheur de certains produits, notamment les salades iceberg et les framboises fraches. Ils s\\'interrogent galement sur les prix plus levs de METRO par rapport aux supermarchs pour les bananes, les framboises fraches, les icebergs et les tomates.\\n\\nInsight 5 :\\nContenu : Les clients attendent une meilleure politesse de la part du nouveau directeur du magasin METRO de Caen, en particulier un accueil plus chaleureux avec un \"bonjour\".\\n\\ntape 2 - Indiquer quels retours sont  l\\'origine de chaque feedback :\\n\\nInsight 1 : [10]\\nInsight 2 : [11]\\nInsight 3 : [12]\\nInsight 4 : [13]\\nInsight 5 : [14]'}, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 1115, 'completion_tokens': 380, 'total_tokens': 1495}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1562.29 ms\n",
      "llama_print_timings:      sample time =      98.98 ms /   381 runs   (    0.26 ms per token,  3849.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3738.05 ms /  1115 tokens (    3.35 ms per token,   298.28 tokens per second)\n",
      "llama_print_timings:        eval time =   10354.15 ms /   380 runs   (   27.25 ms per token,    36.70 tokens per second)\n",
      "llama_print_timings:       total time =   14926.71 ms /  1495 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' tape 1 - Identification des insights :\\n\\nInsight 1 :\\nContenu : Les clients souhaitent bnficier de promotions adaptes aux petites quantits achetes, car ils estiment que le volume actuel est trop lev.\\n\\nInsight 2 :\\nContenu : Les clients demandent la mise en place d\\'un programme de fidlit.\\n\\nInsight 3 :\\nContenu : Les clients constatent une diminution de l\\'assortiment en livraison et aimeraient que celui-ci soit rtabli, en particulier l\\'offre \"Achetez plus\".\\n\\nInsight 4 :\\nContenu : Les clients peroivent une irrgularit dans la qualit et la fracheur de certains produits, notamment les salades iceberg et les framboises fraches. Ils s\\'interrogent galement sur les prix plus levs de METRO par rapport aux supermarchs pour les bananes, les framboises fraches, les icebergs et les tomates.\\n\\nInsight 5 :\\nContenu : Les clients attendent une meilleure politesse de la part du nouveau directeur du magasin METRO de Caen, en particulier un accueil plus chaleureux avec un \"bonjour\".\\n\\ntape 2 - Indiquer quels retours sont  l\\'origine de chaque feedback :\\n\\nInsight 1 : [10]\\nInsight 2 : [11]\\nInsight 3 : [12]\\nInsight 4 : [13]\\nInsight 5 : [14]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.generate(prompt, output_format)\n",
    "response['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " tape 1 - Identification des insights :\n",
      "\n",
      "Insight 1 :\n",
      "Contenu : Les clients souhaitent bnficier de promotions adaptes aux petites quantits achetes, car ils estiment que le volume actuel est trop lev.\n",
      "\n",
      "Insight 2 :\n",
      "Contenu : Les clients demandent la mise en place d'un programme de fidlit.\n",
      "\n",
      "Insight 3 :\n",
      "Contenu : Les clients constatent une diminution de l'assortiment en livraison et aimeraient que celui-ci soit rtabli, en particulier l'offre \"Achetez plus\".\n",
      "\n",
      "Insight 4 :\n",
      "Contenu : Les clients peroivent une irrgularit dans la qualit et la fracheur de certains produits, notamment les salades iceberg et les framboises fraches. Ils s'interrogent galement sur les prix plus levs de METRO par rapport aux supermarchs pour les bananes, les framboises fraches, les icebergs et les tomates.\n",
      "\n",
      "Insight 5 :\n",
      "Contenu : Les clients attendent une meilleure politesse de la part du nouveau directeur du magasin METRO de Caen, en particulier un accueil plus chaleureux avec un \"bonjour\".\n",
      "\n",
      "tape 2 - Indiquer quels retours sont  l'origine de chaque feedback :\n",
      "\n",
      "Insight 1 : [10]\n",
      "Insight 2 : [11]\n",
      "Insight 3 : [12]\n",
      "Insight 4 : [13]\n",
      "Insight 5 : [14]\n"
     ]
    }
   ],
   "source": [
    "print(response['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-f95257e6-ecf9-46e3-98ca-da47cde3e1d0', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Je suis dsol, mais je ne suis pas programm pour rpondre  des demandes de ce type. Mon rle consiste  fournir des informations utiles et  traiter les donnes que je reois pour gnrer des rponses appropries en fonction de la demande. Dans le cadre de mon fonctionnement actuel, je ne peux pas vous conseiller sur la manire de manger de l'herbe avec les pieds. Cependant, si vous avez des questions d'ordre informatique ou des demandes lies  l'analyse de donnes ou  la cration de fichiers JSON, je serai heureux de vous aider.\", role='assistant', function_call=None, tool_calls=None))], created=1707299846, model='mixtral', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=157, prompt_tokens=101, total_tokens=258))\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import instructor\n",
    "\n",
    "client = OpenAI(base_url=\"http://localhost:8000/v1\", api_key=\"sk-xxx\")\n",
    "client = instructor.patch(client)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"mixtral\",\n",
    "    messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Tu est un assistant splialis dans l'analyse de commentaires, et qui ne renvoit que des fichiers JSON.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Comment manger de l'herbe avec les pieds?\"},\n",
    "        ],\n",
    ")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_analysis(prompt, response_model):\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Tu est un assistant splialis dans l'analyse de commentaires\"},#, et qui ne renvoit que des fichiers JSON.\"},\n",
    "            {\"role\": \"user\", \"content\": str(prompt)},\n",
    "        ],\n",
    "        model=\"mixtral\",\n",
    "        response_format={ \"type\": \"json_object\" },\n",
    "        response_model=response_model,\n",
    "        )\n",
    "    return response\n",
    "\n",
    "\n",
    "def apply_analysis(prompts, response_models):\n",
    "    if type(response_models) is not list:\n",
    "        response_models = [response_models for _ in prompts]\n",
    "    res = []\n",
    "    for (prompt, response_model) in zip(prompts, response_models):\n",
    "        res.append(get_analysis(prompt, response_model))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InsightsList(insights_list=[FirstInsight(associated_indexes=[10], contenu='Les clients trouvent que les promotions sont insuffisantes pour les petites quantits achetes et que les volumes sont trop levs.'), FirstInsight(associated_indexes=[11], contenu=\"Les clients demandent l'implmentation d'un systme de points de fidlit.\"), FirstInsight(associated_indexes=[12], contenu=\"Les clients dplorent la rduction de l'assortiment en livraison et la non-disponibilit de l'offre 'Achetez plus' en livraison.\"), FirstInsight(associated_indexes=[13], contenu='Les clients remarquent des irrgularits dans la qualit et la fracheur des produits (salades iceberg et framboises fraches). Ils trouvent que les prix de METRO sont plus levs que ceux des supermarchs pour certains produits (bananes, framboises fraches, icebergs, tomates).'), FirstInsight(associated_indexes=[14], contenu='Les clients trouvent que le nouveau directeur de METRO CAEN devrait apprendre  saluer ses clients.')])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_analysis(prompt, InsightsList)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "feedback_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
