{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_path = \"/media/maitre/HDD1/Models/\"\n",
    "model_name = \"mixtral-8x7b-instruct-v0.1.Q3_K_M.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python[server] --upgrade --force-reinstall --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\n",
      "llama_model_loader: loaded meta data with 26 key-value pairs and 995 tensors from /media/maitre/HDD1/Models/mixtral-8x7b-instruct-v0.1.Q3_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mixtral-8x7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:                         llama.expert_count u32              = 8\n",
      "llama_model_loader: - kv  10:                    llama.expert_used_count u32              = 2\n",
      "llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  12:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  13:                          general.file_type u32              = 12\n",
      "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:   32 tensors\n",
      "llama_model_loader: - type q8_0:   64 tensors\n",
      "llama_model_loader: - type q3_K:  801 tensors\n",
      "llama_model_loader: - type q4_K:   32 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 8\n",
      "llm_load_print_meta: n_expert_used    = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q3_K - Medium\n",
      "llm_load_print_meta: model params     = 46.70 B\n",
      "llm_load_print_meta: model size       = 18.96 GiB (3.49 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mixtral-8x7b-instruct-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.76 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =    53.71 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size = 19365.55 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host input buffer size   =     9.01 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   125.98 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =     8.80 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 3\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '12', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'llama.expert_count': '8', 'llama.context_length': '32768', 'general.name': 'mistralai_mixtral-8x7b-instruct-v0.1', 'llama.expert_used_count': '2'}\n",
      "Using chat template: {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\n",
      "Using chat eos_token: \n",
      "Using chat bos_token: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-430a8e18-c841-440b-9314-bd4140ad769d', 'object': 'text_completion', 'created': 1707237530, 'model': '/media/maitre/HDD1/Models/mixtral-8x7b-instruct-v0.1.Q3_K_M.gguf', 'choices': [{'text': 'Q: Name the planets in the solar system? A: 1) Mercury, 2) Venus, 3) Earth, 4) Mars, 5) Jupiter, 6) Saturn', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 14, 'completion_tokens': 32, 'total_tokens': 46}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1936.40 ms\n",
      "llama_print_timings:      sample time =       8.61 ms /    32 runs   (    0.27 ms per token,  3717.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1936.37 ms /    14 tokens (  138.31 ms per token,     7.23 tokens per second)\n",
      "llama_print_timings:        eval time =     802.53 ms /    31 runs   (   25.89 ms per token,    38.63 tokens per second)\n",
      "llama_print_timings:       total time =    2799.14 ms /    45 tokens\n"
     ]
    }
   ],
   "source": [
    "llm = Llama(\n",
    "      model_path=models_path+model_name,\n",
    "      n_gpu_layers=-1, # Uncomment to use GPU acceleration\n",
    "      # seed=1337, # Uncomment to set a specific seed\n",
    "      # n_ctx=2048, # Uncomment to increase the context window\n",
    ")\n",
    "output = llm(\n",
    "      \"Q: Name the planets in the solar system? A: \", # Prompt\n",
    "      max_tokens=32, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
    "      stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
    "      echo=True # Echo the prompt back in the output\n",
    ") # Generate a completion, can also call create_completion\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\n",
      "llama_model_loader: loaded meta data with 26 key-value pairs and 995 tensors from /media/maitre/HDD1/Models/mixtral-8x7b-instruct-v0.1.Q3_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mixtral-8x7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:                         llama.expert_count u32              = 8\n",
      "llama_model_loader: - kv  10:                    llama.expert_used_count u32              = 2\n",
      "llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  12:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  13:                          general.file_type u32              = 12\n",
      "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:   32 tensors\n",
      "llama_model_loader: - type q8_0:   64 tensors\n",
      "llama_model_loader: - type q3_K:  801 tensors\n",
      "llama_model_loader: - type q4_K:   32 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 8\n",
      "llm_load_print_meta: n_expert_used    = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q3_K - Medium\n",
      "llm_load_print_meta: model params     = 46.70 B\n",
      "llm_load_print_meta: model size       = 18.96 GiB (3.49 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mixtral-8x7b-instruct-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.76 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =    53.71 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size = 19365.55 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host input buffer size   =     9.01 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   125.98 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =     8.80 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 3\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '12', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'llama.expert_count': '8', 'llama.context_length': '32768', 'general.name': 'mistralai_mixtral-8x7b-instruct-v0.1', 'llama.expert_used_count': '2'}\n",
      "\n",
      "llama_print_timings:        load time =     390.67 ms\n",
      "llama_print_timings:      sample time =     124.95 ms /   480 runs   (    0.26 ms per token,  3841.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =     390.62 ms /    32 tokens (   12.21 ms per token,    81.92 tokens per second)\n",
      "llama_print_timings:        eval time =   12520.37 ms /   479 runs   (   26.14 ms per token,    38.26 tokens per second)\n",
      "llama_print_timings:       total time =   13961.41 ms /   511 tokens\n"
     ]
    }
   ],
   "source": [
    "llm = Llama(model_path=models_path+model_name, n_gpu_layers=-1, chat_format=\"llama-2\")  # Set chat_format according to the model you are using\n",
    "response = llm.create_chat_completion(\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a story writing assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a story about llamas.\"\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-f9645834-f843-4686-9c5b-e30f164eefe4',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1707237630,\n",
       " 'model': '/media/maitre/HDD1/Models/mixtral-8x7b-instruct-v0.1.Q3_K_M.gguf',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': ' Once upon a time, in the vast and rugged terrain of the Andes Mountains, there lived a group of playful and mischievous llamas. These lovable creatures were known for their long necks, beautiful woolly coats, and their unique ability to carry heavy loads on their backs. They spent their days roaming the mountainsides, nibbling on the lush green grass and wildflowers that covered the slopes.\\n\\nThe leader of the llama herd was a wise and gentle llama named Luna. She was respected by all the other llamas for her knowledge of the mountains and her ability to find the best grazing spots. Luna was also known for her striking coat, which was a beautiful mix of white and gray.\\n\\nOne day, Luna received a vision from the Great Spirit of the Andes. The vision showed that a terrible drought was coming to the mountains, and that the llama herd would have to leave their home to find food and water. Luna knew that she had to lead her herd to safety, but she also knew that the journey would be long and difficult.\\n\\nUndeterred, Luna gathered her herd and set off on the journey. They traveled for days, crossing raging rivers and steep mountain passes. Along the way, they encountered many dangers, including fierce jaguars and eagles looking for their next meal. But Luna was a skilled leader, and she was able to guide her herd safely through each challenge.\\n\\nAfter many days of travel, the llama herd finally reached the lush and verdant valley that Luna had seen in her vision. They found plenty of food and water, and they were able to rest and recover from their long journey.\\n\\nThe herd was grateful to Luna for leading them to safety, and they celebrated her bravery and wisdom. From that day on, Luna was known as the greatest leader of all the llama herds in the Andes, and her legend was passed down from generation to generation.\\n\\nAnd so, the llamas lived happily ever after in their new home, free to roam the valley and enjoy the bounty of the earth. They were a testament to the power of leadership, perseverance, and'},\n",
       "   'finish_reason': 'length'}],\n",
       " 'usage': {'prompt_tokens': 32, 'completion_tokens': 480, 'total_tokens': 512}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Once upon a time, in the vast and rugged terrain of the Andes Mountains, there lived a group of playful and mischievous llamas. These lovable creatures were known for their long necks, beautiful woolly coats, and their unique ability to carry heavy loads on their backs. They spent their days roaming the mountainsides, nibbling on the lush green grass and wildflowers that covered the slopes.\n",
      "\n",
      "The leader of the llama herd was a wise and gentle llama named Luna. She was respected by all the other llamas for her knowledge of the mountains and her ability to find the best grazing spots. Luna was also known for her striking coat, which was a beautiful mix of white and gray.\n",
      "\n",
      "One day, Luna received a vision from the Great Spirit of the Andes. The vision showed that a terrible drought was coming to the mountains, and that the llama herd would have to leave their home to find food and water. Luna knew that she had to lead her herd to safety, but she also knew that the journey would be long and difficult.\n",
      "\n",
      "Undeterred, Luna gathered her herd and set off on the journey. They traveled for days, crossing raging rivers and steep mountain passes. Along the way, they encountered many dangers, including fierce jaguars and eagles looking for their next meal. But Luna was a skilled leader, and she was able to guide her herd safely through each challenge.\n",
      "\n",
      "After many days of travel, the llama herd finally reached the lush and verdant valley that Luna had seen in her vision. They found plenty of food and water, and they were able to rest and recover from their long journey.\n",
      "\n",
      "The herd was grateful to Luna for leading them to safety, and they celebrated her bravery and wisdom. From that day on, Luna was known as the greatest leader of all the llama herds in the Andes, and her legend was passed down from generation to generation.\n",
      "\n",
      "And so, the llamas lived happily ever after in their new home, free to roam the valley and enjoy the bounty of the earth. They were a testament to the power of leadership, perseverance, and\n"
     ]
    }
   ],
   "source": [
    "rep = response['choices'][0]['message']['content']\n",
    "print(rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 26 key-value pairs and 995 tensors from /media/maitre/HDD1/Models/mixtral-8x7b-instruct-v0.1.Q3_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mixtral-8x7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:                         llama.expert_count u32              = 8\n",
      "llama_model_loader: - kv  10:                    llama.expert_used_count u32              = 2\n",
      "llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  12:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  13:                          general.file_type u32              = 12\n",
      "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:   32 tensors\n",
      "llama_model_loader: - type q8_0:   64 tensors\n",
      "llama_model_loader: - type q3_K:  801 tensors\n",
      "llama_model_loader: - type q4_K:   32 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 8\n",
      "llm_load_print_meta: n_expert_used    = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q3_K - Medium\n",
      "llm_load_print_meta: model params     = 46.70 B\n",
      "llm_load_print_meta: model size       = 18.96 GiB (3.49 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mixtral-8x7b-instruct-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.38 MiB\n",
      "llm_load_tensors: offloading 30 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 30/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size = 19419.27 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =     9.01 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   125.98 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '12', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'llama.expert_count': '8', 'llama.context_length': '32768', 'general.name': 'mistralai_mixtral-8x7b-instruct-v0.1', 'llama.expert_used_count': '2'}\n",
      "\n",
      "llama_print_timings:        load time =   20853.62 ms\n",
      "llama_print_timings:      sample time =      22.22 ms /    76 runs   (    0.29 ms per token,  3419.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =   20853.32 ms /   436 tokens (   47.83 ms per token,    20.91 tokens per second)\n",
      "llama_print_timings:        eval time =   17770.86 ms /    75 runs   (  236.94 ms per token,     4.22 tokens per second)\n",
      "llama_print_timings:       total time =   38798.85 ms /   511 tokens\n"
     ]
    }
   ],
   "source": [
    "llm = Llama(\n",
    "    model_path=models_path+model_name, \n",
    "    n_gpu_layers=-1, \n",
    "    chat_format=\"llama-2\"\n",
    ")  # Set chat_format according to the model you are using\n",
    "\n",
    "\n",
    "\n",
    "response = llm.create_chat_completion(\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a story writing assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ],\n",
    "    response_format={\n",
    "        \"type\": \"json_object\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The author of this story appears to have an appreciation for nature, as evidenced by the detailed and loving description of the Peruvian plateaus and the llama herd. The focus on community and teamwork within the herd suggests that the author values cooperation and collective action. Luna's leadership role and her selfless actions to rescue the young llama indicate\n"
     ]
    }
   ],
   "source": [
    "rep = response['choices'][0]['message']['content']\n",
    "print(rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\n",
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from /home/maitre/Downloads/llama-2-7b.Q5_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q5_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 4.45 GiB (5.68 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "llm_load_tensors: offloading 28 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 28/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  4560.87 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  3818.00 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 3584\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:  CUDA_Host KV buffer size =   224.00 MiB\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =  1568.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1792.00 MiB, K (f16):  896.00 MiB, V (f16):  896.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host input buffer size   =    15.28 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   285.43 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =   277.60 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 5\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '32', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '17'}\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "llm = Llama(model_path=\"/home/maitre/Downloads/llama-2-7b.Q5_K_M.gguf\", \n",
    "            n_gpu_layers=28, n_threads=6, n_ctx=3584, n_batch=521, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "from mistralai.client import MistralClient\n",
    "from mistralai.models.chat_completion import ChatMessage\n",
    "from pydantic import BaseModel, ValidationError, ConfigDict\n",
    "from typing import Type, Optional\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from llama_cpp import Llama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python3 -m llama_cpp.server --model /media/maitre/HDD1/Models/mixtral-8x7b-instruct-v0.1.Q3_K_M.gguf --n_gpu_layers -1 --chat_format llama-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_path = \"/media/maitre/HDD1/Models/\"\n",
    "model_name = \"mixtral-8x7b-instruct-v0.1.Q3_K_M.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MistralLanguageModel:\n",
    "    def __init__(self, api_key=None,\n",
    "                 model=\"mistral-tiny\", local=True):\n",
    "\n",
    "        if local:\n",
    "            self.llm = Llama(\n",
    "                model_path=models_path+model_name, \n",
    "                n_gpu_layers=-1, \n",
    "                chat_format=\"llama-2\",\n",
    "                n_ctx=2048\n",
    "                )\n",
    "            self.chat = self.llm.create_chat_completion\n",
    "        else:\n",
    "            self.model = model\n",
    "            self.api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
    "            self.chat = MistralClient(api_key=self.api_key).chat\n",
    "\n",
    "    def generate(self, prompt: str,\n",
    "                 output_format: Optional[Type[BaseModel]] = None,\n",
    "                 max_tokens: int = None):\n",
    "\n",
    "        #retry_delay = 0.1\n",
    "\n",
    "        #while True:\n",
    "        #\"    try:\n",
    "        system_message = \"You are a helpful assistant.\"\n",
    "        if output_format:\n",
    "            system_message += f\"\\nRespond with respect to the following JSON format: {output_format.model_json_schema()}\" # noqa\n",
    "\n",
    "        print(\"system_message\")\n",
    "        print(system_message)\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "            #ChatMessage(role=\"system\", content=system_message),\n",
    "            #ChatMessage(role=\"user\", content=prompt)\n",
    "        ]\n",
    "        params = {\n",
    "            #\"model\": self.model,\n",
    "            \"messages\": messages,\n",
    "            \"response_format\":{\n",
    "                \"type\": \"json_object\",\n",
    "            },\n",
    "            #\"temperature\": self.temperature\n",
    "        }\n",
    "\n",
    "        if max_tokens is not None:\n",
    "            params[\"max_tokens\"] = max_tokens\n",
    "\n",
    "        response = self.chat(**params)\n",
    "        print(\"response\")\n",
    "        print(response)\n",
    "        return response\n",
    "        response_content = response['choices'][0]['message']['content']\n",
    "\n",
    "        if output_format:\n",
    "            if self._is_valid_json_for_model(response_content,\n",
    "                                                output_format):\n",
    "                \n",
    "                return response_content\n",
    "        else:\n",
    "            return response_content\n",
    "\n",
    "        #    except Exception:\n",
    "        #        print(f\"Hit rate limit. Retrying in {retry_delay} seconds.\")\n",
    "        #        time.sleep(retry_delay)\n",
    "        #        retry_delay *= 2\n",
    "\n",
    "    def _model_structure_repr(self, model: Type[BaseModel]) -> str:\n",
    "        fields = model.__annotations__\n",
    "        return ', '.join(f'{key}: {value}' for key, value in fields.items())\n",
    "\n",
    "\n",
    "    def _is_valid_json_for_model(self, text: str, model: Type[BaseModel]) -> bool: # noqa\n",
    "        \"\"\"\n",
    "        Check if a text is valid JSON and if it respects the provided BaseModel. # noqa\n",
    "        \"\"\"\n",
    "        model.model_config = ConfigDict(strict=True)\n",
    "\n",
    "        try:\n",
    "            parsed_data = json.loads(text)\n",
    "            model(**parsed_data)\n",
    "            return True\n",
    "        except (json.JSONDecodeError, ValidationError):\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 26 key-value pairs and 995 tensors from /media/maitre/HDD1/Models/mixtral-8x7b-instruct-v0.1.Q3_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mixtral-8x7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:                         llama.expert_count u32              = 8\n",
      "llama_model_loader: - kv  10:                    llama.expert_used_count u32              = 2\n",
      "llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  12:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  13:                          general.file_type u32              = 12\n",
      "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:   32 tensors\n",
      "llama_model_loader: - type q8_0:   64 tensors\n",
      "llama_model_loader: - type q3_K:  801 tensors\n",
      "llama_model_loader: - type q4_K:   32 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 8\n",
      "llm_load_print_meta: n_expert_used    = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q3_K - Medium\n",
      "llm_load_print_meta: model params     = 46.70 B\n",
      "llm_load_print_meta: model size       = 18.96 GiB (3.49 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mixtral-8x7b-instruct-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.76 MiB\n",
      "ggml_backend_cuda_buffer_type_alloc_buffer: allocating 19365.55 MiB on device 0: cudaMalloc failed: out of memory\n",
      "llama_model_load: error loading model: failed to allocate buffer\n",
      "llama_load_model_from_file: failed to load model\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = MistralLanguageModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_dataset\n",
    "import umap\n",
    "import altair as alt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List\n",
    "import enum\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.output_parsers.regex_dict import RegexDictParser\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, ChatMessage\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from pydantic import BaseModel, Field, validator, create_model\n",
    "from openai import AsyncOpenAI, OpenAI\n",
    "import asyncio\n",
    "import os\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import umap.umap_ as umap\n",
    "#import umap\n",
    "import hdbscan\n",
    "\n",
    "from typing import Literal, Union\n",
    "from pydantic.config import ConfigDict\n",
    "\n",
    "from src.utilities import *\n",
    "\n",
    "import openai\n",
    "import instructor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Tu es analyste marketing au sein de l'entreprise suivante:\n",
    "METRO est une enseigne réservée aux professionnels des métiers de bouche (restaurateurs, traiteurs, bouchers, boulangers…) qui vend des produits alimentaires et non alimentaires adaptés à leur activité, en magasins (appelés entrepôts ou halles) et en livraison. \n",
    "\n",
    "Tu as mené une enquête auprès des client de l'entreprise, et a récupéré une liste de retours. \n",
    "Tu es chargé de faire remonter auprès de l'entreprise les conlusion de ton enquète, c'est à dire les insights que tu as déduit de l'analyse de ces retours.\n",
    "\n",
    "Effectue les étapes suivantes:\n",
    "\n",
    "Étape 1 - Identification des insights\n",
    "Identifie les insights à faire remonter auprès de ton équipe.\n",
    "Voici les contraintes que les insights doivent respecter:\n",
    "- Une personne de ton équipe qui lit un insight doit pouvoir en comprendre le sens, sans qu'il y ait d'ambiguité.\n",
    "- Un insight doit être aussi court que possible, tout en restant parfaitement compréhensible et pertinent.\n",
    "- N'ajoute pas de bouts de phrases unitiles, comme la conséquence quand celle ci est évidente. Par exemple, inutile d'ajouter des bouts de phrase comme \"..., ce qui entraîne un intérêt moindre pour l'enseigne\"\n",
    "- Les insights apportent des informations distinctes.\n",
    "- Un insight est une information que tu trouves réellement intéressante, pas une simple reformulation du retour client.\n",
    "\n",
    "Étape 2 - Indiquer quels retours sont à l'origine de chaque feedback  \n",
    "Associe à chaque insight la liste des indices des retours qui lui sont associés.\n",
    "Par exemple, le retour a pour indice ne nombre 10:\n",
    "'''\n",
    "10 : ras\n",
    "'''\n",
    "\n",
    "Voici les retours à traiter:\n",
    "              \n",
    "10 : pas assez de promos pour petite quantité achetés, volume trop élevé\n",
    "11 : mettre en place des points fidélités en\n",
    "12 : il serait souhaitable de remettre de l assortiment en livraison car depuis la réduction des produits nous ne commandons que 10% de produits dispo sur site abonnés livraison soit 90 % de moins sur 2 entités\n",
    " Egalement ce dispositif achetez plus n est pas dispo en livraison\n",
    "13 : Produits pas très réguliers en qualité et en fraîcheur (salades icebergs et framboises fraîches) Pourquoi les prix de chez métro sont plus chers que les supermarchés ? Bananes, framboises fraîches, icebergs, tomates ? Je ne trouve pas cela normal\n",
    "14 : que le nouveau directeur de metro caen apprenne a dire bonjour !!!!\n",
    "\n",
    "Si un retours n'est pas très intéressant, il ne doit pas faire remonter d'insight.\n",
    "L'ordre des retours est aléatoire, et ne doit pas avoir d'impact sur ton analyse.\n",
    "Le nombre d'insights dépend uniquement de la quantité d'informations qui te parait important de faire remonter, et pas du nombre de retours client.\n",
    "Retourne la liste des insights que tu as déduis. Chaque insight a donc un contenu et une liste contenant les indices des retours associés.\n",
    "Vérifie que la liste des indices des retours associé à chaque insight correspond bien.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_facile = \"\"\"\n",
    "Tu es analyste marketing au sein de l'entreprise suivante:\n",
    "METRO est une enseigne réservée aux professionnels des métiers de bouche (restaurateurs, traiteurs, bouchers, boulangers…) qui vend des produits alimentaires et non alimentaires adaptés à leur activité, en magasins (appelés entrepôts ou halles) et en livraison. \n",
    "\n",
    "Tu as mené une enquête auprès des client de l'entreprise, et a récupéré une liste de retours. \n",
    "Tu es chargé de faire remonter auprès de l'entreprise les conlusion de ton enquète, c'est à dire les insights que tu as déduit de l'analyse de ces retours.\n",
    "\n",
    "Effectue les étapes suivantes:\n",
    "\n",
    "Étape 1 - Identification des insights\n",
    "Identifie un insights à faire remonter auprès de ton équipe.\n",
    "Voici les contraintes que l'insights doit respecter:\n",
    "- Une personne de ton équipe qui lit un insight doit pouvoir en comprendre le sens, sans qu'il y ait d'ambiguité.\n",
    "- Un insight doit être aussi court que possible, tout en restant parfaitement compréhensible et pertinent.\n",
    "- N'ajoute pas de bouts de phrases unitiles, comme la conséquence quand celle ci est évidente. Par exemple, inutile d'ajouter des bouts de phrase comme \"..., ce qui entraîne un intérêt moindre pour l'enseigne\"\n",
    "- Les insights apportent des informations distinctes.\n",
    "- Un insight est une information que tu trouves réellement intéressante, pas une simple reformulation du retour client.\n",
    "\n",
    "Étape 2 - Indiquer quels retours sont à l'origine de ton feedback  \n",
    "Associe à chaque insight la liste des indices des retours qui lui sont associés.\n",
    "Par exemple, le retour a pour indice ne nombre 10:\n",
    "'''\n",
    "10 : ras\n",
    "'''\n",
    "\n",
    "Voici le retours à traiter:\n",
    "              \n",
    "10 : pas assez de promos pour petite quantité achetés, volume trop élevé\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FirstInsight(BaseModel):\n",
    "    #model_config = ConfigDict(title='Main')\n",
    "    \n",
    "    associated_indexes: List[int] = Field(description=\"Indices des retours associés.\")\n",
    "    #titre: str = Field(description=\"Titre de l'insight.\")\n",
    "    contenu: str = Field(description=\"Insight qui a été déduit de l'analyse des retours.\") #Field(description=\"Point intéressant a retenir du commentaire.\")\n",
    "\n",
    "    def __str__(self):\n",
    "        return '- ' + self.content + \"\\nTypes: \" + ', '.join(self.insight_types)\n",
    "    \n",
    "class InsightsList(BaseModel):\n",
    "    insights_list: List[FirstInsight] = Field(description=\"Liste des insights.\")\n",
    "\n",
    "output_format = InsightsList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system_message\n",
      "You are a helpful assistant.\n",
      "Respond with respect to the following JSON format: {'$defs': {'FirstInsight': {'properties': {'associated_indexes': {'description': 'Indices des retours associés.', 'items': {'type': 'integer'}, 'title': 'Associated Indexes', 'type': 'array'}, 'contenu': {'description': \"Insight qui a été déduit de l'analyse des retours.\", 'title': 'Contenu', 'type': 'string'}}, 'required': ['associated_indexes', 'contenu'], 'title': 'FirstInsight', 'type': 'object'}}, 'properties': {'insights_list': {'description': 'Liste des insights.', 'items': {'$ref': '#/$defs/FirstInsight'}, 'title': 'Insights List', 'type': 'array'}}, 'required': ['insights_list'], 'title': 'InsightsList', 'type': 'object'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      "{'id': 'chatcmpl-2bcec634-4eba-46d8-8e60-e83719a84ac5', 'object': 'chat.completion', 'created': 1707243351, 'model': '/media/maitre/HDD1/Models/mixtral-8x7b-instruct-v0.1.Q3_K_M.gguf', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': ' Étape 1 - Identification des insights :\\n\\nInsight 1 :\\nContenu : Les clients souhaitent bénéficier de promotions adaptées aux petites quantités achetées, car ils estiment que le volume actuel est trop élevé.\\n\\nInsight 2 :\\nContenu : Les clients demandent la mise en place d\\'un programme de fidélité.\\n\\nInsight 3 :\\nContenu : Les clients constatent une diminution de l\\'assortiment en livraison et aimeraient que celui-ci soit rétabli, en particulier l\\'offre \"Achetez plus\".\\n\\nInsight 4 :\\nContenu : Les clients perçoivent une irrégularité dans la qualité et la fraîcheur de certains produits, notamment les salades iceberg et les framboises fraîches. Ils s\\'interrogent également sur les prix plus élevés de METRO par rapport aux supermarchés pour les bananes, les framboises fraîches, les icebergs et les tomates.\\n\\nInsight 5 :\\nContenu : Les clients attendent une meilleure politesse de la part du nouveau directeur du magasin METRO de Caen, en particulier un accueil plus chaleureux avec un \"bonjour\".\\n\\nÉtape 2 - Indiquer quels retours sont à l\\'origine de chaque feedback :\\n\\nInsight 1 : [10]\\nInsight 2 : [11]\\nInsight 3 : [12]\\nInsight 4 : [13]\\nInsight 5 : [14]'}, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 1115, 'completion_tokens': 380, 'total_tokens': 1495}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1562.29 ms\n",
      "llama_print_timings:      sample time =      98.98 ms /   381 runs   (    0.26 ms per token,  3849.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3738.05 ms /  1115 tokens (    3.35 ms per token,   298.28 tokens per second)\n",
      "llama_print_timings:        eval time =   10354.15 ms /   380 runs   (   27.25 ms per token,    36.70 tokens per second)\n",
      "llama_print_timings:       total time =   14926.71 ms /  1495 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Étape 1 - Identification des insights :\\n\\nInsight 1 :\\nContenu : Les clients souhaitent bénéficier de promotions adaptées aux petites quantités achetées, car ils estiment que le volume actuel est trop élevé.\\n\\nInsight 2 :\\nContenu : Les clients demandent la mise en place d\\'un programme de fidélité.\\n\\nInsight 3 :\\nContenu : Les clients constatent une diminution de l\\'assortiment en livraison et aimeraient que celui-ci soit rétabli, en particulier l\\'offre \"Achetez plus\".\\n\\nInsight 4 :\\nContenu : Les clients perçoivent une irrégularité dans la qualité et la fraîcheur de certains produits, notamment les salades iceberg et les framboises fraîches. Ils s\\'interrogent également sur les prix plus élevés de METRO par rapport aux supermarchés pour les bananes, les framboises fraîches, les icebergs et les tomates.\\n\\nInsight 5 :\\nContenu : Les clients attendent une meilleure politesse de la part du nouveau directeur du magasin METRO de Caen, en particulier un accueil plus chaleureux avec un \"bonjour\".\\n\\nÉtape 2 - Indiquer quels retours sont à l\\'origine de chaque feedback :\\n\\nInsight 1 : [10]\\nInsight 2 : [11]\\nInsight 3 : [12]\\nInsight 4 : [13]\\nInsight 5 : [14]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.generate(prompt, output_format)\n",
    "response['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Étape 1 - Identification des insights :\n",
      "\n",
      "Insight 1 :\n",
      "Contenu : Les clients souhaitent bénéficier de promotions adaptées aux petites quantités achetées, car ils estiment que le volume actuel est trop élevé.\n",
      "\n",
      "Insight 2 :\n",
      "Contenu : Les clients demandent la mise en place d'un programme de fidélité.\n",
      "\n",
      "Insight 3 :\n",
      "Contenu : Les clients constatent une diminution de l'assortiment en livraison et aimeraient que celui-ci soit rétabli, en particulier l'offre \"Achetez plus\".\n",
      "\n",
      "Insight 4 :\n",
      "Contenu : Les clients perçoivent une irrégularité dans la qualité et la fraîcheur de certains produits, notamment les salades iceberg et les framboises fraîches. Ils s'interrogent également sur les prix plus élevés de METRO par rapport aux supermarchés pour les bananes, les framboises fraîches, les icebergs et les tomates.\n",
      "\n",
      "Insight 5 :\n",
      "Contenu : Les clients attendent une meilleure politesse de la part du nouveau directeur du magasin METRO de Caen, en particulier un accueil plus chaleureux avec un \"bonjour\".\n",
      "\n",
      "Étape 2 - Indiquer quels retours sont à l'origine de chaque feedback :\n",
      "\n",
      "Insight 1 : [10]\n",
      "Insight 2 : [11]\n",
      "Insight 3 : [12]\n",
      "Insight 4 : [13]\n",
      "Insight 5 : [14]\n"
     ]
    }
   ],
   "source": [
    "print(response['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-f95257e6-ecf9-46e3-98ca-da47cde3e1d0', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Je suis désolé, mais je ne suis pas programmé pour répondre à des demandes de ce type. Mon rôle consiste à fournir des informations utiles et à traiter les données que je reçois pour générer des réponses appropriées en fonction de la demande. Dans le cadre de mon fonctionnement actuel, je ne peux pas vous conseiller sur la manière de manger de l'herbe avec les pieds. Cependant, si vous avez des questions d'ordre informatique ou des demandes liées à l'analyse de données ou à la création de fichiers JSON, je serai heureux de vous aider.\", role='assistant', function_call=None, tool_calls=None))], created=1707299846, model='mixtral', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=157, prompt_tokens=101, total_tokens=258))\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import instructor\n",
    "\n",
    "client = OpenAI(base_url=\"http://localhost:8000/v1\", api_key=\"sk-xxx\")\n",
    "client = instructor.patch(client)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"mixtral\",\n",
    "    messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Tu est un assistant spélialisé dans l'analyse de commentaires, et qui ne renvoit que des fichiers JSON.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Comment manger de l'herbe avec les pieds?\"},\n",
    "        ],\n",
    ")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_analysis(prompt, response_model):\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Tu est un assistant spélialisé dans l'analyse de commentaires\"},#, et qui ne renvoit que des fichiers JSON.\"},\n",
    "            {\"role\": \"user\", \"content\": str(prompt)},\n",
    "        ],\n",
    "        model=\"mixtral\",\n",
    "        response_format={ \"type\": \"json_object\" },\n",
    "        response_model=response_model,\n",
    "        )\n",
    "    return response\n",
    "\n",
    "\n",
    "def apply_analysis(prompts, response_models):\n",
    "    if type(response_models) is not list:\n",
    "        response_models = [response_models for _ in prompts]\n",
    "    res = []\n",
    "    for (prompt, response_model) in zip(prompts, response_models):\n",
    "        res.append(get_analysis(prompt, response_model))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InsightsList(insights_list=[FirstInsight(associated_indexes=[10], contenu='Les clients trouvent que les promotions sont insuffisantes pour les petites quantités achetées et que les volumes sont trop élevés.'), FirstInsight(associated_indexes=[11], contenu=\"Les clients demandent l'implémentation d'un système de points de fidélité.\"), FirstInsight(associated_indexes=[12], contenu=\"Les clients déplorent la réduction de l'assortiment en livraison et la non-disponibilité de l'offre 'Achetez plus' en livraison.\"), FirstInsight(associated_indexes=[13], contenu='Les clients remarquent des irrégularités dans la qualité et la fraîcheur des produits (salades iceberg et framboises fraîches). Ils trouvent que les prix de METRO sont plus élevés que ceux des supermarchés pour certains produits (bananes, framboises fraîches, icebergs, tomates).'), FirstInsight(associated_indexes=[14], contenu='Les clients trouvent que le nouveau directeur de METRO CAEN devrait apprendre à saluer ses clients.')])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_analysis(prompt, InsightsList)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "feedback_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
