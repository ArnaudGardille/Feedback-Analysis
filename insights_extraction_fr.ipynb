{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xjPRVXra8lVZ"
   },
   "source": [
    "# Dependancies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKSREXZg85yy"
   },
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zMSukVZh29HK"
   },
   "outputs": [],
   "source": [
    "#!pip install sentence_transformers langchain openai tqdm datasets asyncio scikit-learn cohere tiktoken umap altair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Md3DNHlV22Ai"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_dataset\n",
    "import umap\n",
    "import altair as alt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List\n",
    "import enum\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.output_parsers.regex_dict import RegexDictParser\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, ChatMessage\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from pydantic import BaseModel, Field, validator, create_model\n",
    "from openai import AsyncOpenAI, OpenAI\n",
    "#import asyncio\n",
    "import os\n",
    "\n",
    "import requests\n",
    "\n",
    "from pydantic import BaseModel, ValidationInfo, model_validator\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import umap.umap_ as umap\n",
    "#import umap\n",
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved company Darty : 1707313014508x102198350946437700\n",
      "Retrieved project Darty_trustpilot : 1707329196900x870734705097005300\n"
     ]
    }
   ],
   "source": [
    "from src.bubble import *\n",
    "from src.models import *\n",
    "from src.utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_path = \"Prompts/fr/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqsmhIToV_yX"
   },
   "source": [
    "## Bubble API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedbacks_df = get(\"Feedback\", max_objects=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "types_df = get(\"Type\", constraints=[])\n",
    "categories_df = get(\"Category\")\n",
    "original_subcategories_df = get(\"SubCategory\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_infos = bubble_client.get(\n",
    "    \"Company\",\n",
    "    bubble_id=COMPANY_ID,\n",
    ")\n",
    "project_infos = bubble_client.get(\n",
    "    \"Project\",\n",
    "    bubble_id=PROJECT_ID,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Modified Date</th>\n",
       "      <th>Created Date</th>\n",
       "      <th>Created By</th>\n",
       "      <th>Name</th>\n",
       "      <th>Description</th>\n",
       "      <th>_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-02-07 13:29:49.626000+00:00</td>\n",
       "      <td>2024-02-07 13:27:22.244000+00:00</td>\n",
       "      <td>admin_user_feedback-analysis_test</td>\n",
       "      <td>Point positif</td>\n",
       "      <td>Élément apprécié par le client ou l'utilisateur</td>\n",
       "      <td>1707312442244x926718270622733700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-02-07 13:29:36.367000+00:00</td>\n",
       "      <td>2024-02-07 13:27:32.845000+00:00</td>\n",
       "      <td>admin_user_feedback-analysis_test</td>\n",
       "      <td>Nouvelle fonctionnalité</td>\n",
       "      <td>Suggestion d'évolution faite par le client ou ...</td>\n",
       "      <td>1707312452845x942796991724685900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-02-07 13:29:20.520000+00:00</td>\n",
       "      <td>2024-02-07 13:27:41.737000+00:00</td>\n",
       "      <td>admin_user_feedback-analysis_test</td>\n",
       "      <td>Point de douleur</td>\n",
       "      <td>Problème qui gène ou ennuie le client ou l'uti...</td>\n",
       "      <td>1707312461737x150602429464921100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-02-07 13:30:08.388000+00:00</td>\n",
       "      <td>2024-02-07 13:30:08.385000+00:00</td>\n",
       "      <td>admin_user_feedback-analysis_test</td>\n",
       "      <td>Bug</td>\n",
       "      <td>Anomalie de fonctionnement de l'application dé...</td>\n",
       "      <td>1707312608385x190928778572006180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Modified Date                     Created Date  \\\n",
       "0 2024-02-07 13:29:49.626000+00:00 2024-02-07 13:27:22.244000+00:00   \n",
       "1 2024-02-07 13:29:36.367000+00:00 2024-02-07 13:27:32.845000+00:00   \n",
       "2 2024-02-07 13:29:20.520000+00:00 2024-02-07 13:27:41.737000+00:00   \n",
       "3 2024-02-07 13:30:08.388000+00:00 2024-02-07 13:30:08.385000+00:00   \n",
       "\n",
       "                          Created By                     Name  \\\n",
       "0  admin_user_feedback-analysis_test            Point positif   \n",
       "1  admin_user_feedback-analysis_test  Nouvelle fonctionnalité   \n",
       "2  admin_user_feedback-analysis_test         Point de douleur   \n",
       "3  admin_user_feedback-analysis_test                      Bug   \n",
       "\n",
       "                                         Description  \\\n",
       "0    Élément apprécié par le client ou l'utilisateur   \n",
       "1  Suggestion d'évolution faite par le client ou ...   \n",
       "2  Problème qui gène ou ennuie le client ou l'uti...   \n",
       "3  Anomalie de fonctionnement de l'application dé...   \n",
       "\n",
       "                                _id  \n",
       "0  1707312442244x926718270622733700  \n",
       "1  1707312452845x942796991724685900  \n",
       "2  1707312461737x150602429464921100  \n",
       "3  1707312608385x190928778572006180  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "types_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Type de l'insight.POINT_POSITIF: 'Point positif'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TypeInsight = enum.Enum(\"Type de l'insight\", [(convert_text_to_constants(x), x) for x in types_df.Name])\n",
    "TypeInsight(\"Point positif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Type de l'insight.POINT_POSITIF: 'Point positif'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TypeInsight.POINT_POSITIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TypeInsight = enum.Enum(\"Type de l'insight\", [(convert_text_to_constants(x), x) for x in types_df.Name])\n",
    "CategoryInsight = enum.Enum(\"Categories de l'insight\", [(convert_text_to_constants(x), x) for x in categories_df.Name])\n",
    "dict_SubCategoriesInsight = {\n",
    "    row[\"Name\"]:enum.Enum(\"Categories de l'insight\", [(convert_text_to_constants(x), x) for x in original_subcategories_df[original_subcategories_df[\"Category\"] == row[\"_id\"]].Name])\n",
    "    for _,row in categories_df.iterrows()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Created Date</th>\n",
       "      <th>Created By</th>\n",
       "      <th>Modified Date</th>\n",
       "      <th>Company</th>\n",
       "      <th>Name</th>\n",
       "      <th>Project</th>\n",
       "      <th>_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-02-15 16:45:53.720000+00:00</td>\n",
       "      <td>admin_user_feedback-analysis_test</td>\n",
       "      <td>2024-02-15 16:45:53.721000+00:00</td>\n",
       "      <td>1707313014508x102198350946437700</td>\n",
       "      <td>Recherche de Produit</td>\n",
       "      <td>1707329196900x870734705097005300</td>\n",
       "      <td>1708015553720x307677447830442240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-02-15 16:45:55.422000+00:00</td>\n",
       "      <td>admin_user_feedback-analysis_test</td>\n",
       "      <td>2024-02-15 16:45:55.422000+00:00</td>\n",
       "      <td>1707313014508x102198350946437700</td>\n",
       "      <td>Processus d'Achat</td>\n",
       "      <td>1707329196900x870734705097005300</td>\n",
       "      <td>1708015555422x783692141072623600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-02-15 16:46:09.716000+00:00</td>\n",
       "      <td>admin_user_feedback-analysis_test</td>\n",
       "      <td>2024-02-15 16:46:09.716000+00:00</td>\n",
       "      <td>1707313014508x102198350946437700</td>\n",
       "      <td>Livraison</td>\n",
       "      <td>1707329196900x870734705097005300</td>\n",
       "      <td>1708015569716x598042261554836900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-02-15 16:46:10.991000+00:00</td>\n",
       "      <td>admin_user_feedback-analysis_test</td>\n",
       "      <td>2024-02-15 16:46:10.991000+00:00</td>\n",
       "      <td>1707313014508x102198350946437700</td>\n",
       "      <td>Service Après-Vente</td>\n",
       "      <td>1707329196900x870734705097005300</td>\n",
       "      <td>1708015570991x523003420569127600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-02-15 16:46:17.252000+00:00</td>\n",
       "      <td>admin_user_feedback-analysis_test</td>\n",
       "      <td>2024-02-15 16:46:17.252000+00:00</td>\n",
       "      <td>1707313014508x102198350946437700</td>\n",
       "      <td>Expérience en Magasin</td>\n",
       "      <td>1707329196900x870734705097005300</td>\n",
       "      <td>1708015577252x623767797641778300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Created Date                         Created By  \\\n",
       "0 2024-02-15 16:45:53.720000+00:00  admin_user_feedback-analysis_test   \n",
       "1 2024-02-15 16:45:55.422000+00:00  admin_user_feedback-analysis_test   \n",
       "2 2024-02-15 16:46:09.716000+00:00  admin_user_feedback-analysis_test   \n",
       "3 2024-02-15 16:46:10.991000+00:00  admin_user_feedback-analysis_test   \n",
       "4 2024-02-15 16:46:17.252000+00:00  admin_user_feedback-analysis_test   \n",
       "\n",
       "                     Modified Date                           Company  \\\n",
       "0 2024-02-15 16:45:53.721000+00:00  1707313014508x102198350946437700   \n",
       "1 2024-02-15 16:45:55.422000+00:00  1707313014508x102198350946437700   \n",
       "2 2024-02-15 16:46:09.716000+00:00  1707313014508x102198350946437700   \n",
       "3 2024-02-15 16:46:10.991000+00:00  1707313014508x102198350946437700   \n",
       "4 2024-02-15 16:46:17.252000+00:00  1707313014508x102198350946437700   \n",
       "\n",
       "                    Name                           Project  \\\n",
       "0   Recherche de Produit  1707329196900x870734705097005300   \n",
       "1      Processus d'Achat  1707329196900x870734705097005300   \n",
       "2              Livraison  1707329196900x870734705097005300   \n",
       "3    Service Après-Vente  1707329196900x870734705097005300   \n",
       "4  Expérience en Magasin  1707329196900x870734705097005300   \n",
       "\n",
       "                                _id  \n",
       "0  1708015553720x307677447830442240  \n",
       "1  1708015555422x783692141072623600  \n",
       "2  1708015569716x598042261554836900  \n",
       "3  1708015570991x523003420569127600  \n",
       "4  1708015577252x623767797641778300  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Category</th>\n",
       "      <th>Company</th>\n",
       "      <th>Description</th>\n",
       "      <th>_id</th>\n",
       "      <th>Category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Facilité de Recherche</td>\n",
       "      <td>Recherche de Produit</td>\n",
       "      <td>1707313014508x102198350946437700</td>\n",
       "      <td>Évalue la simplicité avec laquelle les clients...</td>\n",
       "      <td>1708015554131x995344137291493100</td>\n",
       "      <td>1708015553720x307677447830442240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Information Produit</td>\n",
       "      <td>Recherche de Produit</td>\n",
       "      <td>1707313014508x102198350946437700</td>\n",
       "      <td>Concerne la qualité et la quantité des informa...</td>\n",
       "      <td>1708015554529x444035970008549250</td>\n",
       "      <td>1708015553720x307677447830442240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Comparaison de Produits</td>\n",
       "      <td>Recherche de Produit</td>\n",
       "      <td>1707313014508x102198350946437700</td>\n",
       "      <td>Décrit comment les clients perçoivent les outi...</td>\n",
       "      <td>1708015555012x394265267821973400</td>\n",
       "      <td>1708015553720x307677447830442240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Facilité de Navigation</td>\n",
       "      <td>Processus d'Achat</td>\n",
       "      <td>1707313014508x102198350946437700</td>\n",
       "      <td>Se rapporte à l'expérience utilisateur en mati...</td>\n",
       "      <td>1708015555741x283185418299470850</td>\n",
       "      <td>1708015555422x783692141072623600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Options de Paiement</td>\n",
       "      <td>Processus d'Achat</td>\n",
       "      <td>1707313014508x102198350946437700</td>\n",
       "      <td>Concerne la diversité et la sécurité des optio...</td>\n",
       "      <td>1708015568900x728837610303776400</td>\n",
       "      <td>1708015555422x783692141072623600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Name              Category  \\\n",
       "0    Facilité de Recherche  Recherche de Produit   \n",
       "1      Information Produit  Recherche de Produit   \n",
       "2  Comparaison de Produits  Recherche de Produit   \n",
       "3   Facilité de Navigation     Processus d'Achat   \n",
       "4      Options de Paiement     Processus d'Achat   \n",
       "\n",
       "                            Company  \\\n",
       "0  1707313014508x102198350946437700   \n",
       "1  1707313014508x102198350946437700   \n",
       "2  1707313014508x102198350946437700   \n",
       "3  1707313014508x102198350946437700   \n",
       "4  1707313014508x102198350946437700   \n",
       "\n",
       "                                         Description  \\\n",
       "0  Évalue la simplicité avec laquelle les clients...   \n",
       "1  Concerne la qualité et la quantité des informa...   \n",
       "2  Décrit comment les clients perçoivent les outi...   \n",
       "3  Se rapporte à l'expérience utilisateur en mati...   \n",
       "4  Concerne la diversité et la sécurité des optio...   \n",
       "\n",
       "                                _id                       Category_id  \n",
       "0  1708015554131x995344137291493100  1708015553720x307677447830442240  \n",
       "1  1708015554529x444035970008549250  1708015553720x307677447830442240  \n",
       "2  1708015555012x394265267821973400  1708015553720x307677447830442240  \n",
       "3  1708015555741x283185418299470850  1708015555422x783692141072623600  \n",
       "4  1708015568900x728837610303776400  1708015555422x783692141072623600  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df  = categories_df.copy()\n",
    "df['Category'] = df['_id'].astype(str)\n",
    "original_subcategories_df['Category'] = original_subcategories_df['Category'].astype(str)\n",
    "df = pd.merge(original_subcategories_df, df, on=[\"Category\", \"Company\"])\n",
    "df = df[[\"Name_x\", \"Name_y\", \"Company\", \"Description\", \"_id_x\", \"_id_y\"]]\n",
    "df.columns = [\"Name\", \"Category\",  \"Company\", \"Description\", \"_id\", \"Category_id\"]\n",
    "subcategories_df = df\n",
    "subcategories_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "SubCategoriyInsight = enum.Enum(\"Categories de l'insight\", [(row[\"Category\"]+\" : \"+row[\"Name\"], convert_text_to_constants(row[\"Category\"]+\" : \"+row[\"Name\"])) for _, row in subcategories_df.iterrows()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Point positif : Élément apprécié par le client ou l'utilisateur\n",
      "Nouvelle fonctionnalité : Suggestion d'évolution faite par le client ou l'utilisateur\n",
      "Point de douleur : Problème qui gène ou ennuie le client ou l'utilisateur\n",
      "Bug : Anomalie de fonctionnement de l'application détectée par l'utilisateur\n"
     ]
    }
   ],
   "source": [
    "types_descr = columns_to_string(types_df, \"Name\", \"Description\")\n",
    "print(types_descr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Facilité de Recherche : Évalue la simplicité avec laquelle les clients peuvent trouver les produits qu'ils cherchent.\n",
      "Information Produit : Concerne la qualité et la quantité des informations fournies sur les produits.\n",
      "Comparaison de Produits : Décrit comment les clients perçoivent les outils et les informations qui les aident à comparer différents produits.\n",
      "Facilité de Navigation : Se rapporte à l'expérience utilisateur en matière de navigation durant le processus d'achat.\n",
      "Options de Paiement : Concerne la diversité et la sécurité des options de paiement disponibles.\n",
      "Processus de Validation : Évalue l'efficacité et la clarté du processus de validation de la commande.\n",
      "Options de Livraison : Évalue les différentes options de livraison proposées (rapidité, coût, fiabilité).\n",
      "Suivi de Commande : Concerne la capacité à suivre l'état de la commande en temps réel.\n",
      "Respect des Délais : Décrit si les produits sont livrés dans les délais annoncés.\n",
      "Support Client : Concerne la qualité de l'assistance fournie par le service client.\n",
      "Politique de Retour : Évalue la facilité avec laquelle les clients peuvent retourner des produits et la clarté de la politique de retour.\n",
      "Garantie et Réparations : Décrit les services de garantie et de réparation offerts pour les produits.\n",
      "Amabilité du Personnel : Évalue la courtoisie et la disponibilité du personnel en magasin.\n",
      "Environnement de Magasin : Concerne l'atmosphère et l'agencement du magasin, ainsi que la facilité de circulation.\n",
      "Disponibilité des Produits : Décrit si les produits désirés sont en stock et facilement accessibles en magasin.\n"
     ]
    }
   ],
   "source": [
    "tags_descr = columns_to_string(subcategories_df, \"Name\", \"Description\")\n",
    "print(tags_descr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entreprise': 'Darty',\n",
       " 'context': 'Fondée en 1957, Darty est une enseigne française spécialisée dans la distribution d\\'électroménager, d\\'équipements électroniques et de produits culturels. Rachetée par la Fnac en 2016, elle est aujourd\\'hui l\\'un des leaders européens de la distribution omnicanale.\\n\\nÉvènements récents:\\n\\n    2016: Rachat par la Fnac et création du groupe Fnac Darty.\\n    2017: Lancement de la marketplace Darty.com.\\n    2018: Déploiement du \"Contrat de Confiance Fnac Darty\" dans tous les magasins.\\n    2019: Lancement de l\\'offre de services \"Darty+.\"\\n    2020: Accélération de la transformation digitale du groupe.\\n    2021: Acquisition de Mistergooddeal, spécialiste du e-commerce en produits reconditionnés.\\n    2022: Lancement de la Fnac Darty Academy, une plateforme de formation en ligne.\\n\\nConcurrents:\\n\\n    Boulanger\\n    Conforama\\n    Gitem\\n    Amazon\\n    Cdiscount\\n\\nEnjeux:\\n\\n    Darty doit faire face à une concurrence accrue sur le marché de l\\'électroménager et de l\\'électronique.\\n    L\\'entreprise doit poursuivre sa transformation digitale pour répondre aux attentes des clients.\\n    Darty doit continuer à se différencier par son service client et son expertise.',\n",
       " 'role': 'analyste marketing',\n",
       " 'cible': 'client',\n",
       " 'insight_types': \"Point positif : Élément apprécié par le client ou l'utilisateur\\nNouvelle fonctionnalité : Suggestion d'évolution faite par le client ou l'utilisateur\\nPoint de douleur : Problème qui gène ou ennuie le client ou l'utilisateur\\nBug : Anomalie de fonctionnement de l'application détectée par l'utilisateur\",\n",
       " 'insight_categories': \"Facilité de Recherche : Évalue la simplicité avec laquelle les clients peuvent trouver les produits qu'ils cherchent.\\nInformation Produit : Concerne la qualité et la quantité des informations fournies sur les produits.\\nComparaison de Produits : Décrit comment les clients perçoivent les outils et les informations qui les aident à comparer différents produits.\\nFacilité de Navigation : Se rapporte à l'expérience utilisateur en matière de navigation durant le processus d'achat.\\nOptions de Paiement : Concerne la diversité et la sécurité des options de paiement disponibles.\\nProcessus de Validation : Évalue l'efficacité et la clarté du processus de validation de la commande.\\nOptions de Livraison : Évalue les différentes options de livraison proposées (rapidité, coût, fiabilité).\\nSuivi de Commande : Concerne la capacité à suivre l'état de la commande en temps réel.\\nRespect des Délais : Décrit si les produits sont livrés dans les délais annoncés.\\nSupport Client : Concerne la qualité de l'assistance fournie par le service client.\\nPolitique de Retour : Évalue la facilité avec laquelle les clients peuvent retourner des produits et la clarté de la politique de retour.\\nGarantie et Réparations : Décrit les services de garantie et de réparation offerts pour les produits.\\nAmabilité du Personnel : Évalue la courtoisie et la disponibilité du personnel en magasin.\\nEnvironnement de Magasin : Concerne l'atmosphère et l'agencement du magasin, ainsi que la facilité de circulation.\\nDisponibilité des Produits : Décrit si les produits désirés sont en stock et facilement accessibles en magasin.\"}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example_insight = \"Manque de clarté de l'affichage des prix en magasin\"\n",
    "#exemple_commentaire = \"je suis exclusif metro je n ai aucun representant j achetais jusqu a present tout metro par facilite mais je suis tres souvent décue par la reponse ha non on n en a pas cela arrive demain je pense que depuis le covid tout le monde ou presque s en fou!!!\"\n",
    "#examples_insights_df = pd.DataFrame([\n",
    "#    {\"Insights qui devraient en découler\": \"Déceptions face aux retards de livraison\"},\n",
    "#    {\"Insights qui devraient en découler\": \"Impression d'une baisse de qualité du service depuis le Covid\"},\n",
    "#])\n",
    "\n",
    "feedback_context = {\n",
    "            \"entreprise\": company_infos[\"Name\"],\n",
    "            \"context\": company_infos['Context'],\n",
    "            \"role\": company_infos['Role'],\n",
    "            \"cible\": project_infos['Target'],\n",
    "            \"insight_types\": types_descr,\n",
    "            \"insight_categories\": tags_descr,\n",
    "            #\"question\": project_infos['Study_question'],\n",
    "            #\"exemple_commentaire\": exemple_commentaire,\n",
    "            #\"example_insights\": '\\n- '.join(list(examples_insights_df['Insights qui devraient en découler'])),\n",
    "        }\n",
    "\n",
    "feedback_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Insights extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aspects and Insights creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FeedbackIndex = enum.Enum(\"Indice du retour associé\", [(str(i), i) for i in range(BATCH_SIZE)])\n",
    "\n",
    "class SousCategorie(BaseModel):\n",
    "    indice: int = Field(description=\"Indice de la sous-catégorie. Doit être un entier.\")\n",
    "    nom: str = Field(description=\"nom de la caégorie: nom de la sous-catégorie.\")\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.nom + ' ('+str(self.indice)+')'\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def validate_ids(self, info: ValidationInfo):\n",
    "        context = info.context\n",
    "        if context:\n",
    "            tags: List[SousCategorie] = context.get(\"sous_categories\")\n",
    "            assert self.indice in {\n",
    "                tag.indice for tag in tags\n",
    "            }, f\"sous_categories ID {self.indice} not found in context\"\n",
    "            assert self.nom in {\n",
    "                tag.nom for tag in tags\n",
    "            }, f\"sous_categories name {self.nom} not found in context\"\n",
    "        return self\n",
    "    \n",
    "class SousCategorieAvecDescription(SousCategorie):\n",
    "    categorie : str\n",
    "    description: str\n",
    "\n",
    "\n",
    "class Aspect(BaseModel):\n",
    "    sous_categorie : SousCategorie = Field(description=\"Sous-catégorie concernée.\")\n",
    "    note_satisfaction : int = Field(description=\"Note de satisfaction du client concernant cette sous-catégorie, de 1 (pas content) à 5 (très content).\")\n",
    "    explication: str = Field(description=\"Eventuelle explication du ressenti du client, si celle-ci parait importante à faire remonter au sein de l'entreprise. Doit être aussi claire et concise que possible.\") #Field(description=\"Point intéressant a retenir du commentaire.\")\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        return '\\n' + str(self.sous_categorie) + '\\nSatisfaction: ' + str(self.note_satisfaction) + \"/5\\nExplication: \" + self.explication\n",
    "    \n",
    "    @model_validator(mode=\"after\")\n",
    "    def validate_ids(self, info: ValidationInfo):\n",
    "        assert (0 <= self.note_satisfaction) and (self.note_satisfaction <= 5)\n",
    "        return self\n",
    "\n",
    "class ListAspects(BaseModel):\n",
    "    list_aspects: List[Aspect] = Field(description=\"Liste des différents aspects évoqués dans le feedback.\")\n",
    "\n",
    "    def __str__(self):\n",
    "        return '\\n'.join([str(x) for x in self.list_aspects])\n",
    "    \n",
    "\n",
    "class AspectsRequest(BaseModel):\n",
    "    texts: List[str]\n",
    "    sous_categories: List[SousCategorieAvecDescription]\n",
    "\n",
    "\n",
    "class AspectsResponse(BaseModel):\n",
    "    texts: List[str]\n",
    "    predictions: List[Aspect]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(prompts_path+'prompt_aspects.txt') as f:\n",
    "    prompt_aspects = PromptTemplate.from_template(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "AZURE = False\n",
    "if LLL_PROVIDER == \"OPEN_AI\":\n",
    "    EMBEDDING_ENGINE = \"text-embedding-3-large\"\n",
    "    if AZURE:\n",
    "        from openai import AzureOpenAI, AsyncAzureOpenAI\n",
    "        client = AsyncAzureOpenAI(azure_endpoint=\"https://vigieinstance.openai.azure.com/\",\n",
    "        api_version=\"2023-07-01-preview\",\n",
    "        api_key=\"6e612f025340400d827a519b0549cff6\")\n",
    "        GENERATION_ENGINE = \"dep\"\n",
    "    \n",
    "    else:\n",
    "        from openai import OpenAI, AsyncOpenAI\n",
    "        client = AsyncOpenAI(\n",
    "            api_key=\"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\",\n",
    "            organization=\"org-EYbk8L8UD8kpRGOeDarxXD55\",\n",
    "        )\n",
    "        GENERATION_ENGINE = \"gpt-4-turbo-preview\"#\"gpt-3.5-turbo-0125\"\n",
    "\n",
    "    client = instructor.patch(client)\n",
    "\n",
    "    async def get_async_analysis(prompt, response_model):\n",
    "        response: response_model = await client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Tu est un assistant spélialisé dans l'analyse de commentaires, et qui ne renvoit que des fichiers JSON.\"},\n",
    "                {\"role\": \"user\", \"content\": str(prompt)},\n",
    "            ],\n",
    "            response_format={ \"type\": \"json_object\" },\n",
    "            model=GENERATION_ENGINE,\n",
    "            temperature=TEMPERATURE,\n",
    "            #max_retries=MAX_RETRIES,\n",
    "            response_model=response_model,\n",
    "            )\n",
    "        return response #.choices[0].message.content\n",
    "\n",
    "    def apply_async_analysis(prompts, response_models):\n",
    "        if type(response_models) is not list:\n",
    "            response_models = [response_models for _ in prompts]\n",
    "        loop = asyncio.get_event_loop()\n",
    "        tasks = [loop.create_task(get_async_analysis(prompt, response_model)) for (prompt, response_model) in zip(prompts, response_models)]\n",
    "        res =  loop.run_until_complete(asyncio.gather(*tasks))\n",
    "        return res\n",
    "    \n",
    "    \n",
    "    \n",
    "elif LLL_PROVIDER == \"MISTRAL_AI\":\n",
    "    MISTRAL_API_KEY = \"GFBjsGogmbv0LuMWjJewXBXwyN7QeKNj\"\n",
    "    EMBEDDING_ENGINE = \"\"#\"text-embedding-3-large\"\n",
    "    GENERATION_ENGINE = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "    from mistralai.async_client import MistralClient\n",
    "    from mistralai.models.chat_completion import ChatMessage\n",
    "\n",
    "    api_key = MISTRAL_API_KEY\n",
    "    model = \"mistral-tiny\"\n",
    "\n",
    "    client = MistralClient(api_key=api_key)\n",
    "    \n",
    "\n",
    "    messages = [\n",
    "        ChatMessage(role=\"user\", content=\"What is the best French cheese?\")\n",
    "    ]\n",
    "\n",
    "\n",
    "    async def get_async_analysis(prompt, response_model):\n",
    "        response: response_model = await client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Tu est un assistant spélialisé dans l'analyse de commentaires, et qui ne renvoit que des fichiers JSON.\"},\n",
    "                {\"role\": \"user\", \"content\": str(prompt)},\n",
    "            ],\n",
    "            #response_format={ \"type\": \"json_object\" },\n",
    "            model=GENERATION_ENGINE,\n",
    "            temperature=TEMPERATURE,\n",
    "            max_retries=MAX_RETRIES,\n",
    "            response_model=response_model,\n",
    "            )\n",
    "        return response #.choices[0].message.content\n",
    "\n",
    "    def apply_async_analysis(prompts, response_models):\n",
    "        if type(response_models) is not list:\n",
    "            response_models = [response_models for _ in prompts]\n",
    "        loop = asyncio.get_event_loop()\n",
    "        tasks = [loop.create_task(get_async_analysis(prompt, response_model)) for (prompt, response_model) in zip(prompts, response_models)]\n",
    "        res =  loop.run_until_complete(asyncio.gather(*tasks))\n",
    "        return res\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "449958ce9d7949b982e55b93208ba1fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'code': '429', 'message': 'Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2023-07-01-preview have exceeded token rate limit of your current OpenAI S0 pricing tier. Please retry after 40 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m     feedbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(feedbacks_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     11\u001b[0m     prompts \u001b[38;5;241m=\u001b[39m [prompt_aspects\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeedback\u001b[39m\u001b[38;5;124m\"\u001b[39m: feedback, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubcategories\u001b[39m\u001b[38;5;124m\"\u001b[39m: subcategories})\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m feedback \u001b[38;5;129;01min\u001b[39;00m feedbacks]\n\u001b[0;32m---> 13\u001b[0m     aspects \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m apply_async_analysis(prompts, ListAspects)\n\u001b[1;32m     14\u001b[0m     sleep(\u001b[38;5;241m60\u001b[39m)\n\u001b[1;32m     15\u001b[0m aspects\n",
      "Cell \u001b[0;32mIn[43], line 40\u001b[0m, in \u001b[0;36mapply_async_analysis\u001b[0;34m(prompts, response_models)\u001b[0m\n\u001b[1;32m     38\u001b[0m loop \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mget_event_loop()\n\u001b[1;32m     39\u001b[0m tasks \u001b[38;5;241m=\u001b[39m [loop\u001b[38;5;241m.\u001b[39mcreate_task(get_async_analysis(prompt, response_model)) \u001b[38;5;28;01mfor\u001b[39;00m (prompt, response_model) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(prompts, response_models)]\n\u001b[0;32m---> 40\u001b[0m res \u001b[38;5;241m=\u001b[39m  loop\u001b[38;5;241m.\u001b[39mrun_until_complete(asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks))\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/feedback_env/lib/python3.11/site-packages/nest_asyncio.py:90\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39mresult()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/feedback_env/lib/python3.11/asyncio/tasks.py:277\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "Cell \u001b[0;32mIn[43], line 22\u001b[0m, in \u001b[0;36mget_async_analysis\u001b[0;34m(prompt, response_model)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_async_analysis\u001b[39m(prompt, response_model):\n\u001b[0;32m---> 22\u001b[0m     response: response_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     23\u001b[0m         messages\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     24\u001b[0m             {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTu est un assistant spélialisé dans l\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manalyse de commentaires, et qui ne renvoit que des fichiers JSON.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     25\u001b[0m             {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(prompt)},\n\u001b[1;32m     26\u001b[0m         ],\n\u001b[1;32m     27\u001b[0m         response_format\u001b[38;5;241m=\u001b[39m{ \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson_object\u001b[39m\u001b[38;5;124m\"\u001b[39m },\n\u001b[1;32m     28\u001b[0m         model\u001b[38;5;241m=\u001b[39mGENERATION_ENGINE,\n\u001b[1;32m     29\u001b[0m         temperature\u001b[38;5;241m=\u001b[39mTEMPERATURE,\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;66;03m#max_retries=MAX_RETRIES,\u001b[39;00m\n\u001b[1;32m     31\u001b[0m         response_model\u001b[38;5;241m=\u001b[39mresponse_model,\n\u001b[1;32m     32\u001b[0m         )\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/feedback_env/lib/python3.11/site-packages/instructor/patch.py:370\u001b[0m, in \u001b[0;36mwrap_chatcompletion.<locals>.new_chatcompletion_async\u001b[0;34m(response_model, validation_context, max_retries, *args, **kwargs)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_chatcompletion_async\u001b[39m(\n\u001b[1;32m    361\u001b[0m     response_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    366\u001b[0m ):\n\u001b[1;32m    367\u001b[0m     response_model, new_kwargs \u001b[38;5;241m=\u001b[39m handle_response_model(\n\u001b[1;32m    368\u001b[0m         response_model\u001b[38;5;241m=\u001b[39mresponse_model, kwargs\u001b[38;5;241m=\u001b[39mkwargs, mode\u001b[38;5;241m=\u001b[39mmode\n\u001b[1;32m    369\u001b[0m     )  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 370\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m retry_async(\n\u001b[1;32m    371\u001b[0m         func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m    372\u001b[0m         response_model\u001b[38;5;241m=\u001b[39mresponse_model,\n\u001b[1;32m    373\u001b[0m         validation_context\u001b[38;5;241m=\u001b[39mvalidation_context,\n\u001b[1;32m    374\u001b[0m         max_retries\u001b[38;5;241m=\u001b[39mmax_retries,\n\u001b[1;32m    375\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m    376\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mnew_kwargs,\n\u001b[1;32m    377\u001b[0m         mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[1;32m    378\u001b[0m     )  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/feedback_env/lib/python3.11/site-packages/instructor/patch.py:237\u001b[0m, in \u001b[0;36mretry_async\u001b[0;34m(func, response_model, validation_context, args, kwargs, max_retries, strict, mode)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m retries \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m max_retries:\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 237\u001b[0m         response: ChatCompletion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    238\u001b[0m         stream \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, ChatCompletion) \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39musage \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/feedback_env/lib/python3.11/site-packages/openai/resources/chat/completions.py:1330\u001b[0m, in \u001b[0;36mAsyncCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1278\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m   1280\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1328\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m   1329\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m AsyncStream[ChatCompletionChunk]:\n\u001b[0;32m-> 1330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m   1331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1332\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[1;32m   1333\u001b[0m             {\n\u001b[1;32m   1334\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m   1335\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m   1336\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m   1337\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m   1338\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m   1339\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m   1340\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m   1341\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m   1342\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m   1343\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m   1344\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m   1345\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m   1346\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m   1347\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m   1348\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m   1349\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m   1350\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m   1351\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m   1352\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m   1353\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m   1354\u001b[0m             },\n\u001b[1;32m   1355\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m   1356\u001b[0m         ),\n\u001b[1;32m   1357\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m   1358\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m   1359\u001b[0m         ),\n\u001b[1;32m   1360\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m   1361\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1362\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mAsyncStream[ChatCompletionChunk],\n\u001b[1;32m   1363\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/feedback_env/lib/python3.11/site-packages/openai/_base_client.py:1725\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1711\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1712\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1713\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1720\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1721\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _AsyncStreamT:\n\u001b[1;32m   1722\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1723\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1724\u001b[0m     )\n\u001b[0;32m-> 1725\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/feedback_env/lib/python3.11/site-packages/openai/_base_client.py:1428\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1419\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m   1420\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1421\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1426\u001b[0m     remaining_retries: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1427\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _AsyncStreamT:\n\u001b[0;32m-> 1428\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1429\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1430\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1431\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1432\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1433\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[1;32m   1434\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/feedback_env/lib/python3.11/site-packages/openai/_base_client.py:1504\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1503\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maclose()\n\u001b[0;32m-> 1504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m   1505\u001b[0m         options,\n\u001b[1;32m   1506\u001b[0m         cast_to,\n\u001b[1;32m   1507\u001b[0m         retries,\n\u001b[1;32m   1508\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m   1509\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1510\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1511\u001b[0m     )\n\u001b[1;32m   1513\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/feedback_env/lib/python3.11/site-packages/openai/_base_client.py:1550\u001b[0m, in \u001b[0;36mAsyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1546\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying request to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m, options\u001b[38;5;241m.\u001b[39murl, timeout)\n\u001b[1;32m   1548\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m anyio\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1550\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1551\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1552\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1553\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[1;32m   1554\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1555\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1556\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/feedback_env/lib/python3.11/site-packages/openai/_base_client.py:1504\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1503\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maclose()\n\u001b[0;32m-> 1504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m   1505\u001b[0m         options,\n\u001b[1;32m   1506\u001b[0m         cast_to,\n\u001b[1;32m   1507\u001b[0m         retries,\n\u001b[1;32m   1508\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m   1509\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1510\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1511\u001b[0m     )\n\u001b[1;32m   1513\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/feedback_env/lib/python3.11/site-packages/openai/_base_client.py:1550\u001b[0m, in \u001b[0;36mAsyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1546\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying request to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m, options\u001b[38;5;241m.\u001b[39murl, timeout)\n\u001b[1;32m   1548\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m anyio\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1550\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1551\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1552\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1553\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[1;32m   1554\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1555\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1556\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/feedback_env/lib/python3.11/site-packages/openai/_base_client.py:1519\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maread()\n\u001b[1;32m   1518\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1521\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1522\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1523\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1526\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1527\u001b[0m )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'code': '429', 'message': 'Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2023-07-01-preview have exceeded token rate limit of your current OpenAI S0 pricing tier. Please retry after 40 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.'}}"
     ]
    }
   ],
   "source": [
    "#feedback =\"I ordered a pair of shoes on your site. The site was easy to use but I had a hard time finding my size. The delivery was super fast, but the shoes were too small. I contacted the customer service to return them and they told me I had to pay the return shipping. So I decided to keep them and give them to my sister. They are good but a little too tight for me.\"\n",
    "from time import sleep\n",
    "\n",
    "batch_size = 10\n",
    "aspects = []\n",
    "for batch_df in tqdm(batchify(subcategories_df[:20], batch_size)):\n",
    "\n",
    "    subcategories = \"\\n\".join([f\"{i} : '\"+row[\"Category\"]+\" : \"+row[\"Name\"]+\"'\" for i, row in batch_df.iterrows()])\n",
    "    feedbacks = list(feedbacks_df[\"Content\"])\n",
    "\n",
    "    prompts = [prompt_aspects.invoke({\"feedback\": feedback, \"subcategories\": subcategories}).text for feedback in feedbacks]\n",
    "\n",
    "    aspects += apply_async_analysis(prompts, ListAspects)\n",
    "    sleep(60)\n",
    "aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8053f14e9ce74bfaba417f962a55b741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bs/0f5dcrc501sf9wtpqltg62940000gn/T/ipykernel_97296/1146200902.py:4: RuntimeWarning: coroutine 'AsyncCompletions.create' was never awaited\n",
      "  response = tag_single_request(prompt.text, sous_categories=sous_categories)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "responses = []\n",
    "for i, feedback in tqdm(feedbacks_df.iterrows()):\n",
    "    prompt = prompt_aspects.invoke({\"feedback\": feedback['Content']})\n",
    "    response = tag_single_request(prompt.text, sous_categories=sous_categories)\n",
    "    responses.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17fc94ea96834300a62c012f69bdc26a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'coroutine' object has no attribute 'list_aspects'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 13\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, feedback \u001b[38;5;129;01min\u001b[39;00m tqdm(feedbacks_df\u001b[38;5;241m.\u001b[39miterrows()):\n\u001b[1;32m      2\u001b[0m     results \u001b[38;5;241m=\u001b[39m bubble_client\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAspect\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m         [{\n\u001b[1;32m      5\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompany\u001b[39m\u001b[38;5;124m\"\u001b[39m: COMPANY_ID,\n\u001b[1;32m      6\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProject\u001b[39m\u001b[38;5;124m\"\u001b[39m: PROJECT_ID,\n\u001b[1;32m      7\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCategory\u001b[39m\u001b[38;5;124m\"\u001b[39m: subcategories_df\u001b[38;5;241m.\u001b[39mloc[aspect\u001b[38;5;241m.\u001b[39msous_categorie\u001b[38;5;241m.\u001b[39mindice, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCategory_id\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      8\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConsequence\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExplanation\u001b[39m\u001b[38;5;124m\"\u001b[39m: aspect\u001b[38;5;241m.\u001b[39mexplication,\n\u001b[1;32m     10\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRating\u001b[39m\u001b[38;5;124m\"\u001b[39m: aspect\u001b[38;5;241m.\u001b[39mnote_satisfaction,\n\u001b[1;32m     11\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSub_category\u001b[39m\u001b[38;5;124m\"\u001b[39m: subcategories_df\u001b[38;5;241m.\u001b[39mloc[aspect\u001b[38;5;241m.\u001b[39msous_categorie\u001b[38;5;241m.\u001b[39mindice, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_id\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     12\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAssociated_feedback\u001b[39m\u001b[38;5;124m\"\u001b[39m: feedback[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_id\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m---> 13\u001b[0m             }  \u001b[38;5;28;01mfor\u001b[39;00m aspect \u001b[38;5;129;01min\u001b[39;00m responses[i]\u001b[38;5;241m.\u001b[39mlist_aspects]\n\u001b[1;32m     14\u001b[0m         )\n\u001b[1;32m     16\u001b[0m     bubble_client\u001b[38;5;241m.\u001b[39mupdate_object(bubble_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeedback\u001b[39m\u001b[38;5;124m\"\u001b[39m, bubble_id\u001b[38;5;241m=\u001b[39mfeedback[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_id\u001b[39m\u001b[38;5;124m\"\u001b[39m], fields\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAspects\u001b[39m\u001b[38;5;124m\"\u001b[39m: [res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results]})\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'coroutine' object has no attribute 'list_aspects'"
     ]
    }
   ],
   "source": [
    "for i, feedback in tqdm(feedbacks_df.iterrows()):\n",
    "    results = bubble_client.create(\n",
    "        \"Aspect\",\n",
    "        [{\n",
    "            \"Company\": COMPANY_ID,\n",
    "            \"Project\": PROJECT_ID,\n",
    "            \"Category\": subcategories_df.loc[aspect.sous_categorie.indice, \"Category_id\"],\n",
    "            \"Consequence\": \"\",\n",
    "            \"Explanation\": aspect.explication,\n",
    "            \"Rating\": aspect.note_satisfaction,\n",
    "            \"Sub_category\": subcategories_df.loc[aspect.sous_categorie.indice, \"_id\"],\n",
    "            \"Associated_feedback\": feedback[\"_id\"],\n",
    "            }  for aspect in responses[i].list_aspects]\n",
    "        )\n",
    "\n",
    "    bubble_client.update_object(bubble_type=\"Feedback\", bubble_id=feedback[\"_id\"], fields={\"Aspects\": [res['id'] for res in results]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(prompts_path+'prompt_insights_creation.txt') as f:\n",
    "    prompt_insights = PromptTemplate.from_template(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = []\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "for batch_df in batchify(feedbacks_df, size=BATCH_SIZE):\n",
    "    context = deepcopy(feedback_context)\n",
    "    context[\"feedbacks\"] = '\\n\\n'.join([str(i)+\" : \"+x for i, x in zip(batch_df.index, batch_df[\"Content\"])])  \n",
    "    #\"- \"+\"\\n- \".join(batch_df['content'])\n",
    "    #context[\"insights\"] = \"- \"+\"\\n- \".join(batch_df['content'])\n",
    "    prompts.append(prompt_insights.invoke(context))\n",
    "\n",
    "print(len(prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompts[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = apply_analysis(prompts, InsightsList, bar=True)\n",
    "list_batch_insights_df = [pd.DataFrame(enum_to_str(response.insights_list)) for response in responses]\n",
    "\n",
    "print(len(list_batch_insights_df), \"batch have been processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses[0].insights_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(df) for df in list_batch_insights_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_batch_insights_df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(list_batch_insights_df[0]['contenu'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accociate newly created insights to feedbacks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(prompts_path+'prompt_feedbacks.txt') as f:\n",
    "    prompt_feedbacks = PromptTemplate.from_template(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentiment(str, enum.Enum):\n",
    "    POSITIF = \"Positif\"\n",
    "    NEUTRE = \"Neutre\"\n",
    "    NEGATIF = \"Négatif\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "InsightsIndex = enum.Enum(\"Indice de l'insight associé\", [(str(i), i) for i in range(BATCH_SIZE)])\n",
    "\n",
    "class Feedback(BaseModel):\n",
    "        insights_list: List[InsightsIndex] = Field(description=\"Indices des insights associés à ce retour\")\n",
    "        sentiment: Sentiment = Field(description=\"Sentiment exprimé\")\n",
    "\n",
    "class FeedbackInfosList(BaseModel):\n",
    "        feedbacks_list: List[Feedback] = Field(description=\"Liste des informations associées aux feedbacks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = []\n",
    "for batch_insights_df, batch_feedbacks_df in zip(list_batch_insights_df, batchify(feedbacks_df, size=BATCH_SIZE)):\n",
    "    #InsightsEnum = enum.Enum(\"Insight associé\", [(convert_text_to_constants(x), i) for i, x in zip(batch_insights_df.index, batch_insights_df[\"content\"])])\n",
    "\n",
    "    context = deepcopy(feedback_context)\n",
    "    #context[\"feedbacks\"] = \"- \"+\"\\n- \".join(batch_feedbacks_df['content'])\n",
    "    context[\"feedbacks\"] = '\\n'.join([str(i)+\" : \"+x for i, x in zip(batch_insights_df.index, batch_feedbacks_df[\"content\"])])  \n",
    "    context[\"insights\"] = '\\n'.join([str(i)+\" : \"+x for i, x in zip(batch_insights_df.index, batch_insights_df[\"contenu\"])])\n",
    "    prompts.append(prompt_feedbacks.invoke(context))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompts[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "responses = apply_async_analysis(prompts, FeedbackInfosList)\n",
    "\n",
    "list_enriched_feedbacks_df = [pd.DataFrame(enum_to_str(response.feedbacks_list)) for response in responses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(df) for df in list_enriched_feedbacks_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pd.concat(list_enriched_feedbacks_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_insights_df, batch_index_feedbacks, enriched_feedbacks_df in zip(list_batch_insights_df, batchify(feedbacks_df.index, size=BATCH_SIZE), list_enriched_feedbacks_df):\n",
    "    feedbacks_df.loc[batch_index_feedbacks, 'sentiment'] = enriched_feedbacks_df['sentiment']\n",
    "    feedbacks_df.loc[batch_index_feedbacks, 'insights_index'] = enriched_feedbacks_df['insights_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_insights_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_batch_feedbacks_df = [pd.DataFrame(enum_to_str(response.feedbacks_list)) for response in responses]\n",
    "list_batch_feedbacks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedbacks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_batch_feedbacks_df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_batch_insights_df[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in batchify(feedbacks_df, size=BATCH_SIZE)][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(df) for df in list_batch_feedbacks_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "l = [response.feedbacks_list for response in responses]\n",
    "l = list(itertools.chain.from_iterable(l))\n",
    "feedbacks_infos_df = pd.DataFrame(enum_to_str(l))\n",
    "feedbacks_infos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedbacks_infos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedbacks_infos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedbacks_df['sentiment'] = feedbacks_infos_df['sentiment']\n",
    "feedbacks_df['insights_list'] = feedbacks_infos_df['insights_list']\n",
    "feedbacks_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedbacks attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insights_enum = enum.Enum(\"Insight associé\", [(convert_text_to_constants(x), i) for i, x in zip(batch_insights_df.index, batch_insights_df[\"content\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(prompts_path+'prompt_feedbacks.txt') as f:\n",
    "    prompt_feedbacks = PromptTemplate.from_template(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rmFsbtT722Aq"
   },
   "outputs": [],
   "source": [
    "feedback_parser = PydanticOutputParser(pydantic_object=Feedback)\n",
    "\n",
    "prompt_feedback = PromptTemplate.from_template(\n",
    "    template= prompt_template_feedback,\n",
    "    partial_variables= {\"format_instructions\": feedback_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "prompts = []\n",
    "for feedback in feedbacks_df[feedbacks_column]:\n",
    "    context = deepcopy(feedback_context)\n",
    "    context[\"feedback\"] = feedback\n",
    "    prompts.append(prompt_feedback.invoke(context))\n",
    "\n",
    "#print(prompts[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jFraYQTLOmz-"
   },
   "outputs": [],
   "source": [
    "parsed_responses = safe_async_analysis(prompts, feedback_parser)\n",
    "\n",
    "feedbacks_df[\"sentiment\"] = [rep.sentiment for rep in parsed_responses]\n",
    "feedbacks_df[\"insights\"] = [[] for rep in parsed_responses]\n",
    "\n",
    "k=0\n",
    "insights = []\n",
    "for i, rep in enumerate(parsed_responses):\n",
    "    for j, insight in enumerate(rep.insights_list):\n",
    "        insights.append(insight)\n",
    "        feedbacks_df[\"insights\"].iloc[i].append(str(k))\n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedbacks_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hKTksTKah7At"
   },
   "outputs": [],
   "source": [
    "insights_df = pd.DataFrame({\n",
    "    \"content\":insights,\n",
    "    \"feedback_count\": 1,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedbacks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insights_df[\"related_feedback\"] = [[] for _ in range(len(insights_df))]\n",
    "\n",
    "for i, row in feedbacks_df.iterrows():\n",
    "    for j in row[\"insights\"]:\n",
    "        insights_df[\"related_feedback\"].iloc[int(j)] = row['_id'] #[int(i)]\n",
    "\n",
    "insights_df[\"childrens\"] = [[] for _ in range(len(insights_df))]\n",
    "\n",
    "insights_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k88axy7kPxQE"
   },
   "source": [
    "# Insights categorisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i, filter in filters_df.iterrows():\n",
    "    prompt_tags += '\\n\\n'+filter[\"Name\"]#+' ('+filter[\"_id\"] +')'\n",
    "    tags = tags_df[tags_df[\"Filter\"] == filter[\"_id\"]]\n",
    "    for _, tag in tags.iterrows():\n",
    "        prompt_tags += '\\n'+\"- \"+tag[\"Name\"]+' ('+tag[\"_id\"] +')'\n",
    "\n",
    "print(prompt_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(prompts_path+'prompt_categorsiation.txt') as f:\n",
    "    prompt_categorsiation = PromptTemplate.from_template(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hjHnM12_KMN3"
   },
   "outputs": [],
   "source": [
    "class FirstInsight(BaseModel):\n",
    "    tags_id: List[str] = Field(description=\"Identifiants des tags de l'insight\")\n",
    "    content: str = \"\" #Field(description=\"Point intéressant a retenir du commentaire.\")\n",
    "\n",
    "    def __str__(self):\n",
    "        return '- ' + self.content + \"\\nTypes: \" + ', '.join(self.insight_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorsiation_parser = PydanticOutputParser(pydantic_object=FirstInsight)\n",
    "\n",
    "prompt_categorsiation = PromptTemplate.from_template(\n",
    "    template= prompt_template_categorsiation,\n",
    "    partial_variables= {\"format_instructions\": categorsiation_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "prompts = []\n",
    "for insight in insights_df[\"content\"]:\n",
    "    context = deepcopy(feedback_context)\n",
    "    context[\"insight\"] = insight\n",
    "    prompts.append(prompt_categorsiation.invoke(context))\n",
    "\n",
    "#print(prompts[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_responses = safe_async_analysis(prompts, categorsiation_parser)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "insights_df[\"tag\"] = [rep.tags_id for rep in parsed_responses]\n",
    "#insights_df[\"Insights\"] = [[] for rep in parsed_responses]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types affectation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_types = \"\"\n",
    "\n",
    "for _, tag in types_df.iterrows():\n",
    "    prompt_types += '\\n'+\"- \"+tag[\"Title\"]+' ('+tag[\"_id\"] +') : ' + tag[\"Definition\"]\n",
    "\n",
    "print(prompt_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorsiation_parser = PydanticOutputParser(pydantic_object=FirstInsight)\n",
    "\n",
    "prompt_categorsiation = PromptTemplate.from_template(\n",
    "    template= prompt_template_types,\n",
    "    partial_variables= {\"format_instructions\": categorsiation_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "prompts = []\n",
    "for insight in insights_df[\"content\"]:\n",
    "    context = deepcopy(feedback_context)\n",
    "    context[\"insight\"] = insight\n",
    "    prompts.append(prompt_categorsiation.invoke(context))\n",
    "\n",
    "#print(prompts[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_responses = safe_async_analysis(prompts, categorsiation_parser)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insights_df[\"type\"] = [rep.insight_type for rep in parsed_responses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedbacks_df.to_csv(project_path+'/feedbacks.csv', index_label='Index')\n",
    "insights_df.to_csv(project_path+'/insights.csv', index_label='Index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insight_layers = [\n",
    "    pd.DataFrame(bubble_client.get_objects(\n",
    "        \"python_insight\",\n",
    "        [\n",
    "            BubbleField(\"step\") == i+1,\n",
    "            BubbleField(\"company\") == company_id,\n",
    "            ],\n",
    "    )) for i in range(n_layers)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insight_layers[0].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = insight_layers[0][\"content\"]\n",
    "sentence_embeddings = embedding_model.encode(sentences)\n",
    "sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insight_layers[0]['parent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insight_layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_int(i):\n",
    "    try:\n",
    "        return int(i)\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "for layer in insight_layers:\n",
    "    layer['parent'] = layer['parent'].apply(to_int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(insight_layers[1][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, layer in enumerate(insight_layers):\n",
    "    print(list(insight_layers[0][insight_layers[0]['parent'] == 'None'][\"content\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(insight_layers[0]['parent']<0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insight_layers[1].iloc[insight_layers[0]['parent'], \"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insight_layers[0].loc[0, \"cluster\"] == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_to_parent(0, insight_layers[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insight_layers[1].loc[0, 'parent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@Insight Plot the archive {display-mode: \"form\"}\n",
    "\n",
    "# UMAP reduces the dimensions from 1024 to 2 dimensions that we can plot\n",
    "reducer = umap.UMAP(n_neighbors=15)\n",
    "umap_embeds = reducer.fit_transform(sentence_embeddings)\n",
    "\n",
    "def map_to_parent(i, parents_df):\n",
    "    try:\n",
    "        return parents_df.loc[i, 'content']\n",
    "    except:\n",
    "        return \"\"\n",
    "    \n",
    "# Prepare the data to plot and interactive visualization\n",
    "# using Altair\n",
    "df_explore = pd.DataFrame(data={\n",
    "    'content': insight_layers[0]['content'], \n",
    "    'parent': insight_layers[0]['parent'].apply(lambda x: map_to_parent(x, insight_layers[1])),\n",
    "    'cluster': insight_layers[0]['cluster'].astype(str),\n",
    "    })\n",
    "df_explore['x'] = umap_embeds[:,0]\n",
    "df_explore['y'] = umap_embeds[:,1]\n",
    "df_explore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot\n",
    "chart = alt.Chart(df_explore).mark_circle(size=60).encode(\n",
    "    x=#'x',\n",
    "    alt.X('x',\n",
    "        scale=alt.Scale(zero=False)\n",
    "    ),\n",
    "    y=\n",
    "    alt.Y('y',\n",
    "        scale=alt.Scale(zero=False)\n",
    "    ),\n",
    "    color='cluster',\n",
    "    tooltip=['content', \"parent\"]\n",
    ").properties(\n",
    "    width=700,\n",
    "    height=400\n",
    ")\n",
    "chart.interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_idf(documents)\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectors = vectorizer.fit_transform(documents)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    dense = vectors.todense()\n",
    "    denselist = dense.tolist()\n",
    "    df = pd.DataFrame(denselist, columns=feature_names)\n",
    "    df = df[df.columns.difference(stopwords.words('french'))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = td_idf(feedbacks_df['content'])\n",
    "#print('\\n'.join(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('\\n'.join(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_top_two_columns(row):\n",
    "    top_two_indexes = row.nlargest(5).index.tolist()\n",
    "    return top_two_indexes\n",
    "\n",
    "top_two_columns_df = df.apply(get_top_two_columns, axis=1)\n",
    "\n",
    "print(top_two_columns_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('\\n'.join(insights_df['content']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "cell_execution_strategy": "setup",
   "collapsed_sections": [
    "pKSREXZg85yy",
    "iiOlJz-d9AsB"
   ],
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
