{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xjPRVXra8lVZ"
   },
   "source": [
    "# Dependancies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKSREXZg85yy"
   },
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zMSukVZh29HK"
   },
   "outputs": [],
   "source": [
    "#!pip install sentence_transformers langchain openai tqdm datasets asyncio scikit-learn cohere tiktoken umap altair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Md3DNHlV22Ai"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_dataset\n",
    "import umap\n",
    "import altair as alt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List, Optional\n",
    "import enum\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.output_parsers.regex_dict import RegexDictParser\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, ChatMessage\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from pydantic import BaseModel, Field, validator, create_model\n",
    "from openai import AsyncOpenAI, OpenAI\n",
    "#import asyncio\n",
    "import os\n",
    "from typing import Tuple, List, Optional\n",
    "\n",
    "import requests\n",
    "\n",
    "from pydantic import BaseModel, ValidationInfo, model_validator\n",
    "\n",
    "import json\n",
    "\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import umap.umap_ as umap\n",
    "#import umap\n",
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.bubble import *\n",
    "from src.models import *\n",
    "from src.utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_path = \"Prompts/fr/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqsmhIToV_yX"
   },
   "source": [
    "## Bubble API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedbacks_df = get(\"Feedback\", max_objects=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_df = get(\"Category\")\n",
    "original_subcategories_df = get(\"SubCategory\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_CATEG_NONE = categories_df[categories_df[\"Name\"].isna()].iloc[0][\"_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_infos = bubble_client.get(\n",
    "    \"Company\",\n",
    "    bubble_id=COMPANY_ID,\n",
    ")\n",
    "project_infos = bubble_client.get(\n",
    "    \"Project\",\n",
    "    bubble_id=PROJECT_ID,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CategoryInsight = enum.Enum(\"Categories de l'insight\", [(convert_text_to_constants(x), x) for x in list(categories_df[categories_df[\"Name\"].notna()].Name)])\n",
    "\n",
    "dict_SubCategoriesInsight = {}\n",
    "for _,row in categories_df[categories_df[\"Name\"].notna()].iterrows():\n",
    "    concerned_subcat_df = original_subcategories_df[original_subcategories_df[\"Category\"] == row[\"_id\"]]\n",
    "    concerned_subcat_df = concerned_subcat_df[concerned_subcat_df[\"Name\"].notna()]\n",
    "    row[\"Name\"] = enum.Enum(\"Categories de l'insight\", [(convert_text_to_constants(x), x) for x in concerned_subcat_df.Name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df  = categories_df.copy()\n",
    "df['Category'] = df['_id'].astype(str)\n",
    "original_subcategories_df['Category'] = original_subcategories_df['Category'].astype(str)\n",
    "df = pd.merge(original_subcategories_df, df, on=[\"Category\", \"Company\"])\n",
    "df = df[[\"Name_x\", \"Name_y\", \"Company\", \"_id_x\", \"_id_y\"]]\n",
    "df.columns = [\"Name\", \"Category\",  \"Company\", \"_id\", \"Category_id\"]\n",
    "subcategories_df = df\n",
    "subcategories_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types_descr = columns_to_string(subcategories_df[subcategories_df[\"Name\"].notna()], \"Category\", \"Name\")\n",
    "print(types_descr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example_insight = \"Manque de clarté de l'affichage des prix en magasin\"\n",
    "#exemple_commentaire = \"je suis exclusif metro je n ai aucun representant j achetais jusqu a present tout metro par facilite mais je suis tres souvent décue par la reponse ha non on n en a pas cela arrive demain je pense que depuis le covid tout le monde ou presque s en fou!!!\"\n",
    "#examples_insights_df = pd.DataFrame([\n",
    "#    {\"Insights qui devraient en découler\": \"Déceptions face aux retards de livraison\"},\n",
    "#    {\"Insights qui devraient en découler\": \"Impression d'une baisse de qualité du service depuis le Covid\"},\n",
    "#])\n",
    "\n",
    "\n",
    "feedback_context = {\n",
    "    \"entreprise\": company_infos[\"Name\"],\n",
    "    \"context\": company_infos['Context'],\n",
    "    \"role\": company_infos['Role'],\n",
    "    \"cible\": project_infos['Target'],\n",
    "    \"insight_types\": types_descr,\n",
    "    #\"insight_categories\": tags_descr,\n",
    "    #\"question\": project_infos['Study_question'],\n",
    "    #\"exemple_commentaire\": exemple_commentaire,\n",
    "    #\"example_insights\": '\\n- '.join(list(examples_insights_df['Insights qui devraient en découler'])),\n",
    "}\n",
    "\n",
    "feedback_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Insights extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aspects and Insights creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FeedbackIndex = enum.Enum(\"Indice du retour associé\", [(str(i), i) for i in range(BATCH_SIZE)])\n",
    "\n",
    "class Categorie(BaseModel):\n",
    "    nb_parents: int = Field(description=\"Nombre de  parents dans l'arbdre des catégories.\")\n",
    "    indice: str = Field(description=\"Indice de la catégorie. Doit être un string.\")\n",
    "    nom: str = Field(description=\"Nom de cette catégorie.\")\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.nom + ' ('+str(self.indice)+')'\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def validate_ids(self, info: ValidationInfo):\n",
    "        context = info.context\n",
    "        if context:\n",
    "            tags = context.get(\"sous_categories\")\n",
    "            assert self.indice in {\n",
    "                tag.indice for tag in tags\n",
    "            }, f\"sous_categories ID {self.indice} not found in context\"\n",
    "            assert self.nom in {\n",
    "                tag.nom for tag in tags\n",
    "            }, f\"sous_categories name {self.nom} not found in context\"\n",
    "        return self\n",
    "    \n",
    "\n",
    "\n",
    "class Aspect(BaseModel):\n",
    "    categorie : Categorie = Field(description=\"Sous-catégorie concernée.\")\n",
    "    note_satisfaction : int = Field(description=\"Note de satisfaction du client concernant cette sous-catégorie, de 1 (pas content) à 5 (très content).\")\n",
    "    explication: Optional[str] = Field(description=\"Eventuel insight qui permetterait d'améliorer l'experience client, les produits ou la stratégie de l'entreprise. Ne doit être ajouté que ni réellement intéressant, et doit alors être aussi claire et concise que possible.\") #Field(description=\"Point intéressant a retenir du commentaire.\")\n",
    "\n",
    "    def __str__(self):\n",
    "        res = '\\n' + str(self.sous_categorie) + '\\nSatisfaction: ' + str(self.note_satisfaction) \n",
    "        if self.explication is not None:\n",
    "            res += \"\\nExplication: \" + self.explication\n",
    "        return MAX_RETRIES\n",
    "    \n",
    "    @model_validator(mode=\"after\")\n",
    "    def validate_ids(self, info: ValidationInfo):\n",
    "        assert (0 <= self.note_satisfaction) and (self.note_satisfaction <= 5)\n",
    "        return self\n",
    "\n",
    "class ListAspects(BaseModel):\n",
    "    list_aspects:  Optional[List[Aspect]] = Field(description=\"Eventielle liste des différents aspects évoqués dans le feedback.\")\n",
    "\n",
    "    def __str__(self):\n",
    "        if self.list_aspects is not None:\n",
    "            return '\\n'.join([str(x) for x in self.list_aspects])\n",
    "        else:\n",
    "            return ''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(prompts_path+'prompt_aspects.txt') as f:\n",
    "    prompt_aspects = PromptTemplate.from_template(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feedback =\"I ordered a pair of shoes on your site. The site was easy to use but I had a hard time finding my size. The delivery was super fast, but the shoes were too small. I contacted the customer service to return them and they told me I had to pay the return shipping. So I decided to keep them and give them to my sister. They are good but a little too tight for me.\"\n",
    "from time import sleep\n",
    "\n",
    "batch_size = 100\n",
    "aspects = []\n",
    "for batch_df in tqdm(batchify(feedbacks_df, batch_size)):\n",
    "\n",
    "    subcategories = \"\\n\".join([f\"{i} : '\"+row[\"Category\"]+\" : \"+row[\"Name\"]+\"'\" for i, row in subcategories_df[subcategories_df[\"Name\"].notna()].iterrows()])\n",
    "    batch_feedbacks = list(batch_df[\"Content\"])\n",
    "\n",
    "    prompts = [prompt_aspects.invoke({\"feedback\": feedback, \"subcategories\": subcategories}).text for feedback in batch_feedbacks]\n",
    "\n",
    "    aspects += apply_async_analysis(prompts, ListAspects)\n",
    "    sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([aspect.list_aspects is None for aspect in aspects])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, feedback in tqdm(feedbacks_df.iterrows()):\n",
    "    if (aspects[i].list_aspects is not None) and (aspects[i].list_aspects != []):\n",
    "        results = bubble_client.create(\n",
    "            \"Aspect\",\n",
    "            [{\n",
    "                \"Company\": COMPANY_ID,\n",
    "                \"Project\": PROJECT_ID,\n",
    "                \"Category\": subcategories_df.loc[int(aspect.categorie.indice), \"Category_id\"],\n",
    "                \"Consequence\": \"\",\n",
    "                \"Date\": str(feedback[\"Date\"]),\n",
    "                \"Explanation\": aspect.explication,\n",
    "                \"Rating\": aspect.note_satisfaction,\n",
    "                \"SubCategory\": subcategories_df.loc[int(aspect.categorie.indice), \"_id\"],\n",
    "                \"Associated_feedback\": feedback[\"_id\"],\n",
    "                }  for aspect in aspects[i].list_aspects]\n",
    "            )\n",
    "\n",
    "    bubble_client.update_object(bubble_type=\"Feedback\", bubble_id=feedback[\"_id\"], fields={\"Aspects\": [res['id'] for res in results]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepared for visu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspects_df = get(\"Aspect\")\n",
    "ID_CATEG_NONE = categories_df[categories_df[\"Name\"].isna()].iloc[0][\"_id\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def global_stats_and_rating_counts(df, interval=\"M\"):\n",
    "    \"\"\"\n",
    "    Calculates various statistics (mean, max, count) and rating counts for each groupby variable\n",
    "    and interval in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        interval (str, optional): The time interval for grouping. Defaults to \"M\".\n",
    "        groupby (str, optional): The column to group by. Defaults to \"Category\".\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The output DataFrame with additional columns for statistics and rating counts.\n",
    "    \"\"\"\n",
    "\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])  # Ensure correct datetime format\n",
    "    groups = df.groupby(df[\"Date\"].dt.to_period(interval))\n",
    "\n",
    "    # Calculate mean, max, and total count using agg\n",
    "    statistics = groups.agg(\n",
    "        mean_rating=(\"Rating\", \"mean\"),\n",
    "        max_rating=(\"Rating\", \"max\"),\n",
    "        min_rating=(\"Rating\", \"min\"),\n",
    "        median_rating=(\"Rating\", \"median\"),\n",
    "        q1_rating = (\"Rating\", lambda x: np.quantile(x, 0.25)),\n",
    "        q3_rating = (\"Rating\", lambda x: np.quantile(x, 0.75)),\n",
    "        count=(\"Rating\", \"count\")\n",
    "    ).reset_index()\n",
    "\n",
    "    # Calculate rating counts using value_counts() within a loop\n",
    "    rating_counts = []\n",
    "    for name, group in groups:\n",
    "        rating_counts.append(group[\"Rating\"].value_counts().rename(name))\n",
    "\n",
    "    # Combine rating counts into a single DataFrame\n",
    "    rating_counts_df = pd.concat(rating_counts, axis=1).fillna(0)\n",
    "\n",
    "    merged_df = pd.concat([statistics.set_index(\"Date\"), rating_counts_df.T], axis=1)\n",
    "    # Merge statistics and rating counts\n",
    "    display_format = \"%m/%d/%Y\"\n",
    "\n",
    "    merged_df = merged_df.rename_axis('Date').reset_index()\n",
    "\n",
    "    merged_df[\"Date\"] = merged_df[\"Date\"].apply(lambda x:x.strftime(display_format))\n",
    "    merged_df['Date'] = pd.to_datetime(merged_df[\"Date\"])  # Ensure correct datetime format\n",
    "\n",
    "    if interval == 'W':\n",
    "        merged_df['Date'] = merged_df['Date'] - pd.to_timedelta(merged_df['Date'].dt.day_of_week, unit='d')\n",
    "    elif interval == 'M':\n",
    "        merged_df['Date'] = merged_df['Date'] - pd.to_timedelta(merged_df['Date'].dt.day-1, unit='d')\n",
    "    elif interval == 'Y':\n",
    "        merged_df['Date'] = merged_df['Date'] - pd.to_timedelta(merged_df['Date'].dt.day_of_year-1, unit='d')\n",
    "\n",
    "    merged_df[\"Date\"] = merged_df[\"Date\"].apply(lambda x:x.strftime(display_format))\n",
    "    #merged_df[\"Date\"] = merged_df[\"Date\"].apply(lambda x:x.to_timestamp(interval))\n",
    "    merged_df[\"Period\"] = interval\n",
    "    merged_df[\"Category\"] = ID_CATEG_NONE\n",
    "    merged_df[\"SubCategory\"] = None\n",
    "    merged_df[\"Grouped by\"] = None\n",
    "    return merged_df\n",
    "\n",
    "# Example usage\n",
    "df = aspects_df  # Load your DataFrame\n",
    "#statistics, rating_counts_df = group_stats_and_rating_counts(df)\n",
    "statistics_and_counts = global_stats_and_rating_counts(df, interval=\"M\")\n",
    "statistics_and_counts #.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def group_stats_and_rating_counts(df, interval=\"M\", groupby=\"Category\"):\n",
    "    \"\"\"\n",
    "    Calculates various statistics (mean, max, count) and rating counts for each groupby variable\n",
    "    and interval in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        interval (str, optional): The time interval for grouping. Defaults to \"M\".\n",
    "        groupby (str, optional): The column to group by. Defaults to \"Category\".\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The output DataFrame with additional columns for statistics and rating counts.\n",
    "    \"\"\"\n",
    "    if groupby is None:\n",
    "        return global_stats_and_rating_counts(df, interval=interval)\n",
    "\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])  # Ensure correct datetime format\n",
    "    added_columns = [groupby] if groupby is not None else []\n",
    "    added_columns += [\"Category\"] if groupby==\"SubCategory\" else []\n",
    "    groups = df.groupby([df[\"Date\"].dt.to_period(interval)]+added_columns)\n",
    "\n",
    "    # Calculate mean, max, and total count using agg\n",
    "    statistics = groups.agg(\n",
    "        mean_rating=(\"Rating\", \"mean\"),\n",
    "        max_rating=(\"Rating\", \"max\"),\n",
    "        min_rating=(\"Rating\", \"min\"),\n",
    "        median_rating=(\"Rating\", \"median\"),\n",
    "        q1_rating = (\"Rating\", lambda x: np.quantile(x, 0.25)),\n",
    "        q3_rating = (\"Rating\", lambda x: np.quantile(x, 0.75)),\n",
    "        count=(\"Rating\", \"count\")\n",
    "    ).reset_index()\n",
    "\n",
    "    # Calculate rating counts using value_counts() within a loop\n",
    "    rating_counts = []\n",
    "    for name, group in groups:\n",
    "        rating_counts.append(group[\"Rating\"].value_counts().rename(name))\n",
    "\n",
    "    # Combine rating counts into a single DataFrame\n",
    "    rating_counts_df = pd.concat(rating_counts, axis=1).fillna(0)\n",
    "\n",
    "    multi_index = [\"Date\"] + added_columns\n",
    "    merged_df = pd.concat([statistics.set_index(multi_index), rating_counts_df.T], axis=1)\n",
    "    # Merge statistics and rating counts\n",
    "    #merged_df = pd.merge(statistics, rating_counts_df, on=[name[0], groupby])\n",
    "\n",
    "    #display_format = \"%Y\"\n",
    "    #display_format += \"-%m\" if interval != \"Y\" else \"\"\n",
    "    #display_format += \"-%d\" if interval != \"M\" else \"\"\n",
    "    display_format = \"%m/%d/%Y\"\n",
    "\n",
    "    print(multi_index)\n",
    "    merged_df = merged_df.reset_index(names=multi_index)\n",
    "\n",
    "    merged_df[\"Date\"] = merged_df[\"Date\"].apply(lambda x:x.strftime(display_format))\n",
    "    merged_df['Date'] = pd.to_datetime(merged_df[\"Date\"])  # Ensure correct datetime format\n",
    "\n",
    "    if interval == 'W':\n",
    "        merged_df['Date'] = merged_df['Date'] - pd.to_timedelta(merged_df['Date'].dt.day_of_week, unit='d')\n",
    "    elif interval == 'M':\n",
    "        merged_df['Date'] = merged_df['Date'] - pd.to_timedelta(merged_df['Date'].dt.day-1, unit='d')\n",
    "    elif interval == 'Y':\n",
    "        merged_df['Date'] = merged_df['Date'] - pd.to_timedelta(merged_df['Date'].dt.day_of_year-1, unit='d')\n",
    "\n",
    "    merged_df[\"Date\"] = merged_df[\"Date\"].apply(lambda x:x.strftime(display_format))\n",
    "    #merged_df[\"Date\"] = merged_df[\"Date\"].apply(lambda x:x.to_timestamp(interval))\n",
    "    merged_df[\"Period\"] = interval\n",
    "    merged_df[\"Grouped by\"] = groupby\n",
    "    if \"SubCategory\" not in merged_df:\n",
    "        merged_df[\"SubCategory\"] = None\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "# Example usage\n",
    "df = aspects_df  # Load your DataFrame\n",
    "#statistics, rating_counts_df = group_stats_and_rating_counts(df)\n",
    "statistics_and_counts = group_stats_and_rating_counts(df, interval=\"M\", groupby=\"Category\")\n",
    "statistics_and_counts #.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_empty_subcat(cat_id):\n",
    "    df = original_subcategories_df.copy()\n",
    "\n",
    "    df = df[df[\"Category\"]==cat_id]\n",
    "    return df[df[\"Name\"].isna()].iloc[0][\"_id\"]\n",
    "\n",
    "find_empty_subcat(\"1709253065849x444427432726514300\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_aspects(df, batch_size=1000):\n",
    "    d = {\n",
    "        \"Company\": COMPANY_ID,\n",
    "        \"Project\": PROJECT_ID,\n",
    "        \"Grouped by\": row[\"Grouped by\"],\n",
    "        \"Category\": row['Category'],\n",
    "        \"SubCategory\": row['SubCategory'],\n",
    "        \"Date\": row[\"Date\"],\n",
    "        \"Period\": row[\"Period\"],\n",
    "        \"Mean Rating\": row[\"mean_rating\"],\n",
    "        \"Min Rating\": row[\"min_rating\"],\n",
    "        \"Max Rating\": row[\"max_rating\"],\n",
    "        \"Q1 Rating\": row[\"max_rating\"],\n",
    "        \"Median Rating\": row[\"max_rating\"],\n",
    "        \"Q3 Rating\": row[\"max_rating\"],\n",
    "        \"Count\": sum([row[i] for i in range(1,6)]),\n",
    "        \"Count of 1s\": row[1],\n",
    "        \"Count of 2s\": row[2],\n",
    "        \"Count of 3s\": row[3],\n",
    "        \"Count of 4s\": row[4],\n",
    "        \"Count of 5s\": row[5],\n",
    "        }  \n",
    "\n",
    "    bubble_id = bubble_client.create(\"Aspect Evol\",d)\n",
    "\n",
    "all_statistics = []\n",
    "for groupby in [None, \"Category\", \"SubCategory\"]:\n",
    "  for interval in [\"Y\", \"M\", \"W\", \"D\"]:\n",
    "    statistics = group_stats_and_rating_counts(aspects_df, interval=interval, groupby=groupby)\n",
    "\n",
    "    #if \"Category\" not in statistics:\n",
    "    #   statistics[\"Category\"] = ID_SUBCATEG_NONE\n",
    "\n",
    "    #if \"SubCategory\" not in statistics:\n",
    "    #   statistics[\"SubCategory\"] = ID_SUBCATEG_NONE\n",
    "\n",
    "    assert statistics[\"Category\"].isna().sum() == 0\n",
    "    \n",
    "    #statistics[\"Category\"] = statistics[\"Category\"].fillna(ID_CATEG_NONE)\n",
    "\n",
    "    \n",
    "    #statistics[\"SubCategory\"] = statistics[\"SubCategory\"].fillna(ID_SUBCATEG_NONE)\n",
    "    #\n",
    "    # _aspects(statistics)\n",
    "    all_statistics.append(statistics)\n",
    "all_statistics_df = pd.concat(all_statistics).reset_index()\n",
    "all_statistics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = all_statistics_df.copy()\n",
    "#df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "df.set_index(\"Date\", drop=\"True\", inplace=True)\n",
    "df = df[df[\"Category\"]=='1709322143530x849396050152903400']\n",
    "\n",
    "df = df[df[\"Period\"]=='M']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df[[1, 2, 3, 4, 5]]\n",
    "df.plot(kind='bar', stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = all_statistics_df.copy()\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "df = df[df[\"Category\"]!='1709322143530x849396050152903400']\n",
    "df = df[df[\"SubCategory\"].isna()]\n",
    "\n",
    "df = df[df[\"Period\"]=='M']\n",
    "#df.set_index([\"Date\", \"Category\"], drop=\"True\", inplace=True)\n",
    "df.set_index(\"Date\", drop=\"True\", inplace=True)\n",
    "df = df[[\"count\", \"Category\"]]\n",
    "df.head().to_csv(\"brouillon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in categories_df[\"_id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reshape the data frame using pivot_table to prepare it for plotting\n",
    "df_pivoted = df.pivot_table(index='Date', columns='Category', values='count', aggfunc='sum')\n",
    "\n",
    "df_pivoted.columns = [categories_df[categories_df[\"_id\"]==c].iloc[0][\"Name\"] for c in list(df_pivoted.columns)]\n",
    "\n",
    "# Create the stacked bar plot using Matplotlib\n",
    "plt.figure(figsize=(10, 6))  # Adjust figure size as needed\n",
    "df_pivoted.plot(kind='bar', stacked=True, colormap='Set2')\n",
    "\n",
    "# Customize plot elements (optional):\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Stacked Bar Plot')\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for readability\n",
    "plt.legend(title='Category')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"Date\"].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# data from https://allisonhorst.github.io/palmerpenguins/\n",
    "\n",
    "species = (\n",
    "    \"Adelie\\n $\\\\mu=$3700.66g\",\n",
    "    \"Chinstrap\\n $\\\\mu=$3733.09g\",\n",
    "    \"Gentoo\\n $\\\\mu=5076.02g$\",\n",
    ")\n",
    "weight_counts = {\n",
    "    \"Below\": np.array([70, 31, 58]),\n",
    "    \"Above\": np.array([82, 37, 66]),\n",
    "}\n",
    "width = 0.5\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "bottom = np.zeros(3)\n",
    "\n",
    "for boolean, weight_count in weight_counts.items():\n",
    "    p = ax.bar(species, weight_count, width, label=boolean, bottom=bottom)\n",
    "    bottom += weight_count\n",
    "\n",
    "ax.set_title(\"Number of penguins with above average body mass\")\n",
    "ax.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_statistics_df[all_statistics_df[\"SubCategory\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = all_statistics_df.copy()\n",
    "df = df[df[\"SubCategory\"].isna()]\n",
    "df[\"SubCategory\"] = df[\"Category\"].apply(find_empty_subcat)\n",
    "res_df = all_statistics_df.copy()\n",
    "res_df[res_df[\"SubCategory\"].isna()] = df\n",
    "all_statistics_df = res_df\n",
    "all_statistics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "send_aspects(all_statistics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "cell_execution_strategy": "setup",
   "collapsed_sections": [
    "pKSREXZg85yy",
    "iiOlJz-d9AsB"
   ],
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
