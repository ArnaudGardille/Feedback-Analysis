{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'async for' requires an object with __aiter__ method, got coroutine",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# With async\u001b[39;00m\n\u001b[1;32m     14\u001b[0m async_response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat(model\u001b[38;5;241m=\u001b[39mmodel, messages\u001b[38;5;241m=\u001b[39mmessages)\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m async_response: \n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(chunk\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdelta\u001b[38;5;241m.\u001b[39mcontent)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'async for' requires an object with __aiter__ method, got coroutine"
     ]
    }
   ],
   "source": [
    "from mistralai.async_client import MistralAsyncClient\n",
    "from mistralai.models.chat_completion import ChatMessage\n",
    "\n",
    "api_key = MISTRAL_API_KEY\n",
    "model = \"mistral-tiny\"\n",
    "\n",
    "client = MistralAsyncClient(api_key=api_key)\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(role=\"user\", content=\"What is the best French cheese?\")\n",
    "]\n",
    "\n",
    "# With async\n",
    "async_response = client.chat(model=model, messages=messages)\n",
    "\n",
    "async for chunk in async_response: \n",
    "    print(chunk.choices[0].delta.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"first_name\": \"Alice\",\n",
      "  \"last_name\": \"Johnson\",\n",
      "  \"city\": \"Rome\"\n",
      "}\n",
      "{\n",
      "  \"first_name\": \"Emmanuel\",\n",
      "  \"last_name\": \"Macron\",\n",
      "  \"city\": \"New York\"\n",
      "}\n",
      "{\n",
      "  \"first_name\": \"Growing\",\n",
      "  \"last_name\": \"Ananas\",\n",
      "  \"city\": \"Paris\"\n",
      "}\n",
      "['{\\n  \"first_name\": \"Alice\",\\n  \"last_name\": \"Johnson\",\\n  \"city\": \"Rome\"\\n}', '{\\n  \"first_name\": \"Emmanuel\",\\n  \"last_name\": \"Macron\",\\n  \"city\": \"New York\"\\n}', '{\\n  \"first_name\": \"Growing\",\\n  \"last_name\": \"Ananas\",\\n  \"city\": \"Paris\"\\n}']\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "from mistralai.client import MistralClient\n",
    "from mistralai.async_client import MistralAsyncClient\n",
    "\n",
    "from mistralai.models.chat_completion import ChatMessage\n",
    "from pydantic import BaseModel, ValidationError, ConfigDict\n",
    "from typing import Type, Optional\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv()\n",
    "MISTRAL_API_KEY = \"GFBjsGogmbv0LuMWjJewXBXwyN7QeKNj\"\n",
    "\n",
    "\n",
    "class MistralLanguageModel:\n",
    "    def __init__(self, api_key=MISTRAL_API_KEY,\n",
    "                 model=\"mistral-tiny\", temperature=0.0):\n",
    "\n",
    "        if api_key is None:\n",
    "            raise ValueError(\"The Mistral API KEY must be provided either as \"\n",
    "                             \"an argument or as an environment variable named 'MISTRAL_API_KEY'\") # noqa\n",
    "\n",
    "        self.api_key = api_key\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "        \n",
    "        #self.client = MistralAsyncClient(api_key=self.api_key)\n",
    "        self.client = MistralClient(api_key=self.api_key)\n",
    "\n",
    "    def async_generation(self,  prompt: str, #async\n",
    "                 output_format: Optional[Type[BaseModel]] = None,\n",
    "                 max_tokens: int = None):\n",
    "        system_message = \"You are a helpful assistant.\"\n",
    "        if output_format:\n",
    "            system_message += f\" Respond in a JSON format that contains the following keys: {self._model_structure_repr(output_format)}. You must only return a JSON, nothing else. You are strictly forbidden to return anything else, like an explanation.\" # noqa\n",
    "\n",
    "        params = {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": [\n",
    "                ChatMessage(role=\"system\", content=system_message),\n",
    "                ChatMessage(role=\"user\", content=prompt)\n",
    "            ],\n",
    "            \"temperature\": self.temperature}\n",
    "\n",
    "        if max_tokens is not None:\n",
    "            params[\"max_tokens\"] = max_tokens\n",
    "\n",
    "        return self.client.chat(**params)\n",
    "    \n",
    "\n",
    "    def generate(self, prompts: list[str],\n",
    "                 output_format: Optional[Type[BaseModel]] = None,\n",
    "                 max_tokens: int = None):\n",
    "\n",
    "        \n",
    "\n",
    "        \"\"\"loop = asyncio.get_event_loop()\n",
    "        tasks = [loop.create_task(self.async_generation(prompt, output_format)) for prompt in prompts]\n",
    "\n",
    "        responses = loop.run_until_complete(asyncio.gather(*tasks))\n",
    "        responses = [self.async_generation(prompt, output_format) for prompt in prompts]\n",
    "        \"\"\"\n",
    "        responses = []\n",
    "        for prompt in prompts:\n",
    "            res = self.async_generation(prompt, output_format).choices[0].message.content\n",
    "            print(res)\n",
    "            assert self._is_valid_json_for_model(res, output_format)\n",
    "            responses.append(res)\n",
    "        #responses = [response.choices[0].message.content for response in responses]\n",
    "        return responses\n",
    "        \n",
    "        print(responses)\n",
    "        if output_format:\n",
    "            for res in responses:\n",
    "                assert self._is_valid_json_for_model(res, output_format)\n",
    "                \n",
    "        return responses\n",
    "\n",
    "    def _model_structure_repr(self, model: Type[BaseModel]) -> str:\n",
    "        fields = model.__annotations__\n",
    "        return ', '.join(f'{key}: {value}' for key, value in fields.items())\n",
    "\n",
    "    def _is_valid_json_for_model(self, text: str, model: Type[BaseModel]) -> bool: # noqa\n",
    "        \"\"\"\n",
    "        Check if a text is valid JSON and if it respects the provided BaseModel. # noqa\n",
    "        \"\"\"\n",
    "        model.model_config = ConfigDict(strict=True)\n",
    "\n",
    "        try:\n",
    "            parsed_data = json.loads(text)\n",
    "            model(**parsed_data)\n",
    "            return True\n",
    "        except (json.JSONDecodeError, ValidationError):\n",
    "            parsed_data = json.loads(text)\n",
    "            model(**parsed_data)\n",
    "            return False\n",
    "\n",
    "\n",
    "class Output(BaseModel):\n",
    "    first_name: str\n",
    "    last_name: str\n",
    "    city: str\n",
    "\n",
    "\n",
    "llm = MistralLanguageModel()\n",
    "prompts = [\n",
    "    'Extract the requested  information from the following sentence: \"Alice Johnson is visiting Rome.\"',\n",
    "    'Extract the requested  information from the following sentence: \"Emmanuel Macron is visiting New York.\"',\n",
    "    'Extract the requested  information from the following sentence: \"Growing Ananas is visiting Paris.\"'\n",
    "]\n",
    "response = llm.generate(prompts, output_format=Output)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MistralLanguageModel:\n",
    "    def __init__(self, api_key=MISTRAL_API_KEY,\n",
    "                 model=\"mistral-tiny\", temperature=0.5):\n",
    "\n",
    "        if api_key is None:\n",
    "            raise ValueError(\"The Mistral API KEY must be provided either as \"\n",
    "                             \"an argument or as an environment variable named 'MISTRAL_API_KEY'\") # noqa\n",
    "\n",
    "        self.api_key = api_key\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "        self.client = MistralClient(api_key=self.api_key)\n",
    "\n",
    "    def generate(self, prompt: str,\n",
    "                 output_format: Optional[Type[BaseModel]] = None,\n",
    "                 max_tokens: int = None):\n",
    "\n",
    "        retry_delay = 0.1\n",
    "\n",
    "        while True:\n",
    "            if True:\n",
    "            #try:\n",
    "                system_message = \"You are a helpful assistant.\"\n",
    "                if output_format:\n",
    "                    system_message += f\" Respond in a JSON format that contains the following keys: {self._model_structure_repr(output_format)}. You must only return a JSON, nothing else. You are strictly forbidden to return anything else, like an explanation.\" # noqa\n",
    "\n",
    "                messages = [\n",
    "                    ChatMessage(role=\"system\", content=system_message),\n",
    "                    ChatMessage(role=\"user\", content=prompt)\n",
    "                ]\n",
    "                params = {\n",
    "                    \"model\": self.model,\n",
    "                    \"messages\": messages,\n",
    "                    \"temperature\": self.temperature\n",
    "                }\n",
    "\n",
    "                if max_tokens is not None:\n",
    "                    params[\"max_tokens\"] = max_tokens\n",
    "\n",
    "                response = self.client.chat(**params)\n",
    "                response_content = response.choices[0].message.content\n",
    "                print(response_content)\n",
    "\n",
    "                if output_format:\n",
    "                    assert self._is_valid_json_for_model(response_content,\n",
    "                                                     output_format)\n",
    "                    return response_content\n",
    "                #else:\n",
    "                #    return response_content\n",
    "\n",
    "            #except Exception:\n",
    "            #    print(f\"Hit rate limit. Retrying in {retry_delay} seconds.\")\n",
    "            #    time.sleep(retry_delay)\n",
    "            #    retry_delay *= 2\n",
    "\n",
    "    def _model_structure_repr(self, model: Type[BaseModel]) -> str:\n",
    "        fields = model.__annotations__\n",
    "        return ', '.join(f'{key}: {value}' for key, value in fields.items())\n",
    "\n",
    "\n",
    "    def _is_valid_json_for_model(self, text: str, model: Type[BaseModel]) -> bool: # noqa\n",
    "        \"\"\"\n",
    "        Check if a text is valid JSON and if it respects the provided BaseModel. # noqa\n",
    "        \"\"\"\n",
    "        model.model_config = ConfigDict(strict=True)\n",
    "\n",
    "        try:\n",
    "            parsed_data = json.loads(text)\n",
    "            model(**parsed_data)\n",
    "            return True\n",
    "        except (json.JSONDecodeError, ValidationError):\n",
    "            return False\n",
    "llm = MistralLanguageModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"first_name\": \"Alice\",\n",
      "  \"last_name\": \"Johnson\",\n",
      "  \"city\": \"Rome\"\n",
      "}\n",
      "{\n",
      "  \"first_name\": \"Emmanuel\",\n",
      "  \"last_name\": \"Macron\",\n",
      "  \"city\": \"New York\"\n",
      "}\n",
      "{\n",
      "  \"first_name\": \"Growing\",\n",
      "  \"last_name\": \"Ananas\",\n",
      "  \"city\": \"Paris\"\n",
      "}\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sleep\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m----> 9\u001b[0m     responses\u001b[38;5;241m.\u001b[39mappend(llm\u001b[38;5;241m.\u001b[39mgenerate(prompt, output_format\u001b[38;5;241m=\u001b[39mOutput))\n\u001b[1;32m     10\u001b[0m     sleep(\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[57], line 45\u001b[0m, in \u001b[0;36mMistralLanguageModel.generate\u001b[0;34m(self, prompt, output_format, max_tokens)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(response_content)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_format:\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_valid_json_for_model(response_content,\n\u001b[1;32m     46\u001b[0m                                      output_format)\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response_content\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    'Extract the requested  information from the following sentence: \"Alice Johnson is visiting Rome.\"',\n",
    "    'Extract the requested  information from the following sentence: \"Emmanuel Macron is visiting New York.\"'\n",
    "    'Extract the requested  information from the following sentence: \"Growing Ananas is visiting Paris.\"'\n",
    "]\n",
    "responses = []\n",
    "from time import sleep\n",
    "for prompt in prompts:\n",
    "    responses.append(llm.generate(prompt, output_format=Output))\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first_name': 'Growing', 'last_name': 'Ananas', 'city': 'Paris'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(\"\"\"{\n",
    "  \"first_name\": \"Growing\",\n",
    "  \"last_name\": \"Ananas\",\n",
    "  \"city\": \"Paris\"\n",
    "}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first_name': 'Emmanuel', 'last_name': 'Macron', 'city': 'New York'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = ['{\\n  \"first_name\": \"Alice\",\\n  \"last_name\": \"Johnson\",\\n  \"city\": \"Rome\"\\n}', '{\\n  \"first_name\": \"Emmanuel\",\\n  \"last_name\": \"Macron\",\\n  \"city\": \"New York\"\\n}\\n','\\n{\\n  \"first_name\": \"Growing\",  \"last_name\": \"Ananas\",\\n  \"city\": \"Paris\"\\n}']\n",
    "\n",
    "\n",
    "eval(l[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai==0.28\n",
      "  Using cached openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: requests>=2.20 in /Users/gardille/opt/anaconda3/envs/feedback_env/lib/python3.11/site-packages (from openai==0.28) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /Users/gardille/opt/anaconda3/envs/feedback_env/lib/python3.11/site-packages (from openai==0.28) (4.66.1)\n",
      "Requirement already satisfied: aiohttp in /Users/gardille/opt/anaconda3/envs/feedback_env/lib/python3.11/site-packages (from openai==0.28) (3.9.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/gardille/opt/anaconda3/envs/feedback_env/lib/python3.11/site-packages (from requests>=2.20->openai==0.28) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/gardille/opt/anaconda3/envs/feedback_env/lib/python3.11/site-packages (from requests>=2.20->openai==0.28) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/gardille/opt/anaconda3/envs/feedback_env/lib/python3.11/site-packages (from requests>=2.20->openai==0.28) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/gardille/opt/anaconda3/envs/feedback_env/lib/python3.11/site-packages (from requests>=2.20->openai==0.28) (2023.11.17)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/gardille/opt/anaconda3/envs/feedback_env/lib/python3.11/site-packages (from aiohttp->openai==0.28) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/gardille/opt/anaconda3/envs/feedback_env/lib/python3.11/site-packages (from aiohttp->openai==0.28) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/gardille/opt/anaconda3/envs/feedback_env/lib/python3.11/site-packages (from aiohttp->openai==0.28) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/gardille/opt/anaconda3/envs/feedback_env/lib/python3.11/site-packages (from aiohttp->openai==0.28) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/gardille/opt/anaconda3/envs/feedback_env/lib/python3.11/site-packages (from aiohttp->openai==0.28) (1.3.1)\n",
      "Using cached openai-0.28.0-py3-none-any.whl (76 kB)\n",
      "Installing collected packages: openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.10.0\n",
      "    Uninstalling openai-1.10.0:\n",
      "      Successfully uninstalled openai-1.10.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "instructor 0.4.8 requires openai<2.0.0,>=1.1.0, but you have openai 0.28.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed openai-0.28.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai==0.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai==1.10.0\n",
      "  Using cached openai-1.10.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/gardille/opt/anaconda3/envs/feedback_env/lib/python3.11/site-packages (from openai==1.10.0) (3.5.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/gardille/opt/anaconda3/envs/feedback_env/lib/python3.11/site-packages (from openai==1.10.0) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/gardille/opt/anaconda3/envs/feedback_env/lib/python3.11/site-packages (from openai==1.10.0) (0.25.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/gardille/opt/anaconda3/envs/feedback_env/lib/python3.11/site-packages (from openai==1.10.0) (2.6.1)\n",
      "Requirement already satisfied: sniffio in /Users/gardille/opt/anaconda3/envs/feedback_env/lib/python3.11/site-packages (from openai==1.10.0) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /Users/gardille/opt/anaconda3/envs/feedback_env/lib/python3.11/site-packages (from openai==1.10.0) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/gardille/opt/anaconda3/envs/feedback_env/lib/python3.11/site-packages (from openai==1.10.0) (4.9.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/gardille/opt/anaconda3/envs/feedback_env/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai==1.10.0) (3.4)\n",
      "Requirement already satisfied: certifi in /Users/gardille/opt/anaconda3/envs/feedback_env/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai==1.10.0) (2023.11.17)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/gardille/opt/anaconda3/envs/feedback_env/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai==1.10.0) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/gardille/opt/anaconda3/envs/feedback_env/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.10.0) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/gardille/opt/anaconda3/envs/feedback_env/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai==1.10.0) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.2 in /Users/gardille/opt/anaconda3/envs/feedback_env/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai==1.10.0) (2.16.2)\n",
      "Using cached openai-1.10.0-py3-none-any.whl (225 kB)\n",
      "Installing collected packages: openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 0.28.0\n",
      "    Uninstalling openai-0.28.0:\n",
      "      Successfully uninstalled openai-0.28.0\n",
      "Successfully installed openai-1.10.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai==1.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: The openai-python library support for Azure OpenAI is in preview.\n",
    "      #Note: This code sample requires OpenAI Python library version 0.28.1 or lower.\n",
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = \"https://vigieinstance.openai.azure.com/\"\n",
    "openai.api_version = \"2023-07-01-preview\"\n",
    "openai.api_key = \"6e612f025340400d827a519b0549cff6\"\n",
    "\n",
    "message_text = [{\"role\":\"system\",\"content\":\"Assistant is an AI chatbot that helps users turn a natural language list into JSON format. After users input a list they want in JSON format, it will provide suggested list of attribute labels if the user has not provided any, then ask the user to confirm them before creating the list.\"}]\n",
    "\n",
    "completion = openai.ChatCompletion.create(\n",
    "  engine=\"dep\",\n",
    "  messages = message_text,\n",
    "  temperature=0.2,\n",
    "  max_tokens=350,\n",
    "  top_p=0.95,\n",
    "  frequency_penalty=0,\n",
    "  presence_penalty=0,\n",
    "  stop=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-8s0I2aZv3YMhrHcciMxIAiBAG80qd at 0x10a456a50> JSON: {\n",
       "  \"id\": \"chatcmpl-8s0I2aZv3YMhrHcciMxIAiBAG80qd\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"created\": 1707881518,\n",
       "  \"model\": \"gpt-4\",\n",
       "  \"prompt_filter_results\": [\n",
       "    {\n",
       "      \"prompt_index\": 0,\n",
       "      \"content_filter_results\": {}\n",
       "    }\n",
       "  ],\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"index\": 0,\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"message\": {\n",
       "        \"role\": \"assistant\",\n",
       "        \"content\": \"Sure, I can help you with that. Please provide me with the natural language list that you would like to convert into JSON format. If you have specific attribute labels in mind, include them as well; otherwise, I can suggest some for you.\"\n",
       "      },\n",
       "      \"content_filter_result\": {\n",
       "        \"error\": {\n",
       "          \"code\": \"content_filter_error\",\n",
       "          \"message\": \"The contents are not filtered\"\n",
       "        }\n",
       "      },\n",
       "      \"content_filter_results\": {}\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 64,\n",
       "    \"completion_tokens\": 50,\n",
       "    \"total_tokens\": 114\n",
       "  },\n",
       "  \"system_fingerprint\": \"fp_68a7d165bf\"\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: The openai-python library support for Azure OpenAI is in preview.\n",
    "      #Note: This code sample requires OpenAI Python library version 0.28.1 or lower.\n",
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = \"https://vigieinstance.openai.azure.com/\"\n",
    "#openai.api_version = \"2023-07-01-preview\"\n",
    "openai.api_key = \"6e612f025340400d827a519b0549cff6\"\n",
    "\n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "message_text = [\n",
    "    #{\"role\":\"system\",\"content\":\"Assistant is an AI chatbot that helps users turn a natural language list into JSON format. After users input a list they want in JSON format, it will provide suggested list of attribute labels if the user has not provided any, then ask the user to confirm them before creating the list.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Say this is a test\"},\n",
    "    ]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  #engine=\"dep\",\n",
    "  model=\"gpt-4\",\n",
    "  messages = message_text,\n",
    "  temperature=0.2,\n",
    "  max_tokens=350,\n",
    "  top_p=0.95,\n",
    "  frequency_penalty=0,\n",
    "  presence_penalty=0,\n",
    "  stop=None\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "feedback_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
