{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'async for' requires an object with __aiter__ method, got coroutine",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# With async\u001b[39;00m\n\u001b[1;32m     14\u001b[0m async_response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat(model\u001b[38;5;241m=\u001b[39mmodel, messages\u001b[38;5;241m=\u001b[39mmessages)\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m async_response: \n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(chunk\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdelta\u001b[38;5;241m.\u001b[39mcontent)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'async for' requires an object with __aiter__ method, got coroutine"
     ]
    }
   ],
   "source": [
    "from mistralai.async_client import MistralAsyncClient\n",
    "from mistralai.models.chat_completion import ChatMessage\n",
    "\n",
    "api_key = MISTRAL_API_KEY\n",
    "model = \"mistral-tiny\"\n",
    "\n",
    "client = MistralAsyncClient(api_key=api_key)\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(role=\"user\", content=\"What is the best French cheese?\")\n",
    "]\n",
    "\n",
    "# With async\n",
    "async_response = client.chat(model=model, messages=messages)\n",
    "\n",
    "async for chunk in async_response: \n",
    "    print(chunk.choices[0].delta.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"first_name\": \"Alice\",\n",
      "  \"last_name\": \"Johnson\",\n",
      "  \"city\": \"Rome\"\n",
      "}\n",
      "{\n",
      "  \"first_name\": \"Emmanuel\",\n",
      "  \"last_name\": \"Macron\",\n",
      "  \"city\": \"New York\"\n",
      "}\n",
      "{\n",
      "  \"first_name\": \"Growing\",\n",
      "  \"last_name\": \"Ananas\",\n",
      "  \"city\": \"Paris\"\n",
      "}\n",
      "['{\\n  \"first_name\": \"Alice\",\\n  \"last_name\": \"Johnson\",\\n  \"city\": \"Rome\"\\n}', '{\\n  \"first_name\": \"Emmanuel\",\\n  \"last_name\": \"Macron\",\\n  \"city\": \"New York\"\\n}', '{\\n  \"first_name\": \"Growing\",\\n  \"last_name\": \"Ananas\",\\n  \"city\": \"Paris\"\\n}']\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "from mistralai.client import MistralClient\n",
    "from mistralai.async_client import MistralAsyncClient\n",
    "\n",
    "from mistralai.models.chat_completion import ChatMessage\n",
    "from pydantic import BaseModel, ValidationError, ConfigDict\n",
    "from typing import Type, Optional\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv()\n",
    "MISTRAL_API_KEY = \"GFBjsGogmbv0LuMWjJewXBXwyN7QeKNj\"\n",
    "\n",
    "\n",
    "class MistralLanguageModel:\n",
    "    def __init__(self, api_key=MISTRAL_API_KEY,\n",
    "                 model=\"mistral-tiny\", temperature=0.0):\n",
    "\n",
    "        if api_key is None:\n",
    "            raise ValueError(\"The Mistral API KEY must be provided either as \"\n",
    "                             \"an argument or as an environment variable named 'MISTRAL_API_KEY'\") # noqa\n",
    "\n",
    "        self.api_key = api_key\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "        \n",
    "        #self.client = MistralAsyncClient(api_key=self.api_key)\n",
    "        self.client = MistralClient(api_key=self.api_key)\n",
    "\n",
    "    def async_generation(self,  prompt: str, #async\n",
    "                 output_format: Optional[Type[BaseModel]] = None,\n",
    "                 max_tokens: int = None):\n",
    "        system_message = \"You are a helpful assistant.\"\n",
    "        if output_format:\n",
    "            system_message += f\" Respond in a JSON format that contains the following keys: {self._model_structure_repr(output_format)}. You must only return a JSON, nothing else. You are strictly forbidden to return anything else, like an explanation.\" # noqa\n",
    "\n",
    "        params = {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": [\n",
    "                ChatMessage(role=\"system\", content=system_message),\n",
    "                ChatMessage(role=\"user\", content=prompt)\n",
    "            ],\n",
    "            \"temperature\": self.temperature}\n",
    "\n",
    "        if max_tokens is not None:\n",
    "            params[\"max_tokens\"] = max_tokens\n",
    "\n",
    "        return self.client.chat(**params)\n",
    "    \n",
    "\n",
    "    def generate(self, prompts: list[str],\n",
    "                 output_format: Optional[Type[BaseModel]] = None,\n",
    "                 max_tokens: int = None):\n",
    "\n",
    "        \n",
    "\n",
    "        \"\"\"loop = asyncio.get_event_loop()\n",
    "        tasks = [loop.create_task(self.async_generation(prompt, output_format)) for prompt in prompts]\n",
    "\n",
    "        responses = loop.run_until_complete(asyncio.gather(*tasks))\n",
    "        responses = [self.async_generation(prompt, output_format) for prompt in prompts]\n",
    "        \"\"\"\n",
    "        responses = []\n",
    "        for prompt in prompts:\n",
    "            res = self.async_generation(prompt, output_format).choices[0].message.content\n",
    "            print(res)\n",
    "            assert self._is_valid_json_for_model(res, output_format)\n",
    "            responses.append(res)\n",
    "        #responses = [response.choices[0].message.content for response in responses]\n",
    "        return responses\n",
    "        \n",
    "        print(responses)\n",
    "        if output_format:\n",
    "            for res in responses:\n",
    "                assert self._is_valid_json_for_model(res, output_format)\n",
    "                \n",
    "        return responses\n",
    "\n",
    "    def _model_structure_repr(self, model: Type[BaseModel]) -> str:\n",
    "        fields = model.__annotations__\n",
    "        return ', '.join(f'{key}: {value}' for key, value in fields.items())\n",
    "\n",
    "    def _is_valid_json_for_model(self, text: str, model: Type[BaseModel]) -> bool: # noqa\n",
    "        \"\"\"\n",
    "        Check if a text is valid JSON and if it respects the provided BaseModel. # noqa\n",
    "        \"\"\"\n",
    "        model.model_config = ConfigDict(strict=True)\n",
    "\n",
    "        try:\n",
    "            parsed_data = json.loads(text)\n",
    "            model(**parsed_data)\n",
    "            return True\n",
    "        except (json.JSONDecodeError, ValidationError):\n",
    "            parsed_data = json.loads(text)\n",
    "            model(**parsed_data)\n",
    "            return False\n",
    "\n",
    "\n",
    "class Output(BaseModel):\n",
    "    first_name: str\n",
    "    last_name: str\n",
    "    city: str\n",
    "\n",
    "\n",
    "llm = MistralLanguageModel()\n",
    "prompts = [\n",
    "    'Extract the requested  information from the following sentence: \"Alice Johnson is visiting Rome.\"',\n",
    "    'Extract the requested  information from the following sentence: \"Emmanuel Macron is visiting New York.\"',\n",
    "    'Extract the requested  information from the following sentence: \"Growing Ananas is visiting Paris.\"'\n",
    "]\n",
    "response = llm.generate(prompts, output_format=Output)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MistralLanguageModel:\n",
    "    def __init__(self, api_key=MISTRAL_API_KEY,\n",
    "                 model=\"mistral-tiny\", temperature=0.5):\n",
    "\n",
    "        if api_key is None:\n",
    "            raise ValueError(\"The Mistral API KEY must be provided either as \"\n",
    "                             \"an argument or as an environment variable named 'MISTRAL_API_KEY'\") # noqa\n",
    "\n",
    "        self.api_key = api_key\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "        self.client = MistralClient(api_key=self.api_key)\n",
    "\n",
    "    def generate(self, prompt: str,\n",
    "                 output_format: Optional[Type[BaseModel]] = None,\n",
    "                 max_tokens: int = None):\n",
    "\n",
    "        retry_delay = 0.1\n",
    "\n",
    "        while True:\n",
    "            if True:\n",
    "            #try:\n",
    "                system_message = \"You are a helpful assistant.\"\n",
    "                if output_format:\n",
    "                    system_message += f\" Respond in a JSON format that contains the following keys: {self._model_structure_repr(output_format)}. You must only return a JSON, nothing else. You are strictly forbidden to return anything else, like an explanation.\" # noqa\n",
    "\n",
    "                messages = [\n",
    "                    ChatMessage(role=\"system\", content=system_message),\n",
    "                    ChatMessage(role=\"user\", content=prompt)\n",
    "                ]\n",
    "                params = {\n",
    "                    \"model\": self.model,\n",
    "                    \"messages\": messages,\n",
    "                    \"temperature\": self.temperature\n",
    "                }\n",
    "\n",
    "                if max_tokens is not None:\n",
    "                    params[\"max_tokens\"] = max_tokens\n",
    "\n",
    "                response = self.client.chat(**params)\n",
    "                response_content = response.choices[0].message.content\n",
    "                print(response_content)\n",
    "\n",
    "                if output_format:\n",
    "                    assert self._is_valid_json_for_model(response_content,\n",
    "                                                     output_format)\n",
    "                    return response_content\n",
    "                #else:\n",
    "                #    return response_content\n",
    "\n",
    "            #except Exception:\n",
    "            #    print(f\"Hit rate limit. Retrying in {retry_delay} seconds.\")\n",
    "            #    time.sleep(retry_delay)\n",
    "            #    retry_delay *= 2\n",
    "\n",
    "    def _model_structure_repr(self, model: Type[BaseModel]) -> str:\n",
    "        fields = model.__annotations__\n",
    "        return ', '.join(f'{key}: {value}' for key, value in fields.items())\n",
    "\n",
    "\n",
    "    def _is_valid_json_for_model(self, text: str, model: Type[BaseModel]) -> bool: # noqa\n",
    "        \"\"\"\n",
    "        Check if a text is valid JSON and if it respects the provided BaseModel. # noqa\n",
    "        \"\"\"\n",
    "        model.model_config = ConfigDict(strict=True)\n",
    "\n",
    "        try:\n",
    "            parsed_data = json.loads(text)\n",
    "            model(**parsed_data)\n",
    "            return True\n",
    "        except (json.JSONDecodeError, ValidationError):\n",
    "            return False\n",
    "llm = MistralLanguageModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"first_name\": \"Alice\",\n",
      "  \"last_name\": \"Johnson\",\n",
      "  \"city\": \"Rome\"\n",
      "}\n",
      "{\n",
      "  \"first_name\": \"Emmanuel\",\n",
      "  \"last_name\": \"Macron\",\n",
      "  \"city\": \"New York\"\n",
      "}\n",
      "{\n",
      "  \"first_name\": \"Growing\",\n",
      "  \"last_name\": \"Ananas\",\n",
      "  \"city\": \"Paris\"\n",
      "}\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sleep\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m----> 9\u001b[0m     responses\u001b[38;5;241m.\u001b[39mappend(llm\u001b[38;5;241m.\u001b[39mgenerate(prompt, output_format\u001b[38;5;241m=\u001b[39mOutput))\n\u001b[1;32m     10\u001b[0m     sleep(\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[57], line 45\u001b[0m, in \u001b[0;36mMistralLanguageModel.generate\u001b[0;34m(self, prompt, output_format, max_tokens)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(response_content)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_format:\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_valid_json_for_model(response_content,\n\u001b[1;32m     46\u001b[0m                                      output_format)\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response_content\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    'Extract the requested  information from the following sentence: \"Alice Johnson is visiting Rome.\"',\n",
    "    'Extract the requested  information from the following sentence: \"Emmanuel Macron is visiting New York.\"'\n",
    "    'Extract the requested  information from the following sentence: \"Growing Ananas is visiting Paris.\"'\n",
    "]\n",
    "responses = []\n",
    "from time import sleep\n",
    "for prompt in prompts:\n",
    "    responses.append(llm.generate(prompt, output_format=Output))\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first_name': 'Growing', 'last_name': 'Ananas', 'city': 'Paris'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(\"\"\"{\n",
    "  \"first_name\": \"Growing\",\n",
    "  \"last_name\": \"Ananas\",\n",
    "  \"city\": \"Paris\"\n",
    "}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first_name': 'Emmanuel', 'last_name': 'Macron', 'city': 'New York'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = ['{\\n  \"first_name\": \"Alice\",\\n  \"last_name\": \"Johnson\",\\n  \"city\": \"Rome\"\\n}', '{\\n  \"first_name\": \"Emmanuel\",\\n  \"last_name\": \"Macron\",\\n  \"city\": \"New York\"\\n}\\n','\\n{\\n  \"first_name\": \"Growing\",  \"last_name\": \"Ananas\",\\n  \"city\": \"Paris\"\\n}']\n",
    "\n",
    "\n",
    "eval(l[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "feedback_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
